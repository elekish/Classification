{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9685965,"sourceType":"datasetVersion","datasetId":5921041},{"sourceId":9876246,"sourceType":"datasetVersion","datasetId":6063390}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/isabbaggin/classification?scriptVersionId=217763074\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-01-15T08:07:14.173868Z","iopub.execute_input":"2025-01-15T08:07:14.174246Z","iopub.status.idle":"2025-01-15T08:07:15.637678Z","shell.execute_reply.started":"2025-01-15T08:07:14.174197Z","shell.execute_reply":"2025-01-15T08:07:15.636273Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/datanew/data2/240427SG_BN.xlsx\n/kaggle/input/datanew/data2/240511IH_AN.xlsx\n/kaggle/input/datanew/data2/240405SM_BN.xlsx\n/kaggle/input/datanew/data2/240511IS_BN.xlsx\n/kaggle/input/datanew/data2/240420BR_AN.xlsx\n/kaggle/input/datanew/data2/240501BD_BP.xlsx\n/kaggle/input/datanew/data2/240501SK_BP.xlsx\n/kaggle/input/datanew/data2/240420AM_AP.xlsx\n/kaggle/input/datanew/data2/240501BD_AP.xlsx\n/kaggle/input/datanew/data2/240511BI_BN.xlsx\n/kaggle/input/datanew/data2/240427AD_AP.xlsx\n/kaggle/input/datanew/data2/240420RM_BP.xlsx\n/kaggle/input/datanew/data2/240501DM_BP.xlsx\n/kaggle/input/datanew/data2/240229SM_BN.xlsx\n/kaggle/input/datanew/data2/240427IM_AN.xlsx\n/kaggle/input/datanew/data2/240427BB_BN.xlsx\n/kaggle/input/datanew/data2/240501SK_AP.xlsx\n/kaggle/input/datanew/data2/240420BM_BP.xlsx\n/kaggle/input/datanew/data2/240501DM_AP.xlsx\n/kaggle/input/datanew/data2/240511IH_BN.xlsx\n/kaggle/input/datanew/data2/240405SM_AN.xlsx\n/kaggle/input/datanew/data2/240427SD_AN.xlsx\n/kaggle/input/datanew/data2/240405DH_BP.xlsx\n/kaggle/input/datanew/data2/240427SS_AN.xlsx\n/kaggle/input/datanew/data2/240420AM_BP.xlsx\n/kaggle/input/datanew/data2/240420AJ_BP.xlsx\n/kaggle/input/datanew/data2/240501AR_AP.xlsx\n/kaggle/input/datanew/data2/240501BS_AP.xlsx\n/kaggle/input/datanew/data2/240427BB_AN.xlsx\n/kaggle/input/datanew/data2/240427AD_BP.xlsx\n/kaggle/input/datanew/data2/240420AJ_AP.xlsx\n/kaggle/input/datanew/data2/240420DH_AP.xlsx\n/kaggle/input/datanew/data2/240511IS_AN.xlsx\n/kaggle/input/datanew/data2/240511MD_AN.xlsx\n/kaggle/input/datanew/data2/240427SG_AN.xlsx\n/kaggle/input/datanew/data2/240501AR_BP.xlsx\n/kaggle/input/datanew/data2/240427SD_BN.xlsx\n/kaggle/input/datanew/data2/240420RM_AP.xlsx\n/kaggle/input/datanew/data2/240511JI_BN.xlsx\n/kaggle/input/datanew/data2/240427SS_BN.xlsx\n/kaggle/input/datanew/data2/240229DH_AP.xlsx\n/kaggle/input/datanew/data2/240511MD_BN.xlsx\n/kaggle/input/datanew/data2/240420DH_BP.xlsx\n/kaggle/input/datanew/data2/240511SL_BP.xlsx\n/kaggle/input/datanew/data2/240229SM_AN.xlsx\n/kaggle/input/datanew/data2/240427IM_BN.xlsx\n/kaggle/input/datanew/data2/240511KM_BN.xlsx\n/kaggle/input/datanew/data2/240229DH_BP.xlsx\n/kaggle/input/datanew/data2/240405DH_AP.xlsx\n/kaggle/input/datanew/data2/240511SL_AP.xlsx\n/kaggle/input/datanew/data2/240501BS_BP.xlsx\n/kaggle/input/datanew/data2/240511BI_AN.xlsx\n/kaggle/input/datanew/data2/240420BR_BN.xlsx\n/kaggle/input/datanew/data2/240511JI_AN.xlsx\n/kaggle/input/datanew/data2/240511KM_AN.xlsx\n/kaggle/input/datanew/data2/240420BM_AP.xlsx\n/kaggle/input/datafilesy/Data/240427SG_BN.xlsx\n/kaggle/input/datafilesy/Data/240420RM_AN.xlsx\n/kaggle/input/datafilesy/Data/240511IH_AN.xlsx\n/kaggle/input/datafilesy/Data/240501SK_BN.xlsx\n/kaggle/input/datafilesy/Data/240405SM_BN.xlsx\n/kaggle/input/datafilesy/Data/240511IS_BN.xlsx\n/kaggle/input/datafilesy/Data/240420BR_AN.xlsx\n/kaggle/input/datafilesy/Data/240501BD_BP.xlsx\n/kaggle/input/datafilesy/Data/240420AM_AP.xlsx\n/kaggle/input/datafilesy/Data/240501BD_AP.xlsx\n/kaggle/input/datafilesy/Data/240511BI_BN.xlsx\n/kaggle/input/datafilesy/Data/240501DM_BP.xlsx\n/kaggle/input/datafilesy/Data/240229SM_BN.xlsx\n/kaggle/input/datafilesy/Data/240427IM_AN.xlsx\n/kaggle/input/datafilesy/Data/240427BB_BN.xlsx\n/kaggle/input/datafilesy/Data/240511SL_AN.xlsx\n/kaggle/input/datafilesy/Data/240501DM_AP.xlsx\n/kaggle/input/datafilesy/Data/240511IH_BN.xlsx\n/kaggle/input/datafilesy/Data/240405SM_AN.xlsx\n/kaggle/input/datafilesy/Data/240501SK_AN.xlsx\n/kaggle/input/datafilesy/Data/240427SD_AN.xlsx\n/kaggle/input/datafilesy/Data/240405DH_BP.xlsx\n/kaggle/input/datafilesy/Data/240427SS_AN.xlsx\n/kaggle/input/datafilesy/Data/240420AM_BP.xlsx\n/kaggle/input/datafilesy/Data/240420AJ_BP.xlsx\n/kaggle/input/datafilesy/Data/240501AR_AP.xlsx\n/kaggle/input/datafilesy/Data/240501BS_AP.xlsx\n/kaggle/input/datafilesy/Data/240427BB_AN.xlsx\n/kaggle/input/datafilesy/Data/240427AD_BN.xlsx\n/kaggle/input/datafilesy/Data/240511SL_BN.xlsx\n/kaggle/input/datafilesy/Data/240420AJ_AP.xlsx\n/kaggle/input/datafilesy/Data/240420DH_AP.xlsx\n/kaggle/input/datafilesy/Data/240420BM_AN.xlsx\n/kaggle/input/datafilesy/Data/240420BM_BN.xlsx\n/kaggle/input/datafilesy/Data/240511IS_AN.xlsx\n/kaggle/input/datafilesy/Data/240511MD_AN.xlsx\n/kaggle/input/datafilesy/Data/240427SG_AN.xlsx\n/kaggle/input/datafilesy/Data/240501AR_BP.xlsx\n/kaggle/input/datafilesy/Data/240427SD_BN.xlsx\n/kaggle/input/datafilesy/Data/240511JI_BN.xlsx\n/kaggle/input/datafilesy/Data/240427AD_AN.xlsx\n/kaggle/input/datafilesy/Data/240427SS_BN.xlsx\n/kaggle/input/datafilesy/Data/240229DH_AP.xlsx\n/kaggle/input/datafilesy/Data/240511MD_BN.xlsx\n/kaggle/input/datafilesy/Data/240420DH_BP.xlsx\n/kaggle/input/datafilesy/Data/240229SM_AN.xlsx\n/kaggle/input/datafilesy/Data/240427IM_BN.xlsx\n/kaggle/input/datafilesy/Data/240511KM_BN.xlsx\n/kaggle/input/datafilesy/Data/240229DH_BP.xlsx\n/kaggle/input/datafilesy/Data/240405DH_AP.xlsx\n/kaggle/input/datafilesy/Data/240501BS_BP.xlsx\n/kaggle/input/datafilesy/Data/240511BI_AN.xlsx\n/kaggle/input/datafilesy/Data/240420BR_BN.xlsx\n/kaggle/input/datafilesy/Data/240511JI_AN.xlsx\n/kaggle/input/datafilesy/Data/240420RM_BN.xlsx\n/kaggle/input/datafilesy/Data/240511KM_AN.xlsx\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nfrom matplotlib.widgets import CheckButtons\n\n\n# def plot_data(matrix):\n#     time_per_row = np.arange(0, 12000, 1)  # Time vector for 2400 rows\n#     matrix=np.array(matrix)\n#     plt.figure(figsize=(20,8))\n#     for i in range(matrix.shape[0]):\n#       plt.plot(np.array(matrix[i,:]))\n#\n#     # for i, row in enumerate(MATRIX):\n#     #     plt.scatter(time_per_row, row)\n#\n#     # Adding labels and title\n#     plt.title('LH_B_P')\n#     plt.xlabel('TIME')\n#     plt.ylabel('AMPLITUDE')\n#     plt.legend()\n#     plt.show()\n#\n#     # Plot LH data\n#     # for i in range(LH_all_pre.shape[0]):\n#     #     axs[0].plot(time_per_row, LH_all_pre[i, :, 0])\n#     #\n#     # axs[0].set_title(\"LH Data - Time vs Amplitude\")\n#     # axs[0].set_xlabel(\"Time (minutes)\")\n#     # axs[0].set_ylabel(\"Amplitude\")\n#     # axs[0].legend()\n#     # axs[0].grid(True)\n#     #\n#     # # Plot RH data\n#     # for i in range(RH_all_pre.shape[0]):\n#     #     axs[1].plot(time_per_row, RH_all_pre[i, :, 0])\n#     #\n#     # axs[1].set_title(\"RH Data - Time vs Amplitude\")\n#     # axs[1].set_xlabel(\"Time (minutes)\")\n#     # axs[1].set_ylabel(\"Amplitude\")\n#     # axs[1].legend()\n#     # axs[1].grid(True)\n#     #\n#     # # Plot LL data\n#     # for i in range(LL_all_pre.shape[0]):\n#     #     axs[2].plot(time_per_row, LL_all_pre[i, :, 0])\n#     #\n#     # axs[2].set_title(\"LL Data - Time vs Amplitude\")\n#     # axs[2].set_xlabel(\"Time (minutes)\")\n#     # axs[2].set_ylabel(\"Amplitude\")\n#     # axs[2].legend()\n#     # axs[2].grid(True)\n#     #\n#     # # Plot RL data\n#     # for i in range(RL_all_pre.shape[0]):\n#     #     axs[3].plot(time_per_row, RL_all_pre[i, :, 0])\n#     #\n#     # axs[3].set_title(\"RL Data - Time vs Amplitude\")\n#     # axs[3].set_xlabel(\"Time (minutes)\")\n#     # axs[3].set_ylabel(\"Amplitude\")\n#     # axs[3].legend()\n#     # axs[3].grid(True)\n#\n#     plt.tight_layout(pad=2.0)\n#     plt.show()\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport ipywidgets as widgets\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport ipywidgets as widgets\n\ndef plot_with_widgets(matrix):\n    matrix = np.array(matrix)\n    fig, ax = plt.subplots(figsize=(10, 5))\n\n    # Initial plot\n    lines = []\n    for i in range(matrix.shape[0]):\n        line, = ax.plot(matrix[i, :], label=f'Line {i+1}')\n        lines.append(line)\n    ax.legend()\n\n    # Create checkboxes for each line\n    checkboxes = [widgets.Checkbox(value=True, description=f'Line {i+1}') for i in range(matrix.shape[0])]\n    check_ui = widgets.VBox(checkboxes)\n\n    def update_plot(*args):\n        # Clear the current axes\n        ax.clear()\n\n        # Re-plot only the lines with checked checkboxes\n        for i, checkbox in enumerate(checkboxes):\n            if checkbox.value:  # If checkbox is checked\n                ax.plot(matrix[i, :], label=f'Line {i+1}')\n\n        # Redraw the legend and labels\n        ax.legend()\n        ax.set_title('Interactive Line Plot')\n        ax.set_xlabel('X-axis')\n        ax.set_ylabel('Y-axis')\n\n        # Redraw the figure\n        fig.canvas.draw()\n\n    # Link the checkboxes to the update_plot function\n    for checkbox in checkboxes:\n        checkbox.observe(update_plot, 'value')\n\n    # Display the checkboxes UI\n    display(check_ui)\n    plt.show()\n\n\n\n\n\n\ndef plot_data(matrix, save_path='C:\\\\Users\\\\PC1\\\\Pictures\\\\plot\\\\deviation_post_P.jpg'):\n    sns.set(style=\"whitegrid\")\n    matrix = np.array(matrix)\n\n    plt.figure(figsize=(20, 10))\n    palette = sns.color_palette(\"husl\", matrix.shape[0])\n    for i in range(matrix.shape[0]):\n        plt.plot(matrix[i, :], color=palette[i], linewidth=1.0)\n    plt.title('deviation_post_P', fontsize=16)\n    plt.xlabel('TIME', fontsize=14)\n    plt.ylabel('AMPLITUDE', fontsize=14)\n    plt.legend(title=\"Data Rows\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=12)\n\n    plt.tight_layout()\n#     plt.savefig(save_path, format='jpg', dpi=300)\n    plt.show()\n\ndef plot(matrix, save_path='C:\\\\Users\\\\PC1\\\\Pictures\\\\plot\\\\deviation_post_NP.jpg'):\n    sns.set(style=\"whitegrid\")\n    matrix = np.array(matrix)\n    fig, ax = plt.subplots(figsize=(20, 10))\n    palette = sns.color_palette(\"husl\", matrix.shape[0])\n    lines = []\n    for i in range(matrix.shape[0]):\n        line, = ax.plot(matrix[i, :], color=palette[i], linewidth=1.0, label=f'Line {i + 1}')\n        lines.append(line)\n    ax.set_title('deviation_post_NP', fontsize=16)\n    ax.set_xlabel('TIME', fontsize=14)\n    ax.set_ylabel('AMPLITUDE', fontsize=14)\n    labels = [f'Line {i + 1}' for i in range(matrix.shape[0])]\n    check = CheckButtons(ax=plt.axes([0.8, 0.4, 0.1, 0.15]), labels=labels, actives=[True] * len(labels))\n\n    for i, line in enumerate(lines):\n        check.labels[i].set_color(line.get_color())\n    def func(label):\n        index = labels.index(label)\n        lines[index].set_visible(not lines[index].get_visible())\n        plt.draw()\n\n    check.on_clicked(func)\n\n    plt.tight_layout()\n    # plt.savefig(save_path, format='jpg', dpi=300)\n    plt.show()\n\n\ndef plot_state_means(LH_all_pre_P, RH_all_pre_P, save_path='C:\\\\Users\\\\PC1\\\\Pictures\\\\plot\\\\pre_P.jpg'):\n\n    def extract_state_data(combined_LH, combined_RH):\n        states = {}\n        num_samples = combined_LH.shape[0]  # Number of samples\n        num_states = combined_LH.shape[2]    # Number of states (5 in this case)\n\n        for i in range(num_samples):  # Iterate through the samples\n            for j in range(num_states):  # Iterate through the states\n                state_name = f'State {j + 1}'\n                if state_name not in states:\n                    states[state_name] = []\n                states[state_name].append(combined_LH[i, :, j])  # Append LH data\n                states[state_name].append(combined_RH[i, :, j])  # Append RH data\n\n        return states\n\n\n    state_data_pre_P = extract_state_data(LH_all_pre_P, RH_all_pre_P)\n    print(state_data_pre_P)\n    means = {}\n    for state_name, data in state_data_pre_P.items():\n        means[state_name] = [np.mean(data_array) for data_array in data]\n    box_data = {state: [] for state in means.keys()}\n    for state in means.keys():\n        box_data[state].extend(means[state])\n    # plt.figure(figsize=(15, 10))\n    # sns.boxplot(data=[box_data[state] for state in box_data], palette=\"Set3\")\n    # plt.xticks(ticks=np.arange(len(box_data)), labels=box_data.keys())\n    # plt.title('Box Plots of Means for Each State')\n    # plt.xlabel('States')\n    # plt.ylabel('Mean Values')\n    # # plt.legend(['Pre P', 'Post P', 'Pre NP', 'Post NP'], loc='upper right')\n    # plt.savefig(save_path, format='jpg', dpi=300)\n    # plt.tight_layout()\n    # plt.show()\n\n# Example usage:\n# if __name__ == \"__main__\":\n#     plot_state_means(LH_all_pre_P, RH_all_pre_P, LH_all_post_P, RH_all_post_P,\n#                      LH_all_pre_NP, RH_all_pre_NP, LH_all_post_NP, RH_all_post_NP)\n\ndef plot_median_of_means(LH_all_pre_P, RH_all_pre_P):\n\n    combined_data = np.concatenate((LH_all_pre_P, RH_all_pre_P), axis=0)\n\n    #Number of states (columns)\n\n    num_states = combined_data.shape[2] # Assuming 5 states as columns \n    means_per_state = []\n\n    # Calculate mean of each column in each 2D array\n\n    for state in range(num_states): \n        means_for_state = []\n\n        for matrix in combined_data: \n            means_for_state.append(np.mean(matrix[:, state])) # Mean of each column for state \n        means_per_state.append(means_for_state)\n\n    return means_per_state\n","metadata":{"execution":{"iopub.status.busy":"2024-11-11T19:03:13.826384Z","iopub.execute_input":"2024-11-11T19:03:13.82705Z","iopub.status.idle":"2024-11-11T19:03:15.712691Z","shell.execute_reply.started":"2024-11-11T19:03:13.826996Z","shell.execute_reply":"2024-11-11T19:03:15.711464Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.stats import kurtosis,skew\n\n\n\n\ndef calculate_statistics(data_3d):\n    data_3d = np.array(pd.to_numeric(data_3d.flatten(), errors='coerce')).reshape(data_3d.shape)\n\n\n    means = []\n    for i in range(data_3d.shape[0]):\n        data_without_nans = np.nan_to_num(data_3d[i], nan=0.0)\n        mean_value = np.sum(data_without_nans) / (2400 * 5)  # Sum and divide by 2400*5\n        means.append(mean_value)\n\n    std_devs = []\n\n    for i in range(data_3d.shape[0]):\n        data_without_nans = np.nan_to_num(data_3d[i], nan=0.0)\n        mean_value = np.sum(data_without_nans) / (2400 * 5)\n        squared_diffs = (data_without_nans - mean_value) ** 2\n        std_dev_value = np.sqrt(np.sum(squared_diffs) / (2400 * 5))\n        std_devs.append(std_dev_value)\n    # std_devs = np.nanstd(data_3d, axis=1)\n\n    return means, std_devs\n\ndef flatten_3d_to_2d(array_3d):\n    if array_3d.ndim != 3:\n        raise ValueError(\"Input array must be 3D\")\n    flattened_arrays = np.array([array_3d[i].flatten() for i in range(array_3d.shape[0])])\n\n    return flattened_arrays\n\n\n# def flatten_3d_to_2d_col(array_3d):\n#     array_3d = np.array(array_3d)\n#     # if array_3d.ndim != 3:\n#     #     raise ValueError(\"Input array must be 3D\")\n#     n_slices, rows, cols = array_3d.shape\n#     flattened_arrays = []\n#     for i in range(n_slices):\n#         array_2d = array_3d[i]\n#         flattened_array = array_2d.T.flatten()  # Transpose to get columns first, then flatten\n#         flattened_arrays.append(flattened_array)\n#\n#     return np.array(flattened_arrays)\n\nimport numpy as np\n\n\ndef flatten_3d_to_2d_col(array_3d):\n    target_shape = (2400, 5)  # The desired shape for all 2D arrays\n\n    # Ensure array_3d is a list and handle inconsistent shapes\n    if isinstance(array_3d, list):\n        padded_arrays = []\n\n        for sub_array in array_3d:\n            sub_array = np.asarray(sub_array)\n\n            # Check the shape of the current sub-array\n            if sub_array.shape != target_shape:\n                # Pad with zeros to make it (2400, 5)\n                padded_sub_array = np.zeros(target_shape)\n                # Fill in the available data\n                rows, cols = sub_array.shape\n                padded_sub_array[:rows, :cols] = sub_array\n                padded_arrays.append(padded_sub_array)\n            else:\n                padded_arrays.append(sub_array)\n\n        array_3d = np.array(padded_arrays)\n\n    # Check if the input is now a valid 3D array\n    if array_3d.ndim != 3:\n        raise ValueError(\"Input array must be 3D\")\n\n    n_slices, rows, cols = array_3d.shape\n    flattened_arrays = []\n\n    for i in range(n_slices):\n        array_2d = array_3d[i]\n        flattened_array = array_2d.T.flatten()  # Transpose to get columns first, then flatten\n        flattened_arrays.append(flattened_array)\n\n    return np.array(flattened_arrays)\n\n\ndef z_normalize(array_1d, mean, std_dev):\n    return (array_1d - mean) / std_dev\n\n\ndef calculate_normalized_variance_and_kurtosis(data):\n    data = np.asarray(data, dtype=float)  # Convert to float, handles None\n    data = np.nan_to_num(data)\n    normalized_variance = np.var(data, ddof=1)  # Sample variance\n    kurt_value = kurtosis(data)\n    return normalized_variance, kurt_value\n\ndef process_batches_for_normalised(array_1d, batch_size=200):\n    num_batches = len(array_1d) // batch_size\n    variances = []\n    kurtoses = []\n\n    for i in range(num_batches):\n        batch = array_1d[i * batch_size:(i + 1) * batch_size]\n        var, kurt = calculate_normalized_variance_and_kurtosis(batch)\n        variances.append(var)\n        kurtoses.append(kurt)\n\n    return np.array(variances), np.array(kurtoses)\n\ndef calculate_statistics_in_batches(data):\n    # Ensure data is numeric and replace NaN/None with 0\n    data = np.asarray(data, dtype=float)  # Convert to float, handles None\n    data = np.nan_to_num(data)  # Replace NaN with 0\n\n    # Calculate mean, standard deviation, variance, and skewness\n    mean_value = np.mean(data)\n    std_dev_value = np.std(data, ddof=1)  # Sample std deviation\n    variance_value = np.var(data, ddof=1)  # Sample variance\n    skewness_value = skew(data)\n\n    return mean_value, std_dev_value, variance_value, skewness_value\n\ndef process_batches_raw(array_1d, batch_size=200):\n    num_batches = len(array_1d) // batch_size\n    means = []\n    stddevs=[]\n    variances=[]\n    skews=[]\n\n    for i in range(num_batches):\n        batch = array_1d[i * batch_size:(i + 1) * batch_size]\n        mm, sstd, varr, skewness = calculate_statistics_in_batches(batch)\n        variances.append(varr)\n        means.append(mm)\n        stddevs.append(sstd)\n        skews.append(skewness)\n\n\n    return np.array(means), np.array(stddevs), np.array(variances), np.array(skews)","metadata":{"execution":{"iopub.status.busy":"2025-01-15T08:07:15.639816Z","iopub.execute_input":"2025-01-15T08:07:15.640458Z","iopub.status.idle":"2025-01-15T08:07:17.047587Z","shell.execute_reply.started":"2025-01-15T08:07:15.640397Z","shell.execute_reply":"2025-01-15T08:07:17.045051Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tkinter import filedialog\n# from plot import plot_data, plot_state_means\n# from characteristics import calculate_statistics, flatten_3d_to_2d, z_normalize, \\\n#    process_batches_for_normalised, process_batches_raw, flatten_3d_to_2d_col\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndef load_and_extract_data(filepath):\n    # try:\n    dataall = pd.read_excel(filepath, sheet_name='240229SN').to_numpy()\n    # except ValueError as e:\n    #     print(f\"Error processing {filename}: {e}\")\n\n\n    LH = dataall[:, 11:43:7]\n    RH = dataall[:, 12:43:7]\n    LL = dataall[:, 13:43:7]\n    RL = dataall[:, 14:43:7]\n\n    LH = LH[2:2404, :]\n    RH = RH[2:2404, :]\n    LL = LL[2:2404, :]\n    RL = RL[2:2404, :]\n\n\n\n    return LH, RH, LL, RL\n\n\ndef reshape_data(data, target_shape=(2400, 5)):\n    if data.shape[0] < target_shape[0]:\n\n        padded = np.zeros(target_shape)\n        padded[:data.shape[0], :data.shape[1]] = data\n        return padded\n    elif data.shape[0] > target_shape[0]:\n\n        return data[:target_shape[0], :target_shape[1]]\n    return data\n\n# folder_path = input(\"Enter the path of the folder containing Excel files: \")\nfolder_path = '/kaggle/input/datanew/data2'\n\n# folder_path = filedialog.askdirectory(title=\"Select a folder containing Excel files\")\ndata_dictP = {\n    'A': {\n        'LH': [],\n        'RH': [],\n        'LL': [],\n        'RL': [],\n    },\n    'B': {\n        'LH': [],\n        'RH': [],\n        'LL': [],\n        'RL': [],\n    },\n}\n\ndata_dictNP = {\n    'A': {\n        'LH': [],\n        'RH': [],\n        'LL': [],\n        'RL': [],\n    },\n    'B': {\n        'LH': [],\n        'RH': [],\n        'LL': [],\n        'RL': [],\n    },\n}\nprint(folder_path)\n\n# for filename in os.listdir(folder_path):\n#     if filename.endswith('.xlsx'):\n#         filepath = os.path.join(folder_path, filename)\n#         try:\n#             LH, RH, LL, RL = load_and_extract_data(filepath)\n#\n#             # Additional debug print to catch string values before processing\n#             print(f\"Checking for non-numeric values in {filename}\")\n#             print(\"LH:\", LH)\n#             print(\"RH:\", RH)\n#\n#         except ValueError as e:\n#             print(f\"Error processing {filename}: {e}\")\nfor filename in os.listdir(folder_path):\n    if filename.endswith('.xlsx'):\n        filepath = os.path.join(folder_path, filename)\n        LH, RH, LL, RL = load_and_extract_data(filepath)\n\n        if filename[9] == 'A':\n            if filename[10]=='P':\n                data_dictP['A']['LH'].append(reshape_data(LH[:, :5]))\n                data_dictP['A']['RH'].append(reshape_data(RH[:, :5]))\n                data_dictP['A']['LL'].append(reshape_data(LL[:, :5]))\n                data_dictP['A']['RL'].append(reshape_data(RL[:, :5]))\n            else:\n                data_dictNP['A']['LH'].append(reshape_data(LH[:, :5]))\n                data_dictNP['A']['RH'].append(reshape_data(RH[:, :5]))\n                data_dictNP['A']['LL'].append(reshape_data(LL[:, :5]))\n                data_dictNP['A']['RL'].append(reshape_data(RL[:, :5]))\n\n        elif filename[9] == 'B':\n            if filename[10]=='P':\n                data_dictP['B']['LH'].append(reshape_data(LH[:, :5]))\n                data_dictP['B']['RH'].append(reshape_data(RH[:, :5]))\n                data_dictP['B']['LL'].append(reshape_data(LL[:, :5]))\n                data_dictP['B']['RL'].append(reshape_data(RL[:, :5]))\n            else:\n                data_dictNP['B']['LH'].append(reshape_data(LH[:, :5]))\n                data_dictNP['B']['RH'].append(reshape_data(RH[:, :5]))\n                data_dictNP['B']['LL'].append(reshape_data(LL[:, :5]))\n                data_dictNP['B']['RL'].append(reshape_data(RL[:, :5]))\n\n\nfor key in data_dictP:\n    for sub_key in data_dictP[key]:\n        if data_dictP[key][sub_key]:\n            data_dictP[key][sub_key] = np.stack(data_dictP[key][sub_key])\n        else:\n            data_dictP[key][sub_key] = np.zeros((0, 2400, 5))\n\n\ndef pad_array_to_shape(arr, target_shape):\n    arr = np.asarray(arr)\n    if arr.dtype.kind in {'U', 'S', 'O'}:\n        arr = np.where(arr == ' ', 0, arr)\n        arr = arr.astype(float)\n    current_shape = arr.shape\n    padded_array = np.zeros(target_shape)\n    rows = min(current_shape[0], target_shape[0])\n    cols = min(current_shape[1], target_shape[1])\n    padded_array[:rows, :cols] = arr[:rows, :cols]\n\n    return padded_array\n\ntarget_shape = (2400, 5)\n\nfor key in data_dictNP:\n    for sub_key in data_dictNP[key]:\n        if data_dictNP[key][sub_key]:\n            padded_arrays = [pad_array_to_shape(arr, target_shape) for arr in data_dictNP[key][sub_key]]\n            data_dictNP[key][sub_key] = np.stack(padded_arrays)\n        else:\n            data_dictNP[key][sub_key] = np.zeros((0, *target_shape))\n\n# for key in data_dictNP:\n#     for sub_key in data_dictNP[key]:\n#         if data_dictNP[key][sub_key]:\n#             data_dictNP[key][sub_key] = np.stack(data_dictNP[key][sub_key])\n#         else:\n#             data_dictNP[key][sub_key] = np.zeros((0, 2400, 5))\n\n# for key in data_dictNP:\n#     for sub_key in data_dictNP[key]:\n#         try:\n#             if data_dictNP[key][sub_key]:\n#                 data_dictNP[key][sub_key] = np.stack(data_dictNP[key][sub_key])\n#             else:\n#                 data_dictNP[key][sub_key] = np.zeros((0, 2400, 5))\n#         except ValueError as e:\n#             print(f\"Error occurred in key: {key}, sub_key: {sub_key}\")\n#             # print(f\"Data: {data_dictNP[key][sub_key]}\")\n#             print(f\"Error message: {e}\")\n\n\nLH_all_pre_P = 1000*data_dictP['B']['LH']\nRH_all_pre_P = 1000*data_dictP['B']['RH']\n# LL_all_pre_P = data_dictP['B']['LL']\n# RL_all_pre_P = data_dictP['B']['RL']\n\nLH_all_post_P = 1000*data_dictP['A']['LH']\nRH_all_post_P = 1000*data_dictP['A']['RH']\n# LL_all_post_P = data_dictP['A']['LL']\n# RL_all_post_P = data_dictP['A']['RL']\n\n\nLH_all_pre_NP = 1000*data_dictNP['B']['LH']\nRH_all_pre_NP = 1000*data_dictNP['B']['RH']\n# LL_all_pre_NP = data_dictNP['B']['LL']\n# RL_all_pre_NP = data_dictNP['B']['RL']\n\nLH_all_post_NP = 1000*data_dictNP['A']['LH']\nRH_all_post_NP = 1000*data_dictNP['A']['RH']\n# LL_all_post_NP = data_dictNP['A']['LL']\n# RL_all_post_NP = data_dictNP['A']['RL']\n\n\n\n\n# means_LH_all_pre_P, std_devs_LH_all_pre_P = calculate_statistics(LH_all_pre_P)\n# means_LH_all_pre_NP, std_devs_LH_all_pre_NP = calculate_statistics(LH_all_pre_NP)\n# means_RH_all_pre_P, std_devs_RH_all_pre_P = calculate_statistics(RH_all_pre_P)\n# means_RH_all_pre_NP, std_devs_RH_all_pre_NP = calculate_statistics(RH_all_pre_NP)\n# means_LH_all_post_P, std_devs_LH_all_post_P = calculate_statistics(LH_all_post_P)\n# means_LH_all_post_NP, std_devs_LH_all_post_NP = calculate_statistics(LH_all_post_NP)\n# means_RH_all_post_P, std_devs_RH_all_post_P = calculate_statistics(RH_all_post_P)\n# means_RH_all_post_NP, std_devs_RH_all_post_NP = calculate_statistics(RH_all_post_NP)\n\n# means_LH_all_pre_P = np.mean(flattened_LH_all_pre_P, axis=1)\n# means_LH_all_pre_NP = np.mean(flattened_LH_all_pre_NP, axis=1)\n# means_RH_all_pre_P = np.mean(flattened_RH_all_pre_P, axis=1)\n# means_RH_all_pre_NP = np.mean(flattened_RH_all_pre_NP, axis=1)\n# means_LH_all_post_P = np.mean(flattened_LH_all_post_P, axis=1)\n# means_LH_all_post_NP = np.mean(flattened_LH_all_post_NP, axis=1)\n# means_RH_all_post_P = np.mean(flattened_RH_all_post_P, axis=1)\n# means_RH_all_post_NP = np.mean(flattened_RH_all_post_NP, axis=1)\n# print(means_LH_all_pre_NP)\n#\n# for i in range(flattened_LH_all_pre_P.shape[0]):\n#     flattened_LH_all_pre_P[i, :] -= means_LH_all_pre_P[i]\n#\n# for i in range(flattened_LH_all_pre_NP.shape[0]):\n#     flattened_LH_all_pre_NP[i, :] -= means_LH_all_pre_NP[i]\n#\n# for i in range(flattened_RH_all_pre_P.shape[0]):\n#     flattened_RH_all_pre_P[i, :] -= means_RH_all_pre_P[i]\n#\n# for i in range(flattened_RH_all_pre_NP.shape[0]):\n#     flattened_RH_all_pre_NP[i, :] -= means_RH_all_pre_NP[i]\n#\n# for i in range(flattened_LH_all_post_P.shape[0]):\n#     flattened_LH_all_post_P[i, :] -= means_LH_all_post_P[i]\n#\n# for i in range(flattened_LH_all_post_NP.shape[0]):\n#     flattened_LH_all_post_NP[i, :] -= means_LH_all_post_NP[i]\n#\n# for i in range(flattened_RH_all_post_P.shape[0]):\n#     flattened_RH_all_post_P[i, :] -= means_RH_all_post_P[i]\n#\n# for i in range(flattened_RH_all_post_NP.shape[0]):\n#     flattened_RH_all_post_NP[i, :] -= means_RH_all_post_NP[i]\n#\n# deviation_pre_P = np.vstack((flattened_LH_all_pre_P, flattened_RH_all_pre_P))\n# deviation_pre_NP = np.vstack((flattened_LH_all_pre_NP, flattened_RH_all_pre_NP))\n# deviation_post_P = np.vstack((flattened_LH_all_post_P, flattened_RH_all_post_P))\n# deviation_post_NP = np.vstack((flattened_LH_all_post_NP, flattened_RH_all_post_NP))\n# plot_data(deviation_post_P)\n\n# Print the results but datatype np.float\n# print(\"LH Means:\\n\", means_LH)\n# print(\"LH Standard Deviations:\\n\", std_devs_LH)\n# print(\"RH Means:\\n\", means_RH)\n# print(\"RH Standard Deviations:\\n\", std_devs_RH)\n\n\n# print(\"LH Means:\")\n# print([float(mean) for mean in means_LH])\n# print(\"LH Standard Deviations:\")\n# print([float(std_dev) for std_dev in std_devs_LH])\n# print(\"RH Means:\")\n# print([float(mean) for mean in means_RH])\n# print(\"RH Standard Deviations:\")\n# print([float(std_dev) for std_dev in std_devs_RH])\n\n# LH_all_pre_normalised=flatten_3d_to_2d(LH_all_pre)\n# LH_all_pre_raw=LH_all_pre_normalised\n# for i in range(0,len(means_LH_all_pre)):\n#   LH_all_pre_normalised[i]=z_normalize(LH_all_pre_normalised[i],means_LH_all_pre[i],std_devs_LH_all_pre[i])\n#\n# RH_all_pre_normalised=flatten_3d_to_2d(RH_all_pre)\n# RH_all_pre_raw=RH_all_pre_normalised\n# for i in range(0,len(means_RH_all_pre)):\n#   RH_all_pre_normalised[i]=z_normalize(RH_all_pre_normalised[i],means_RH_all_pre[i],std_devs_RH_all_pre[i])\n#\n# LH_all_post_normalised=flatten_3d_to_2d(LH_all_post)\n# LH_all_post_raw=LH_all_post_normalised\n# for i in range(0,len(means_LH_all_post)):\n#   LH_all_post_normalised[i]=z_normalize(LH_all_post_normalised[i],means_LH_all_post[i],std_devs_LH_all_post[i])\n#\n# RH_all_post_normalised=flatten_3d_to_2d(RH_all_post)\n# RH_all_post_raw=RH_all_post_normalised\n# for i in range(0,len(means_RH_all_post)):\n#   RH_all_post_normalised[i]=z_normalize(RH_all_post_normalised[i],means_RH_all_post[i],std_devs_RH_all_post[i])\n#\n#\n#\n#\n# normalised_variances_pre_LH = []\n# normalised_kurtoses_pre_LH = []\n# for array in LH_all_pre_normalised:\n#     var, kurt = process_batches_for_normalised(array)\n#     normalised_variances_pre_LH.append(var)\n#     normalised_kurtoses_pre_LH.append(kurt)\n# normalised_variances_pre_LH = np.array(normalised_variances_pre_LH)\n# normalised_kurtoses_pre_LH = np.array(normalised_kurtoses_pre_LH)\n#\n#\n# normalised_variances_pre_RH = []\n# normalised_kurtoses_pre_RH = []\n# for array in RH_all_pre_normalised:\n#     var, kurt = process_batches_for_normalised(array)\n#     normalised_variances_pre_RH.append(var)\n#     normalised_kurtoses_pre_RH.append(kurt)\n# normalised_variances_pre_RH = np.array(normalised_variances_pre_RH)\n# normalised_kurtoses_pre_RH = np.array(normalised_kurtoses_pre_RH)\n#\n#\n# raw_means_pre_LH=[]\n# raw_stddev_pre_LH=[]\n# raw_variance_pre_LH=[]\n# raw_skewness_pre_LH=[]\n# for array in LH_all_pre_raw:\n#     mean, std, var, skew = process_batches_raw(array)\n#     raw_means_pre_LH.append(mean)\n#     raw_stddev_pre_LH.append(std)\n#     raw_variance_pre_LH.append(var)\n#     raw_skewness_pre_LH.append(skew)\n# raw_means_pre_LH = np.array(raw_means_pre_LH)\n# raw_stddev_pre_LH = np.array(raw_stddev_pre_LH)\n# raw_variance_pre_LH=np.array(raw_variance_pre_LH)\n# raw_skewness_pre_LH=np.array(raw_skewness_pre_LH)\n#\n#\n# raw_means_pre_RH=[]\n# raw_stddev_pre_RH=[]\n# raw_variance_pre_RH=[]\n# raw_skewness_pre_RH=[]\n# for array in RH_all_pre_raw:\n#     mean, std, var, skew = process_batches_raw(array)\n#     raw_means_pre_RH.append(mean)\n#     raw_stddev_pre_RH.append(std)\n#     raw_variance_pre_RH.append(var)\n#     raw_skewness_pre_RH.append(skew)\n# raw_means_pre_RH = np.array(raw_means_pre_RH)\n# raw_stddev_pre_RH = np.array(raw_stddev_pre_RH)\n# raw_variance_pre_RH=np.array(raw_variance_pre_RH)\n# raw_skewness_pre_RH=np.array(raw_skewness_pre_RH)\n#\n#\n#\n# normalised_variances_post_LH = []\n# normalised_kurtoses_post_LH = []\n# for array in LH_all_post_normalised:\n#     var, kurt = process_batches_for_normalised(array)\n#     normalised_variances_post_LH.append(var)\n#     normalised_kurtoses_post_LH.append(kurt)\n# normalised_variances_post_LH = np.array(normalised_variances_post_LH)\n# normalised_kurtoses_post_LH = np.array(normalised_kurtoses_post_LH)\n#\n#\n# normalised_variances_post_RH = []\n# normalised_kurtoses_post_RH = []\n# for array in RH_all_post_normalised:\n#     var, kurt = process_batches_for_normalised(array)\n#     normalised_variances_post_RH.append(var)\n#     normalised_kurtoses_post_RH.append(kurt)\n# normalised_variances_post_RH = np.array(normalised_variances_post_RH)\n# normalised_kurtoses_post_RH = np.array(normalised_kurtoses_post_RH)\n#\n#\n# raw_means_post_LH=[]\n# raw_stddev_post_LH=[]\n# raw_variance_post_LH=[]\n# raw_skewness_post_LH=[]\n# for array in LH_all_post_raw:\n#     mean, std, var, skew = process_batches_raw(array)\n#     raw_means_post_LH.append(mean)\n#     raw_stddev_post_LH.append(std)\n#     raw_variance_post_LH.append(var)\n#     raw_skewness_post_LH.append(skew)\n# raw_means_post_LH = np.array(raw_means_post_LH)\n# raw_stddev_post_LH = np.array(raw_stddev_post_LH)\n# raw_variance_post_LH=np.array(raw_variance_post_LH)\n# raw_skewness_post_LH=np.array(raw_skewness_post_LH)\n#\n#\n# raw_means_post_RH=[]\n# raw_stddev_post_RH=[]\n# raw_variance_post_RH=[]\n# raw_skewness_post_RH=[]\n# for array in RH_all_post_raw:\n#     mean, std, var, skew = process_batches_raw(array)\n#     raw_means_post_RH.append(mean)\n#     raw_stddev_post_RH.append(std)\n#     raw_variance_post_RH.append(var)\n#     raw_skewness_post_RH.append(skew)\n# raw_means_post_RH = np.array(raw_means_post_RH)\n# raw_stddev_post_RH = np.array(raw_stddev_post_RH)\n# raw_variance_post_RH=np.array(raw_variance_post_RH)\n# raw_skewness_post_RH=np.array(raw_skewness_post_RH)","metadata":{"execution":{"iopub.status.busy":"2025-01-15T08:07:17.050804Z","iopub.execute_input":"2025-01-15T08:07:17.052144Z","iopub.status.idle":"2025-01-15T08:08:13.941864Z","shell.execute_reply.started":"2025-01-15T08:07:17.052084Z","shell.execute_reply":"2025-01-15T08:08:13.940756Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/datanew/data2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# RAW DATA PLOTS****","metadata":{}},{"cell_type":"code","source":"def plot_data(matrix, save_path):\n    sns.set(style=\"whitegrid\")\n    matrix = np.array(matrix)\n\n    plt.figure(figsize=(20, 10))\n#     palette = sns.color_palette(\"husl\", matrix.shape[0])\n    for i in range(matrix.shape[0]):\n        plt.plot(matrix[i, :],color='blue', linewidth=1.0)\n    plt.xlabel('TIME', fontsize=14)\n    plt.ylabel('AMPLITUDE', fontsize=14)\n    plt.ylim(-20,15)\n    plt.savefig(save_path, format='jpg', dpi=300)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-11T19:04:56.264776Z","iopub.execute_input":"2024-11-11T19:04:56.265308Z","iopub.status.idle":"2024-11-11T19:04:56.273997Z","shell.execute_reply.started":"2024-11-11T19:04:56.265266Z","shell.execute_reply":"2024-11-11T19:04:56.272698Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"flattened_LH_all_pre_P= flatten_3d_to_2d_col(LH_all_pre_P)\nflattened_LH_all_pre_NP= flatten_3d_to_2d_col(LH_all_pre_NP)\nflattened_RH_all_pre_P= flatten_3d_to_2d_col(RH_all_pre_P)\nflattened_RH_all_pre_NP= flatten_3d_to_2d_col(RH_all_pre_NP)\nflattened_LH_all_post_P= flatten_3d_to_2d_col(LH_all_post_P)\nflattened_LH_all_post_NP= flatten_3d_to_2d_col(LH_all_post_NP)\nflattened_RH_all_post_P= flatten_3d_to_2d_col(RH_all_post_P)\nflattened_RH_all_post_NP= flatten_3d_to_2d_col(RH_all_post_NP)","metadata":{"execution":{"iopub.status.busy":"2025-01-15T08:08:14.012405Z","iopub.execute_input":"2025-01-15T08:08:14.013328Z","iopub.status.idle":"2025-01-15T08:08:14.039851Z","shell.execute_reply.started":"2025-01-15T08:08:14.013277Z","shell.execute_reply":"2025-01-15T08:08:14.038829Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"plot_data(flattened_LH_all_pre_P,'LH_all_pre_P_new.jpg')\nplot_data(flattened_RH_all_pre_P,'RH_all_pre_P_new.jpg')\nplot_data(flattened_LH_all_post_P,'LH_all_post_P_new.jpg')\nplot_data(flattened_RH_all_post_P,'RH_all_post_P_new.jpg')\nplot_data(flattened_LH_all_pre_NP,'LH_all_pre_NP_new.jpg')\nplot_data(flattened_RH_all_pre_NP,'RH_all_pre_NP_new.jpg')\nplot_data(flattened_LH_all_post_NP,'LH_all_post_NP_new.jpg')\nplot_data(flattened_RH_all_post_NP,'RH_all_post_NP_new.jpg')","metadata":{"execution":{"iopub.status.busy":"2024-11-11T19:04:56.313355Z","iopub.execute_input":"2024-11-11T19:04:56.313757Z","iopub.status.idle":"2024-11-11T19:05:07.490752Z","shell.execute_reply.started":"2024-11-11T19:04:56.313717Z","shell.execute_reply":"2024-11-11T19:05:07.489404Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DEVIATION PLOTS****","metadata":{}},{"cell_type":"code","source":"def plot_data(matrix, save_path):\n    sns.set(style=\"whitegrid\")\n    matrix = np.array(matrix)\n\n    plt.figure(figsize=(20, 10))\n#     palette = sns.color_palette(\"husl\", matrix.shape[0])\n    for i in range(matrix.shape[0]):\n        plt.plot(matrix[i, :],color='blue', linewidth=1.0)\n    plt.xlabel('TIME', fontsize=14)\n    plt.ylabel('AMPLITUDE', fontsize=14)\n    plt.ylim(-8,8)\n    plt.savefig(save_path, format='jpg', dpi=300)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T19:05:07.492358Z","iopub.execute_input":"2024-11-11T19:05:07.492835Z","iopub.status.idle":"2024-11-11T19:05:07.503059Z","shell.execute_reply.started":"2024-11-11T19:05:07.492778Z","shell.execute_reply":"2024-11-11T19:05:07.501824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_deviations(LH_all_pre_P, RH_all_pre_P):\n    # Initialize output arrays with the same shapes\n    LH_all_pre_P_dev = np.zeros_like(LH_all_pre_P)\n    RH_all_pre_P_dev = np.zeros_like(RH_all_pre_P)\n\n    # Loop through each 2D array (along the first axis) for LH_all_pre_P\n    for i in range(LH_all_pre_P.shape[0]):\n        mean_values = np.mean(LH_all_pre_P[i], axis=0)\n        LH_all_pre_P_dev[i] = LH_all_pre_P[i] - mean_values\n\n    # Loop through each 2D array (along the first axis) for RH_all_pre_P\n    for i in range(RH_all_pre_P.shape[0]):\n        mean_values = np.mean(RH_all_pre_P[i], axis=0)\n        RH_all_pre_P_dev[i] = RH_all_pre_P[i] - mean_values\n    flattened_LH_all_pre_P= flatten_3d_to_2d_col(LH_all_pre_P_dev)\n    flattened_RH_all_pre_P= flatten_3d_to_2d_col(RH_all_pre_P_dev)\n    deviation_pre_P = np.vstack((flattened_LH_all_pre_P, flattened_RH_all_pre_P))\n\n    return deviation_pre_P\n","metadata":{"execution":{"iopub.status.busy":"2024-11-11T19:05:07.504713Z","iopub.execute_input":"2024-11-11T19:05:07.505209Z","iopub.status.idle":"2024-11-11T19:05:07.515663Z","shell.execute_reply.started":"2024-11-11T19:05:07.505134Z","shell.execute_reply":"2024-11-11T19:05:07.514468Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"deviation_pre_P=calculate_deviations(LH_all_pre_P, RH_all_pre_P)\ndeviation_post_P=calculate_deviations(LH_all_post_P, RH_all_post_P)\ndeviation_pre_NP=calculate_deviations(LH_all_pre_NP, RH_all_pre_NP)\ndeviation_post_NP=calculate_deviations(LH_all_post_NP, RH_all_post_NP)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-11T19:05:07.519935Z","iopub.execute_input":"2024-11-11T19:05:07.52038Z","iopub.status.idle":"2024-11-11T19:05:07.63798Z","shell.execute_reply.started":"2024-11-11T19:05:07.520328Z","shell.execute_reply":"2024-11-11T19:05:07.636755Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_data(deviation_pre_P,'deviation_pre_P_new.jpg')\nplot_data(deviation_post_P,'deviation_post_P_new.jpg')\nplot_data(deviation_pre_NP,'deviation_pre_NP_new.jpg')\nplot_data(deviation_post_NP,'deviation_post_NP_new.jpg')","metadata":{"execution":{"iopub.status.busy":"2024-11-11T19:05:07.639539Z","iopub.execute_input":"2024-11-11T19:05:07.639946Z","iopub.status.idle":"2024-11-11T19:05:15.901023Z","shell.execute_reply.started":"2024-11-11T19:05:07.639907Z","shell.execute_reply":"2024-11-11T19:05:15.899681Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CSV OF MEANS (190)****","metadata":{}},{"cell_type":"code","source":"def median_of_means(LH_all_pre_P, RH_all_pre_P):\n\n    combined_data = np.concatenate((LH_all_pre_P, RH_all_pre_P), axis=0)\n\n    #Number of states (columns)\n\n    num_states = combined_data.shape[2] # Assuming 5 states as columns \n    means_per_state = []\n\n    # Calculate mean of each column in each 2D array\n\n    for state in range(num_states): \n        means_for_state = []\n\n        for matrix in combined_data: \n            means_for_state.append(np.mean(matrix[:, state])) # Mean of each column for state \n        means_per_state.append(means_for_state)\n\n    return means_per_state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T19:05:15.902644Z","iopub.execute_input":"2024-11-11T19:05:15.903094Z","iopub.status.idle":"2024-11-11T19:05:15.912302Z","shell.execute_reply.started":"2024-11-11T19:05:15.90304Z","shell.execute_reply":"2024-11-11T19:05:15.911175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x=median_of_means(LH_all_pre_P, RH_all_pre_P)\n# (np.array(x)).tofile('p_pre.csv', sep=',')\n# x=median_of_means(LH_all_post_P, RH_all_post_P)\n# (np.array(x)).tofile('p_post.csv', sep=',')\n# x=median_of_means(LH_all_post_NP, RH_all_post_NP)\n# (np.array(x)).tofile('np_post.csv', sep=',')\n# x=median_of_means(LH_all_pre_NP, RH_all_pre_NP)\n# (np.array(x)).tofile('np_pre.csv', sep=',')","metadata":{"execution":{"iopub.status.busy":"2024-11-11T19:05:15.913928Z","iopub.execute_input":"2024-11-11T19:05:15.914371Z","iopub.status.idle":"2024-11-11T19:05:15.924924Z","shell.execute_reply.started":"2024-11-11T19:05:15.914323Z","shell.execute_reply":"2024-11-11T19:05:15.9237Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CSV OF STDS****","metadata":{}},{"cell_type":"code","source":"def median_of_stds(LH_all_pre_P, RH_all_pre_P):\n\n    combined_data = np.concatenate((LH_all_pre_P, RH_all_pre_P), axis=0)\n\n    #Number of states (columns)\n\n    num_states = combined_data.shape[2] # Assuming 5 states as columns \n    means_per_state = []\n\n    # Calculate mean of each column in each 2D array\n\n    for state in range(num_states): \n        means_for_state = []\n\n        for matrix in combined_data: \n            means_for_state.append(np.std(matrix[:, state])) # Mean of each column for state \n        means_per_state.append(means_for_state)\n\n    return means_per_state","metadata":{"execution":{"iopub.status.busy":"2024-11-11T19:05:15.926706Z","iopub.execute_input":"2024-11-11T19:05:15.927073Z","iopub.status.idle":"2024-11-11T19:05:15.943131Z","shell.execute_reply.started":"2024-11-11T19:05:15.927023Z","shell.execute_reply":"2024-11-11T19:05:15.941619Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x=median_of_stds(LH_all_pre_P, RH_all_pre_P)\n# (np.array(x)).tofile('p_pre_std.csv', sep=',')\n# x=median_of_stds(LH_all_post_P, RH_all_post_P)\n# (np.array(x)).tofile('p_post_std.csv', sep=',')\n# x=median_of_stds(LH_all_post_NP, RH_all_post_NP)\n# (np.array(x)).tofile('np_post_std.csv', sep=',')\n# x=median_of_stds(LH_all_pre_NP, RH_all_pre_NP)\n# (np.array(x)).tofile('np_pre_std.csv', sep=',')","metadata":{"execution":{"iopub.status.busy":"2024-11-11T19:05:15.944963Z","iopub.execute_input":"2024-11-11T19:05:15.945809Z","iopub.status.idle":"2024-11-11T19:05:15.956284Z","shell.execute_reply.started":"2024-11-11T19:05:15.945758Z","shell.execute_reply":"2024-11-11T19:05:15.955092Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# BOX PLOTS OF MEAN****","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.special import erfcinv\n\ndef plot_median_of_mean(LH_all_pre_P, RH_all_pre_P, save_path):\n    combined_data = np.concatenate((LH_all_pre_P, RH_all_pre_P), axis=0)\n    \n    # Number of states (columns)\n    num_states = combined_data.shape[2]  # Assuming states are represented as columns\n    stds_per_state = []\n\n    # Calculate standard deviation of each column in each 2D array for each state\n    for state in range(num_states): \n        stds_for_state = []\n        for matrix in combined_data: \n            stds_for_state.append(np.mean(matrix[:, state]))  # Standard deviation for each state\n        stds_per_state.append(stds_for_state)\n\n    # Prepare data for boxplot\n    data_for_boxplot = [stds_per_state[state] for state in range(num_states)]\n\n    # Calculate outliers based on scaled MAD\n    c = -1 / (np.sqrt(2) * erfcinv(3 / 2))  # Scaling constant\n    outliers_per_state = []\n    \n    for state_data in data_for_boxplot:\n        median = np.median(state_data)\n        mad = np.median(np.abs(state_data - median))  # Median Absolute Deviation\n        scaled_mad = c * mad\n        \n        lower_bound = median - 3 * scaled_mad\n        upper_bound = median + 3 * scaled_mad\n        \n        # Store the outliers for each state, excluding those within the range (-16, 12) as normal\n        outliers = [\n            x for x in state_data \n            if (x < lower_bound or x > upper_bound)\n        ]\n        outliers_per_state.append(outliers)\n\n    # Plotting\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(data=data_for_boxplot, flierprops=dict(marker='+', markersize=10), whis=[0, 100])  # Include all data points\n#     sns.boxplot(data=data_for_boxplot, flierprops=dict(marker='+', markersize=10))\n    \n    # Manually plot outliers calculated above\n    for state in range(num_states):\n        plt.scatter([state] * len(outliers_per_state[state]), outliers_per_state[state], color='red', label='Outliers' if state == 0 else \"\", marker='o')\n\n    plt.xticks(ticks=np.arange(num_states), labels=[f'State {i+1}' for i in range(num_states)])\n    plt.ylabel('Mean')\n    plt.grid(True)\n    plt.ylim(-16, 12)\n    plt.savefig(save_path, format='jpg', dpi=300)\n    plt.show()\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-11T19:05:15.958255Z","iopub.execute_input":"2024-11-11T19:05:15.95883Z","iopub.status.idle":"2024-11-11T19:05:15.976776Z","shell.execute_reply.started":"2024-11-11T19:05:15.958777Z","shell.execute_reply":"2024-11-11T19:05:15.975715Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_median_of_mean(LH_all_pre_P, RH_all_pre_P, 'mean_pre_P_new.jpg')\nplot_median_of_mean(LH_all_post_P, RH_all_post_P, 'mean_post_P_new.jpg')\nplot_median_of_mean(LH_all_post_NP, RH_all_post_NP, 'mean_post_NP_new.jpg')\nplot_median_of_mean(LH_all_pre_NP, RH_all_pre_NP, 'mean_pre_NP_new.jpg')","metadata":{"execution":{"iopub.status.busy":"2024-11-11T19:05:15.978405Z","iopub.execute_input":"2024-11-11T19:05:15.979196Z","iopub.status.idle":"2024-11-11T19:05:18.146933Z","shell.execute_reply.started":"2024-11-11T19:05:15.979117Z","shell.execute_reply":"2024-11-11T19:05:18.14557Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# BOX PLOTS OF STDS****","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.special import erfcinv\n\ndef plot_median_of_std(LH_all_pre_P, RH_all_pre_P, save_path):\n    combined_data = np.concatenate((LH_all_pre_P, RH_all_pre_P), axis=0)\n    \n    # Number of states (columns)\n    num_states = combined_data.shape[2]  # Assuming states are represented as columns\n    stds_per_state = []\n\n    # Calculate standard deviation of each column in each 2D array for each state\n    for state in range(num_states): \n        stds_for_state = []\n        for matrix in combined_data: \n            stds_for_state.append(np.std(matrix[:, state]))  # Standard deviation for each state\n        stds_per_state.append(stds_for_state)\n\n    # Prepare data for boxplot\n    data_for_boxplot = [stds_per_state[state] for state in range(num_states)]\n\n    # Calculate outliers based on scaled MAD\n    c = -1 / (np.sqrt(2) * erfcinv(3 / 2))  # Scaling constant\n    outliers_per_state = []\n    \n    for state_data in data_for_boxplot:\n        median = np.median(state_data)\n        mad = np.median(np.abs(state_data - median))  # Median Absolute Deviation\n        scaled_mad = c * mad\n        \n        lower_bound = median - 3 * scaled_mad\n        upper_bound = median + 3 * scaled_mad\n        \n        # Store the outliers for each state\n        outliers = [x for x in state_data if x < lower_bound or x > upper_bound]\n        outliers_per_state.append(outliers)\n\n    # Plotting\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(data=data_for_boxplot, flierprops=dict(marker='+', markersize=10))\n    \n    # Manually plot outliers calculated above\n#     for state in range(num_states):\n#         plt.scatter([state] * len(outliers_per_state[state]), outliers_per_state[state], label='Outliers' if state == 0 else \"\")\n\n    plt.xticks(ticks=np.arange(num_states), labels=[f'State {i+1}' for i in range(num_states)])\n    plt.ylabel('SD')\n    plt.grid(True)\n    plt.ylim(0, 5)\n    plt.savefig(save_path, format='jpg', dpi=300)  # Save the figure\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-11T19:05:18.150519Z","iopub.execute_input":"2024-11-11T19:05:18.151258Z","iopub.status.idle":"2024-11-11T19:05:18.171421Z","shell.execute_reply.started":"2024-11-11T19:05:18.151194Z","shell.execute_reply":"2024-11-11T19:05:18.169879Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_median_of_std(LH_all_pre_P, RH_all_pre_P, 'std_pre_P_new.jpg')\nplot_median_of_std(LH_all_pre_NP, RH_all_pre_NP, 'std_pre_NP_new.jpg')\nplot_median_of_std(LH_all_post_P, RH_all_post_P, 'std_post_P_new.jpg')\nplot_median_of_std(LH_all_post_NP, RH_all_post_NP, 'std_post_NP_new.jpg')","metadata":{"execution":{"iopub.status.busy":"2024-11-11T19:05:18.173135Z","iopub.execute_input":"2024-11-11T19:05:18.174212Z","iopub.status.idle":"2024-11-11T19:05:20.664447Z","shell.execute_reply.started":"2024-11-11T19:05:18.174135Z","shell.execute_reply":"2024-11-11T19:05:20.663215Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SELECTED LINES PLOTTING****","metadata":{}},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import numpy as np\n\n# # Function to plot based on selected lines\n# def plot_selected_lines(matrix, selected_lines):\n#     matrix = np.array(matrix)\n#     plt.figure(figsize=(10, 5))\n    \n#     for i in selected_lines:\n#         plt.plot(matrix[i, :], label=f'Line {i+1}')\n    \n#     plt.title('Selected Lines')\n#     plt.xlabel('X-axis')\n#     plt.ylabel('Y-axis')\n#     plt.legend()\n#     plt.show()\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-11T19:05:20.665868Z","iopub.execute_input":"2024-11-11T19:05:20.666247Z","iopub.status.idle":"2024-11-11T19:05:20.671625Z","shell.execute_reply.started":"2024-11-11T19:05:20.666207Z","shell.execute_reply":"2024-11-11T19:05:20.670368Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initial plot of all lines\n# plot_selected_lines(matrix, list(range(matrix.shape[0])))\n\n# # Loop to allow user to select lines until they wish to stop\n# while True:\n#     # Simulating user input for line selection\n#     selected_lines = input(\"Enter the indices of lines to plot (e.g., 0, 2, 3): \")\n#     selected_lines = [int(i) for i in selected_lines.split(\",\")]\n    \n#     # Re-plot based on user selection\n#     plot_selected_lines(matrix, selected_lines)\n    \n#     # Ask the user if they wish to continue\n#     continue_plotting = input(\"Do you wish to continue? (yes/no): \").strip().lower()\n    \n#     # Check if user input means 'no', if yes, break the loop\n#     if continue_plotting in ['no', 'n']:\n#         print(\"Exiting plotting.\")\n#         break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T19:05:20.673124Z","iopub.execute_input":"2024-11-11T19:05:20.673612Z","iopub.status.idle":"2024-11-11T19:05:20.682929Z","shell.execute_reply.started":"2024-11-11T19:05:20.67357Z","shell.execute_reply":"2024-11-11T19:05:20.681562Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# flattened_LH_all_pre_P= flatten_3d_to_2d_col(LH_all_pre_P)\n# plot_selected_lines(flattened_LH_all_pre_P, list(range(flattened_LH_all_pre_P.shape[0])))\n# while True:\n#     selected_lines = input(\"Enter the indices of lines to plot (e.g., 0, 2, 3): \")\n#     selected_lines = [int(i) for i in selected_lines.split(\",\")]\n#     plot_selected_lines(flattened_LH_all_pre_P, selected_lines)\n#     continue_plotting = input(\"Do you wish to continue? (yes/no): \").strip().lower()\n\n#     # Check if user input means 'no', if yes, break the loop\n#     if continue_plotting in ['no', 'n']:\n#         print(\"Exiting plotting.\")\n#         break\n\n        \n# plot_selected_lines(flattened_LH_all_pre_P)","metadata":{"execution":{"iopub.status.busy":"2024-11-11T19:05:20.684779Z","iopub.execute_input":"2024-11-11T19:05:20.685234Z","iopub.status.idle":"2024-11-11T19:05:20.69501Z","shell.execute_reply.started":"2024-11-11T19:05:20.685189Z","shell.execute_reply":"2024-11-11T19:05:20.693799Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# NEXT ...****","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Function to calculate mean and standard deviation for 1D arrays\ndef calculate_stats(arr):\n    mean = np.mean(arr)\n    std = np.std(arr)\n    return mean, std\n\n# Function to divide a 1D array into sets of 200 points and calculate stats for each set\ndef segment_stats(arr, segment_size=200):\n    segments = [arr[i:i+segment_size] for i in range(0, len(arr), segment_size)]\n    stats = [calculate_stats(segment) for segment in segments if len(segment) == segment_size]\n    return stats\n\n# Compute mean and standard deviation for each 1D array\noverall_stats_LH_all_pre_P = [calculate_stats(row) for row in flattened_LH_all_pre_P]\noverall_stats_LH_all_post_P = [calculate_stats(row) for row in flattened_LH_all_post_P]\noverall_stats_RH_all_pre_P = [calculate_stats(row) for row in flattened_RH_all_pre_P]\noverall_stats_RH_all_post_P = [calculate_stats(row) for row in flattened_RH_all_post_P]\n\noverall_stats_LH_all_pre_NP = [calculate_stats(row) for row in flattened_LH_all_pre_NP]\noverall_stats_LH_all_post_NP = [calculate_stats(row) for row in flattened_LH_all_post_NP]\noverall_stats_RH_all_pre_NP = [calculate_stats(row) for row in flattened_RH_all_pre_NP]\noverall_stats_RH_all_post_NP = [calculate_stats(row) for row in flattened_RH_all_post_NP]\n\n# Compute segment-wise stats for each 1D array\nsegmented_LH_all_pre_P = [segment_stats(row) for row in flattened_LH_all_pre_P]\nsegmented_LH_all_post_P = [segment_stats(row) for row in flattened_LH_all_post_P]\nsegmented_RH_all_pre_P = [segment_stats(row) for row in flattened_RH_all_pre_P]\nsegmented_RH_all_post_P = [segment_stats(row) for row in flattened_RH_all_post_P]\n\nsegmented_LH_all_pre_NP = [segment_stats(row) for row in flattened_LH_all_pre_NP]\nsegmented_LH_all_post_NP = [segment_stats(row) for row in flattened_LH_all_post_NP]\nsegmented_RH_all_pre_NP = [segment_stats(row) for row in flattened_RH_all_pre_NP]\nsegmented_RH_all_post_NP = [segment_stats(row) for row in flattened_RH_all_post_NP]\n\ndef divide_into_segments(arr, segment_size=2400):\n    # Split the array into segments of the specified size\n    segments = [arr[i:i+segment_size] for i in range(0, len(arr), segment_size)]\n    return segments\n\n# Normalize segments for each row in the 2D array\nsegments_LH_all_pre_P = [\n    divide_into_segments(row)\n    for idx, row in enumerate(flattened_LH_all_pre_P)\n]\nsegments_LH_all_post_P = [\n    divide_into_segments(row)\n    for idx, row in enumerate(flattened_LH_all_post_P)\n]\nsegments_RH_all_pre_P = [\n    divide_into_segments(row)\n    for idx, row in enumerate(flattened_RH_all_pre_P)\n]\nsegments_RH_all_post_P = [\n    divide_into_segments(row)\n    for idx, row in enumerate(flattened_RH_all_post_P)\n]\n\nsegments_LH_all_pre_NP = [\n    divide_into_segments(row)\n    for idx, row in enumerate(flattened_LH_all_pre_NP)\n]\nsegments_LH_all_post_NP = [\n    divide_into_segments(row)\n    for idx, row in enumerate(flattened_LH_all_post_NP)\n]\nsegments_RH_all_pre_NP = [\n    divide_into_segments(row)\n    for idx, row in enumerate(flattened_RH_all_pre_NP)\n]\nsegments_RH_all_post_NP = [\n    divide_into_segments(row)\n    for idx, row in enumerate(flattened_RH_all_post_NP)\n]\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:41:48.673767Z","iopub.execute_input":"2025-01-09T09:41:48.674173Z","iopub.status.idle":"2025-01-09T09:41:49.381637Z","shell.execute_reply.started":"2025-01-09T09:41:48.674119Z","shell.execute_reply":"2025-01-09T09:41:49.380206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.stats import skew, kurtosis\nimport numpy as np\n\n\n\n# Function to calculate desired statistics for a single segment\ndef calculate_extended_stats(segment):\n    mean = np.mean(segment)\n    std = np.std(segment)\n    var = np.var(segment)\n    skewness = skew(segment)\n    kurt = kurtosis(segment)\n    return mean, std, var, skewness, kurt\n\n# Calculate statistics for all rows\ndef calculate_row_stats(normalized_row_segments):\n    row_stats = [calculate_extended_stats(segment) for segment in normalized_row_segments]\n    return row_stats\n\nall_rows_stats_LH_pre_P = [\n    calculate_row_stats(row) for row in segments_LH_all_pre_P\n]\nall_rows_stats_LH_post_P = [\n    calculate_row_stats(row) for row in segments_LH_all_post_P\n]\nall_rows_stats_RH_pre_P = [\n    calculate_row_stats(row) for row in segments_RH_all_pre_P\n]\nall_rows_stats_RH_post_P = [\n    calculate_row_stats(row) for row in segments_RH_all_post_P\n]\n\nall_rows_stats_LH_pre_NP = [\n    calculate_row_stats(row) for row in segments_LH_all_pre_NP\n]\nall_rows_stats_LH_post_NP = [\n    calculate_row_stats(row) for row in segments_LH_all_post_NP\n]\nall_rows_stats_RH_pre_NP = [\n    calculate_row_stats(row) for row in segments_RH_all_pre_NP\n]\nall_rows_stats_RH_post_NP = [\n    calculate_row_stats(row) for row in segments_RH_all_post_NP\n]\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:41:52.888513Z","iopub.execute_input":"2025-01-09T09:41:52.888927Z","iopub.status.idle":"2025-01-09T09:41:53.040982Z","shell.execute_reply.started":"2025-01-09T09:41:52.888889Z","shell.execute_reply":"2025-01-09T09:41:53.039459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import csv\n\n# Prepare data for CSV\ncsv_data = [[\"Row\", \"Segment\", \"Mean\", \"Std\", \"Variance\", \"Skewness\", \"Kurtosis\", \"resp\"]]\n\n# Define the mapping for each stats list to the corresponding \"resp\" value\nstats_to_resp = {\n    \"all_rows_stats_LH_pre_P\": \"LH pre Player\",\n    \"all_rows_stats_LH_post_P\": \"LH post Player\",\n    \"all_rows_stats_RH_pre_P\": \"RH pre Player\",\n    \"all_rows_stats_RH_post_P\": \"RH post Player\",\n    \"all_rows_stats_LH_pre_NP\": \"LH pre Non-Player\",\n    \"all_rows_stats_LH_post_NP\": \"LH post Non-Player\",\n    \"all_rows_stats_RH_pre_NP\": \"RH pre Non-Player\",\n    \"all_rows_stats_RH_post_NP\": \"RH post Non-Player\"\n}\n\n# Define all your stats datasets here\nall_stats_dict = {\n    \"all_rows_stats_LH_pre_P\": all_rows_stats_LH_pre_P,\n    \"all_rows_stats_LH_post_P\": all_rows_stats_LH_post_P,\n    \"all_rows_stats_RH_pre_P\": all_rows_stats_RH_pre_P,\n    \"all_rows_stats_RH_post_P\": all_rows_stats_RH_post_P,\n    \"all_rows_stats_LH_pre_NP\": all_rows_stats_LH_pre_NP,\n    \"all_rows_stats_LH_post_NP\": all_rows_stats_LH_post_NP,\n    \"all_rows_stats_RH_pre_NP\": all_rows_stats_RH_pre_NP,\n    \"all_rows_stats_RH_post_NP\": all_rows_stats_RH_post_NP\n}\n\n# Loop through each dataset and add the corresponding \"resp\"\nfor stats_name, stats_data in all_stats_dict.items():\n    resp_value = stats_to_resp[stats_name]\n    \n    # Add data to csv_data with the 'resp' column\n    for row_idx, row_stats in enumerate(stats_data):\n        for seg_idx, stats in enumerate(row_stats):\n            csv_data.append([row_idx + 1, seg_idx + 1] + list(stats) + [resp_value])\n\n# Write to CSV\noutput_csv_path = \"stats_with_all_resps.csv\"\nwith open(output_csv_path, mode=\"w\", newline=\"\") as file:\n    writer = csv.writer(file)\n    writer.writerows(csv_data)\n\nprint(f\"CSV file saved to {output_csv_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:41:38.306984Z","iopub.execute_input":"2025-01-09T09:41:38.307388Z","iopub.status.idle":"2025-01-09T09:41:38.343455Z","shell.execute_reply.started":"2025-01-09T09:41:38.307355Z","shell.execute_reply":"2025-01-09T09:41:38.34198Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\nimport pandas as pd\n\n# Mapping of row numbers to sub-codes\nrow_to_subcode = {\n    1: \"S041\", 2: \"S041\", 3: \"S046\", 4: \"S045\", 5: \"SO44\",\n    6: \"S041\", 7: \"S042\", 8: \"SO49\", 9: \"S057\", 10: \"SO53\",\n    11: \"S055\", 12: \"SO54\", 13: \"S058\", 14: \"S064\"\n}\n\n# Load the CSV file\ninput_csv_path = \"LH_all_pre_stats.csv\"\ndf = pd.read_csv(input_csv_path)\n\n# We assume 60 segments per row. We will replace 'Row' column values in sets of 60\nsub_codes = []\n\n# Iterate through the rows in chunks of 60\nfor i in range(0, len(df), 60):\n    # Find the sub-code for the current row's group\n    row_num = (i // 60) + 1  # Determine the row number based on chunk position\n    sub_code = row_to_subcode.get(row_num, \"Unknown\")\n    \n    # For each 60-row block, assign the corresponding sub-code\n    sub_codes.extend([sub_code] * 60)\n\n# Replace the 'Row' column with the sub-codes for the entire chunk\ndf['Sub-Code'] = sub_codes\ndf['resp'] = \"LH pre Player\"\n\n# Drop the original 'Row' column\ndf.drop(columns=['Row'], inplace=True)\n\n# Save the updated CSV\noutput_csv_path = \"LH_all_pre_stats_with_subcodes.csv\"\ndf.to_csv(output_csv_path, index=False)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:25:53.771104Z","iopub.execute_input":"2025-01-09T09:25:53.771544Z","iopub.status.idle":"2025-01-09T09:25:53.805216Z","shell.execute_reply.started":"2025-01-09T09:25:53.771506Z","shell.execute_reply":"2025-01-09T09:25:53.803961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_rows = len(all_rows_stats)  # Number of rows\nnum_segments_per_row = [len(row) for row in all_rows_stats]  # Number of segments per row\n\nprint(\"Shape of normalized_segments_LH_all_pre:\")\nprint(f\"Rows: {num_rows}\")\nprint(f\"Segments per row: {num_segments_per_row}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:00:23.082697Z","iopub.execute_input":"2025-01-09T09:00:23.083095Z","iopub.status.idle":"2025-01-09T09:00:23.089045Z","shell.execute_reply.started":"2025-01-09T09:00:23.083062Z","shell.execute_reply":"2025-01-09T09:00:23.087646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Function to divide each row into 5 sets of 2400 points each\ndef divide_into_sets(array, num_sets=5, points_per_set=2400):\n    # Check that each row has the correct number of points (num_sets * points_per_set)\n    if array.shape[1] != num_sets * points_per_set:\n        raise ValueError(f\"Each row must have {num_sets * points_per_set} points to divide into {num_sets} sets of {points_per_set} points.\")\n    \n    # Split each row into the specified number of sets\n    sets = []\n    for row in array:\n        row_sets = np.split(row, num_sets)\n        sets.append(row_sets)\n    \n    return np.array(sets)\n\n# Combine arrays row-wise\ncombined_LH_RH_pre_P = np.vstack((flattened_LH_all_pre_P, flattened_RH_all_pre_P))\ncombined_LH_RH_pre_NP = np.vstack((flattened_LH_all_pre_NP, flattened_RH_all_pre_NP))\ncombined_LH_RH_post_P = np.vstack((flattened_LH_all_post_P, flattened_RH_all_post_P))\ncombined_LH_RH_post_NP = np.vstack((flattened_LH_all_post_NP, flattened_RH_all_post_NP))\n\n\n# Divide each row into 5 sets of 2400 points each\ndata_pre_P = divide_into_sets(combined_LH_RH_pre_P)\ndata_pre_NP = divide_into_sets(combined_LH_RH_pre_NP)\ndata_post_P = divide_into_sets(combined_LH_RH_post_P)\ndata_post_NP = divide_into_sets(combined_LH_RH_post_NP)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:09:54.887865Z","iopub.execute_input":"2025-01-15T08:09:54.888822Z","iopub.status.idle":"2025-01-15T08:09:54.941283Z","shell.execute_reply.started":"2025-01-15T08:09:54.888781Z","shell.execute_reply":"2025-01-15T08:09:54.940022Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"combined_LH_RH_pre_P.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:13:16.675499Z","iopub.execute_input":"2025-01-15T08:13:16.675961Z","iopub.status.idle":"2025-01-15T08:13:16.684881Z","shell.execute_reply.started":"2025-01-15T08:13:16.675926Z","shell.execute_reply":"2025-01-15T08:13:16.683508Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(28, 12000)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"import numpy as np\n\ndef zci_and_m(data):\n    \"\"\"\n    Calculate zero-crossing index (zci) and slope (m) for subsets of 2400 points\n    for each row in the input data.\n\n    Parameters:\n        data (numpy.ndarray): Input data with dimensions (num_rows, num_columns).\n\n    Returns:\n        zci_all (numpy.ndarray): Zero-crossing indices for all subsets and rows.\n        m_all (numpy.ndarray): Slopes for all subsets and rows.\n    \"\"\"\n    num_rows, num_columns = data.shape\n    if num_columns % 2400 != 0:\n        raise ValueError(\"Number of columns in each row must be divisible by 2400.\")\n    \n    # Number of subsets per row\n    num_subsets = num_columns // 2400\n\n    # Initialize results\n    zci_all = np.zeros((num_rows, num_subsets))\n    m_all = np.zeros((num_rows, num_subsets))\n    \n    # Process each row\n    for row_idx in range(num_rows):\n        for subset_idx in range(num_subsets):\n            # Extract the subset for the current row\n            start_idx = subset_idx * 2400\n            end_idx = start_idx + 2400\n            subset = data[row_idx, start_idx:end_idx]\n            \n            # Normalize data by subtracting the mean\n            db_data = subset - np.mean(subset)\n            \n            # Find the sign of the db_data values\n            sgn = np.sign(db_data)\n            zero_crossings = []\n            \n            # Detect zero-crossings\n            for i in range(1, db_data.shape[0]):\n                if sgn[i] != sgn[i-1]:\n                    zero_crossings.append(i)\n            \n            # Find the crossing closest to the center\n            if zero_crossings:\n                ix = np.argmin(np.abs(np.array(zero_crossings) - 2400 / 2))\n                zci_all[row_idx, subset_idx] = zero_crossings[ix]\n                \n                # Calculate the numerator and denominator for slope\n                nu = (np.arange(2400) - zero_crossings[ix]) * db_data\n                dn = (np.arange(2400) - zero_crossings[ix]) ** 2\n                \n                # Compute slope\n                m_all[row_idx, subset_idx] = np.sum(nu) / np.sum(dn)\n    \n    # Normalize zci and scale m\n    zci_all /= 1200\n    m_all *= 1200\n    \n    return zci_all, m_all\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:17:19.788523Z","iopub.execute_input":"2025-01-15T08:17:19.788976Z","iopub.status.idle":"2025-01-15T08:17:19.799637Z","shell.execute_reply.started":"2025-01-15T08:17:19.788942Z","shell.execute_reply":"2025-01-15T08:17:19.798453Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"zci_m_results = {}\n\n# Compute zci and m for each combined dataset\nzci_m_results['LH_pre_P'] = zci_and_m(flattened_LH_all_pre_P)\nzci_m_results['RH_pre_P'] = zci_and_m(flattened_RH_all_pre_P)\nzci_m_results['LH_pre_NP'] = zci_and_m(flattened_LH_pre_NP)\nzci_m_results['RH_pre_NP'] = zci_and_m(flattened_RH_pre_NP)\nzci_m_results['LH_post_P'] = zci_and_m(flattened_LH_post_P)\nzci_m_results['RH_post_P'] = zci_and_m(flattened_RH_post_P)\nzci_m_results['LH_post_NP'] = zci_and_m(flattened_LH_post_NP)\nzci_m_results['RH_post_NP'] = zci_and_m(flattened_RH_post_NP)\n\n# Assign results to variables for clarity\nzci_LH_pre_P, m_LH_pre_P = zci_m_results['LH_pre_P']\nzci_RH_pre_P, m_RH_pre_P = zci_m_results['RH_pre_P']\nzci_LH_pre_NP, m_LH_pre_NP = zci_m_results['LH_pre_NP']\nzci_RH_pre_NP, m_RH_pre_NP = zci_m_results['RH_pre_NP']\nzci_LH_post_P, m_LH_post_P = zci_m_results['LH_post_P']\nzci_RH_post_P, m_RH_post_P = zci_m_results['RH_post_P']\nzci_LH_post_NP, m_LH_post_NP = zci_m_results['LH_post_NP']\nzci_RH_post_NP, m_RH_post_NP = zci_m_results['RH_post_NP']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T08:19:46.087031Z","iopub.execute_input":"2025-01-15T08:19:46.087405Z","iopub.status.idle":"2025-01-15T08:19:46.616143Z","shell.execute_reply.started":"2025-01-15T08:19:46.087374Z","shell.execute_reply":"2025-01-15T08:19:46.614913Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import numpy as np\nfrom scipy.signal import welch\ndef pow2db(power):\n    return 10 * np.log10(power)\n\ndef mnf_and_pmnf(data, fs):\n    \"\"\"\n    Calculate Mean Frequency (MNF) for subsets of 2400 points for each row in the input data.\n\n    Parameters:\n        data (numpy.ndarray): Input data with dimensions (num_rows, num_columns).\n        fs (float): Sampling frequency.\n\n    Returns:\n        mnf_all (numpy.ndarray): Mean frequency values for all subsets and rows.\n    \"\"\"\n    num_rows, num_columns = data.shape\n    if num_columns % 2400 != 0:\n        raise ValueError(\"Number of columns in each row must be divisible by 2400.\")\n    \n    # Number of subsets per row\n    num_subsets = num_columns // 2400\n\n    # Initialize results\n    mnf_all = np.zeros((num_rows, num_subsets))\n    pmnf_all = np.zeros((num_rows, num_subsets))\n    \n    \n    # Process each row\n    for row_idx in range(data.shape[0]):\n        for subset_idx in range(num_subsets):  # 5 subsets\n            start_idx = subset_idx * 2400\n            end_idx = start_idx + 2400\n            subset = combined_LH_RH_pre_P[row_idx, start_idx:end_idx]\n            \n            # Initialize variables for power and frequency calculation\n        # Initialize variables for power and frequency calculation\n            total_power_real = 0.0\n            total_power_imag = 0.0\n            freq_weighted_power_real = 0.0\n            freq_weighted_power_imag = 0.0\n            \n            # Calculate Power Spectral Density (PSD) using Welch's method\n            f, pxx = welch(subset, fs, nperseg=2400, noverlap=0, nfft=2400)\n    \n            # Perform calculations on real and imaginary parts of PSD\n            pxx_real = np.real(pxx)\n            pxx_imag = np.imag(pxx)\n    \n            # Calculate total power (separate real and imaginary sums)\n            total_power_real = np.sum(pxx_real)\n            total_power_imag = np.sum(pxx_imag)\n\n        # Calculate frequency * power and sum it for both real and imaginary parts\n            for i in range(len(f)):\n                freq_weighted_power_real += f[i] * pxx_real[i]\n                freq_weighted_power_imag += f[i] * pxx_imag[i]\n            \n            # Avoid division by zero and calculate Mean Frequency (MNF)\n            if (total_power_real + total_power_imag) != 0:\n                mnf = (freq_weighted_power_real + 1j * freq_weighted_power_imag) / (total_power_real + 1j * total_power_imag)\n                # Store only the real part of MNF (or real + imaginary parts if needed)\n                mnf_all[row_idx, subset_idx] = np.real(mnf)  # Store only real part\n            total_power = np.sqrt(total_power_real**2 + total_power_imag**2)\n            pmnf_all[row_idx, subset_idx] = pow2db(total_power)\n\n    \n    return mnf_all,pmnf_all\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T09:08:31.271984Z","iopub.execute_input":"2025-01-15T09:08:31.272331Z","iopub.status.idle":"2025-01-15T09:08:31.283203Z","shell.execute_reply.started":"2025-01-15T09:08:31.272299Z","shell.execute_reply":"2025-01-15T09:08:31.282271Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"mnf_pmnf_results = {}\n\nfs=50\n# Compute MNF and pmnf for each combined dataset\nmnf_pmnf_results['pre_P'] = mnf_and_pmnf(combined_LH_RH_pre_P,fs)\nmnf_pmnf_results['pre_NP'] = mnf_and_pmnf(combined_LH_RH_pre_NP,fs)\nmnf_pmnf_results['post_P'] = mnf_and_pmnf(combined_LH_RH_post_P,fs)\nmnf_pmnf_results['post_NP'] = mnf_and_pmnf(combined_LH_RH_post_NP,fs)\n\n# Assign results to variables for clarity\nmnf_pre_P, pmnf_pre_P = mnf_pmnf_results['pre_P']\nmnf_pre_NP, pmnf_pre_NP = mnf_pmnf_results['pre_NP']\nmnf_post_P, pmnf_post_P = mnf_pmnf_results['post_P']\nmnf_post_NP, pmnf_post_NP = mnf_pmnf_results['post_NP']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T09:08:33.620476Z","iopub.execute_input":"2025-01-15T09:08:33.621518Z","iopub.status.idle":"2025-01-15T09:08:35.87555Z","shell.execute_reply.started":"2025-01-15T09:08:33.62138Z","shell.execute_reply":"2025-01-15T09:08:35.874592Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/296969508.py:65: ComplexWarning: Casting complex values to real discards the imaginary part\n  pmnf_all[row_idx, subset_idx] = pow2db(total_power)\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"import numpy as np\nfrom scipy.signal import welch\n\n\n\ndef medfreq(pxx, f):\n    \"\"\"\n    Calculate the median frequency from the power spectral density (PSD).\n    \n    Parameters:\n        pxx (numpy.ndarray): Power spectral density (may contain complex values).\n        f (numpy.ndarray): Frequency bins corresponding to the PSD (may contain complex values).\n    \n    Returns:\n        median_freq (float): The median frequency.\n        total_power (float): The total power of the signal.\n    \"\"\"\n    # Convert complex values to magnitude (root(a^2 + b^2)) for pxx and f\n    pxx_magnitude = np.abs(pxx)  # Magnitude of the PSD\n    f_magnitude = np.abs(f)      # Magnitude of the frequency bins\n    \n    # Compute the cumulative power (integral of the PSD)\n    cumulative_power = np.cumsum(pxx_magnitude)\n    total_power = cumulative_power[-1]\n\n    # Find the frequency corresponding to the median power\n    median_freq = f_magnitude[np.searchsorted(cumulative_power, total_power / 2)]\n\n    return median_freq, total_power\n\ndef pmdf(data, fs):\n    \"\"\"\n    Calculate the median frequency (mdf) and power in dB for each 2400-point set in the input data.\n    \n    Parameters:\n        data (numpy.ndarray): Input data with dimensions (num_points, num_channels).\n        fs (float): Sampling frequency.\n    \n    Returns:\n        freq_md (numpy.ndarray): Median frequencies for each set of 2400 points in each row.\n        pow_md (numpy.ndarray): Power in dB for each set of 2400 points in each row.\n    \"\"\"\n    # Initialize result arrays\n    num_rows, num_cols = data.shape\n    num_sets = num_cols // 2400  # 5 sets of 2400 points\n    freq_md_all = np.zeros((num_rows, num_sets))\n    pow_md_all = np.zeros((num_rows, num_sets))\n\n    # Iterate over each row\n    for row in range(num_rows):\n        # Process each set of 2400 points in the row\n        for set_idx in range(num_sets):\n            start_idx = set_idx * 2400\n            end_idx = start_idx + 2400\n            subset = data[row, start_idx:end_idx]\n            \n            # Compute the Power Spectral Density (PSD) using Welch's method\n            f, pxx = welch(subset, fs, nperseg=2400, noverlap=0, nfft=2400)\n            \n            # Calculate the median frequency and total power\n            median_freq, total_power = medfreq(pxx, f)\n            \n            # Store the results\n            freq_md_all[row, set_idx] = median_freq\n            pow_md_all[row, set_idx] = total_power\n    \n    # Convert power to dB\n    pow_md_all = 10 * np.log10(pow_md_all)\n    \n    return freq_md_all, pow_md_all\n\n# Example usage:\nfs = 50  # Sampling frequency (replace with the actual value)\n\n# Assuming data has 28 rows and 12000 columns\n# Call the function for each dataset\nfreq_md_pre_P, pow_md_pre_P = pmdf(combined_LH_RH_pre_P, fs)\nfreq_md_pre_NP, pow_md_pre_NP = pmdf(combined_LH_RH_pre_NP, fs)\nfreq_md_post_P, pow_md_post_P = pmdf(combined_LH_RH_post_P, fs)\nfreq_md_post_NP, pow_md_post_NP = pmdf(combined_LH_RH_post_NP, fs)\n\n# freq_md_pre_P, pow_md_pre_P, etc., will now hold the median frequencies and power in dB for each set of 2400 points for each row\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T09:18:56.039798Z","iopub.execute_input":"2025-01-15T09:18:56.040331Z","iopub.status.idle":"2025-01-15T09:18:56.446882Z","shell.execute_reply.started":"2025-01-15T09:18:56.04028Z","shell.execute_reply":"2025-01-15T09:18:56.445844Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"import numpy as np\nfrom scipy.signal import welch\n\ndef sm1(data, fs):\n    \"\"\"\n    Calculate the first moment (Mom1) of the power spectral density (PSD) for each subset of 2400 points.\n    \n    Parameters:\n        data (numpy.ndarray): Input data with dimensions (num_points, num_channels).\n        fs (float): Sampling frequency.\n    \n    Returns:\n        Mom1_all (numpy.ndarray): First moments for each subset and each row.\n    \"\"\"\n    num_points, num_channels = data.shape\n    num_subsets = num_channels // 2400  # Each subset has 2400 points\n    \n    # Initialize result array\n    Mom1_all = np.zeros((num_points, num_subsets))\n    \n    # Process each row (each row corresponds to one signal across all channels)\n    for row_idx in range(num_points):\n        for subset_idx in range(num_subsets):\n            start_idx = subset_idx * 2400\n            end_idx = start_idx + 2400\n            subset = data[row_idx, start_idx:end_idx]\n            \n            # Calculate PSD using Welch's method\n            f, pxx = welch(subset, fs, nperseg=2400, noverlap=0, nfft=2400)\n            pxx_magnitude = np.abs(pxx)  # Magnitude of the PSD\n            pxx_magnitude = np.asarray(pxx_magnitude)\n            f_magnitude = np.abs(f) \n\n            \n            log_pxx = np.array([np.log(val) / np.log(10) for val in pxx_magnitude])\n            # Calculate the first moment of the power spectral density\n            Mom1_all[row_idx, subset_idx] = np.sum(10*log_pxx * f)\n    \n    return Mom1_all\n\n# Example usage:\nfs = 50  # Example sampling frequency\nsm1_pre_P = sm1(combined_LH_RH_pre_P, fs)\nsm1_pre_NP= sm1(combined_LH_RH_pre_NP, fs)\nsm1_post_P= sm1(combined_LH_RH_post_P, fs)\nsm1_post_NP = sm1(combined_LH_RH_post_NP, fs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T09:32:40.5993Z","iopub.execute_input":"2025-01-15T09:32:40.599709Z","iopub.status.idle":"2025-01-15T09:32:42.756222Z","shell.execute_reply.started":"2025-01-15T09:32:40.599673Z","shell.execute_reply":"2025-01-15T09:32:42.754693Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"import numpy as np\nfrom scipy.signal import welch\n\ndef sm2(data, fs):\n    \"\"\"\n    Calculate the first moment (Mom1) of the power spectral density (PSD) for each subset of 2400 points.\n    \n    Parameters:\n        data (numpy.ndarray): Input data with dimensions (num_points, num_channels).\n        fs (float): Sampling frequency.\n    \n    Returns:\n        Mom1_all (numpy.ndarray): First moments for each subset and each row.\n    \"\"\"\n    num_points, num_channels = data.shape\n    num_subsets = num_channels // 2400  # Each subset has 2400 points\n    \n    # Initialize result array\n    Mom2_all = np.zeros((num_points, num_subsets))\n    \n    # Process each row (each row corresponds to one signal across all channels)\n    for row_idx in range(num_points):\n        for subset_idx in range(num_subsets):\n            start_idx = subset_idx * 2400\n            end_idx = start_idx + 2400\n            subset = data[row_idx, start_idx:end_idx]\n            \n            # Calculate PSD using Welch's method\n            f, pxx = welch(subset, fs, nperseg=2400, noverlap=0, nfft=2400)\n            pxx_magnitude = np.abs(pxx)  # Magnitude of the PSD\n            pxx_magnitude = np.asarray(pxx_magnitude)\n            f_magnitude = np.abs(f) \n\n            \n            log_pxx = np.array([np.log(val) / np.log(10) for val in pxx_magnitude])\n            # Calculate the first moment of the power spectral density\n            Mom2_all[row_idx, subset_idx] = np.sum(10*log_pxx * (f**2))\n    \n    return Mom2_all\n\n# Example usage:\nfs = 50  # Example sampling frequency\nsm2_pre_P = sm2(combined_LH_RH_pre_P, fs)\nsm2_pre_NP= sm2(combined_LH_RH_pre_NP, fs)\nsm2_post_P= sm2(combined_LH_RH_post_P, fs)\nsm2_post_NP = sm2(combined_LH_RH_post_NP, fs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T09:36:20.280099Z","iopub.execute_input":"2025-01-15T09:36:20.280535Z","iopub.status.idle":"2025-01-15T09:36:22.427882Z","shell.execute_reply.started":"2025-01-15T09:36:20.280444Z","shell.execute_reply":"2025-01-15T09:36:22.426792Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"import numpy as np\nfrom scipy.signal import welch\n\ndef sm3(data, fs):\n    \"\"\"\n    Calculate the first moment (Mom1) of the power spectral density (PSD) for each subset of 2400 points.\n    \n    Parameters:\n        data (numpy.ndarray): Input data with dimensions (num_points, num_channels).\n        fs (float): Sampling frequency.\n    \n    Returns:\n        Mom1_all (numpy.ndarray): First moments for each subset and each row.\n    \"\"\"\n    num_points, num_channels = data.shape\n    num_subsets = num_channels // 2400  # Each subset has 2400 points\n    \n    # Initialize result array\n    Mom3_all = np.zeros((num_points, num_subsets))\n    \n    # Process each row (each row corresponds to one signal across all channels)\n    for row_idx in range(num_points):\n        for subset_idx in range(num_subsets):\n            start_idx = subset_idx * 2400\n            end_idx = start_idx + 2400\n            subset = data[row_idx, start_idx:end_idx]\n            \n            # Calculate PSD using Welch's method\n            f, pxx = welch(subset, fs, nperseg=2400, noverlap=0, nfft=2400)\n            pxx_magnitude = np.abs(pxx)  # Magnitude of the PSD\n            pxx_magnitude = np.asarray(pxx_magnitude)\n            f_magnitude = np.abs(f) \n\n            \n            log_pxx = np.array([np.log(val) / np.log(10) for val in pxx_magnitude])\n            # Calculate the first moment of the power spectral density\n            Mom3_all[row_idx, subset_idx] = np.sum(10*log_pxx * (f**3))\n    \n    return Mom3_all\n\n# Example usage:\nfs = 50  # Example sampling frequency\nsm3_pre_P = sm3(combined_LH_RH_pre_P, fs)\nsm3_pre_NP= sm3(combined_LH_RH_pre_NP, fs)\nsm3_post_P= sm3(combined_LH_RH_post_P, fs)\nsm3_post_NP = sm3(combined_LH_RH_post_NP, fs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T09:37:29.802127Z","iopub.execute_input":"2025-01-15T09:37:29.802523Z","iopub.status.idle":"2025-01-15T09:37:31.931196Z","shell.execute_reply.started":"2025-01-15T09:37:29.80249Z","shell.execute_reply":"2025-01-15T09:37:31.929733Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"import numpy as np\nfrom scipy.stats import kurtosis\n\ndef normkurt(data, set_size):\n    \"\"\"\n    Calculate the normalized kurtosis for each subset of the data.\n    \n    Parameters:\n        data (numpy.ndarray): Input data with dimensions (28, 12000).\n        set_size (int): Size of each subset (2400 points).\n    \n    Returns:\n        numpy.ndarray: Normalized kurtosis for each subset of each row.\n        \n    \"\"\"\n    data = np.asarray(data, dtype=np.float64)\n    num_rows, num_cols = data.shape\n    count = num_cols // set_size  # Number of subsets per row\n    \n    # Initialize the result array\n    x = np.zeros_like(data)\n    \n    # Loop through each row\n    for row_idx in range(num_rows):\n        for i in range(count):\n            # Define the current subset\n            subset = data[row_idx, i * set_size : (i + 1) * set_size]\n            \n            # Define the last subset (which is the i-th subset in the row)\n            last_subset = data[row_idx, (i + 1) * set_size - set_size : (i + 1) * set_size]\n            \n            # Calculate the kurtosis for the current subset and the last subset\n            kurt_current = kurtosis(subset)\n            kurt_last = kurtosis(last_subset)\n            \n            # Normalize the kurtosis\n            norm_kurt = kurt_current / kurt_last\n            \n            # Store the result in the output array\n            x[row_idx, i * set_size - 11 : (i + 1) * set_size] = norm_kurt\n    \n    return x\nf=2400\nnormkurt_pre_P = normkurt(combined_LH_RH_pre_P, f)\nnormkurt_pre_NP= normkurt(combined_LH_RH_pre_NP, f)\nnormkurt_post_P= normkurt(combined_LH_RH_post_P, f)\nnormkurt_post_NP = normkurt(combined_LH_RH_post_NP, f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T09:44:03.860696Z","iopub.execute_input":"2025-01-15T09:44:03.861091Z","iopub.status.idle":"2025-01-15T09:44:04.684142Z","shell.execute_reply.started":"2025-01-15T09:44:03.861058Z","shell.execute_reply":"2025-01-15T09:44:04.68294Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"import numpy as np\n\ndef normvar(data, set_size):\n    \"\"\"\n    Calculate the normalized variance for each subset of the data.\n    \n    Parameters:\n        data (numpy.ndarray): Input data with dimensions (28, 12000).\n        set_size (int): Size of each subset (2400 points).\n    \n    Returns:\n        numpy.ndarray: Normalized variance for each subset of each row.\n    \"\"\"\n    # Ensure the data is in numeric format\n    data = np.asarray(data, dtype=np.float64)\n    \n    num_rows, num_cols = data.shape\n    count = num_cols // set_size  # Number of subsets per row\n    \n    # Initialize the result array\n    x = np.zeros_like(data)\n    \n    # Loop through each row\n    for row_idx in range(num_rows):\n        for i in range(count):\n            # Define the current subset\n            subset = data[row_idx, i * set_size : (i + 1) * set_size]\n            \n            # Define the last subset (which is the i-th subset in the row)\n            last_subset = data[row_idx, (i + 1) * set_size - set_size : (i + 1) * set_size]\n            \n            # Calculate the variance for the current subset and the last subset\n            var_current = np.var(subset)\n            var_last = np.var(last_subset)\n            \n            # Normalize the variance\n            norm_var = var_current / var_last\n            \n            # Store the result in the output array\n            x[row_idx, i * set_size - 11 : (i + 1) * set_size] = norm_var\n    \n    return x\n\n# Example usage:\n# Assuming combined_LH_RH_pre_P is already defined\nf = 2400\nnormvar_pre_P = normvar(combined_LH_RH_pre_P, f)\nnormvar_pre_NP = normvar(combined_LH_RH_pre_NP, f)\nnormvar_post_P = normvar(combined_LH_RH_post_P, f)\nnormvar_post_NP = normvar(combined_LH_RH_post_NP, f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T09:45:25.748738Z","iopub.execute_input":"2025-01-15T09:45:25.749146Z","iopub.status.idle":"2025-01-15T09:45:25.829492Z","shell.execute_reply.started":"2025-01-15T09:45:25.749102Z","shell.execute_reply":"2025-01-15T09:45:25.828315Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.stats import entropy\n\ndef entr(data, fs):\n    \"\"\"\n    Calculate the Shannon entropy for each subset of the data.\n    \n    Parameters:\n        data (numpy.ndarray): Input data with dimensions (28, 12000).\n        fs (int): Sampling frequency.\n    \n    Returns:\n        numpy.ndarray: Entropy values for each subset of each row.\n    \"\"\"\n    ts = 1 / fs  # Time step based on the sampling frequency\n    set_size = 2400  # Number of points per subset\n    num_rows, num_cols = data.shape\n    count = num_cols // set_size  # Number of sets per row\n    \n    # Initialize the result array for storing entropy values\n    ent_values = np.zeros(num_rows)\n    \n    # Loop through each row\n    for row_idx in range(num_rows):\n        row_entropy = []\n        for i in range(count):\n            # Extract the current subset\n            subset = data[row_idx, i * set_size : (i + 1) * set_size]\n            \n            # Create the time vector for this subset (it should match the length of the subset)\n            t = np.arange(0, len(subset)) * ts  # Create time vector with the same length as the subset\n            tdur = pd.to_timedelta(t, unit='s')  # Convert time to pandas Timedelta for correct format\n            \n            # Create a pandas DataFrame (equivalent to a timetable) with time as the index\n            xt = pd.DataFrame(subset, index=tdur, columns=['data'])\n            \n            # Calculate the histogram and normalize it\n            hist, bin_edges = np.histogram(xt['data'], bins=100, density=True)\n            \n            # Calculate the Shannon entropy\n            shannon_entropy = entropy(hist)\n            \n            # Store the entropy for this subset\n            row_entropy.append(shannon_entropy)\n        \n        # Average entropy for the row\n        ent_values[row_idx] = np.mean(row_entropy)\n    \n    return ent_values\n\n# Example usage:\nfs = 50  # Example sampling frequency\nentr_pre_P = entr(combined_LH_RH_pre_P, fs)\nentr_pre_NP = entr(combined_LH_RH_pre_NP, fs)\nentr_post_P = entr(combined_LH_RH_post_P, fs)\n\n\n\n\n\nentr_post_NP = entr(combined_LH_RH_post_NP, fs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T09:54:13.023003Z","iopub.execute_input":"2025-01-15T09:54:13.023405Z","iopub.status.idle":"2025-01-15T09:54:14.543262Z","shell.execute_reply.started":"2025-01-15T09:54:13.02337Z","shell.execute_reply":"2025-01-15T09:54:14.54232Z"}},"outputs":[],"execution_count":75}]}