{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9685965,"sourceType":"datasetVersion","datasetId":5921041},{"sourceId":9876246,"sourceType":"datasetVersion","datasetId":6063390}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/isabbaggin/classification?scriptVersionId=218730431\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-01-22T06:33:32.989492Z","iopub.execute_input":"2025-01-22T06:33:32.990458Z","iopub.status.idle":"2025-01-22T06:33:33.049579Z","shell.execute_reply.started":"2025-01-22T06:33:32.990415Z","shell.execute_reply":"2025-01-22T06:33:33.048342Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/datafilesy/Data/240427SG_BN.xlsx\n/kaggle/input/datafilesy/Data/240420RM_AN.xlsx\n/kaggle/input/datafilesy/Data/240511IH_AN.xlsx\n/kaggle/input/datafilesy/Data/240501SK_BN.xlsx\n/kaggle/input/datafilesy/Data/240405SM_BN.xlsx\n/kaggle/input/datafilesy/Data/240511IS_BN.xlsx\n/kaggle/input/datafilesy/Data/240420BR_AN.xlsx\n/kaggle/input/datafilesy/Data/240501BD_BP.xlsx\n/kaggle/input/datafilesy/Data/240420AM_AP.xlsx\n/kaggle/input/datafilesy/Data/240501BD_AP.xlsx\n/kaggle/input/datafilesy/Data/240511BI_BN.xlsx\n/kaggle/input/datafilesy/Data/240501DM_BP.xlsx\n/kaggle/input/datafilesy/Data/240229SM_BN.xlsx\n/kaggle/input/datafilesy/Data/240427IM_AN.xlsx\n/kaggle/input/datafilesy/Data/240427BB_BN.xlsx\n/kaggle/input/datafilesy/Data/240511SL_AN.xlsx\n/kaggle/input/datafilesy/Data/240501DM_AP.xlsx\n/kaggle/input/datafilesy/Data/240511IH_BN.xlsx\n/kaggle/input/datafilesy/Data/240405SM_AN.xlsx\n/kaggle/input/datafilesy/Data/240501SK_AN.xlsx\n/kaggle/input/datafilesy/Data/240427SD_AN.xlsx\n/kaggle/input/datafilesy/Data/240405DH_BP.xlsx\n/kaggle/input/datafilesy/Data/240427SS_AN.xlsx\n/kaggle/input/datafilesy/Data/240420AM_BP.xlsx\n/kaggle/input/datafilesy/Data/240420AJ_BP.xlsx\n/kaggle/input/datafilesy/Data/240501AR_AP.xlsx\n/kaggle/input/datafilesy/Data/240501BS_AP.xlsx\n/kaggle/input/datafilesy/Data/240427BB_AN.xlsx\n/kaggle/input/datafilesy/Data/240427AD_BN.xlsx\n/kaggle/input/datafilesy/Data/240511SL_BN.xlsx\n/kaggle/input/datafilesy/Data/240420AJ_AP.xlsx\n/kaggle/input/datafilesy/Data/240420DH_AP.xlsx\n/kaggle/input/datafilesy/Data/240420BM_AN.xlsx\n/kaggle/input/datafilesy/Data/240420BM_BN.xlsx\n/kaggle/input/datafilesy/Data/240511IS_AN.xlsx\n/kaggle/input/datafilesy/Data/240511MD_AN.xlsx\n/kaggle/input/datafilesy/Data/240427SG_AN.xlsx\n/kaggle/input/datafilesy/Data/240501AR_BP.xlsx\n/kaggle/input/datafilesy/Data/240427SD_BN.xlsx\n/kaggle/input/datafilesy/Data/240511JI_BN.xlsx\n/kaggle/input/datafilesy/Data/240427AD_AN.xlsx\n/kaggle/input/datafilesy/Data/240427SS_BN.xlsx\n/kaggle/input/datafilesy/Data/240229DH_AP.xlsx\n/kaggle/input/datafilesy/Data/240511MD_BN.xlsx\n/kaggle/input/datafilesy/Data/240420DH_BP.xlsx\n/kaggle/input/datafilesy/Data/240229SM_AN.xlsx\n/kaggle/input/datafilesy/Data/240427IM_BN.xlsx\n/kaggle/input/datafilesy/Data/240511KM_BN.xlsx\n/kaggle/input/datafilesy/Data/240229DH_BP.xlsx\n/kaggle/input/datafilesy/Data/240405DH_AP.xlsx\n/kaggle/input/datafilesy/Data/240501BS_BP.xlsx\n/kaggle/input/datafilesy/Data/240511BI_AN.xlsx\n/kaggle/input/datafilesy/Data/240420BR_BN.xlsx\n/kaggle/input/datafilesy/Data/240511JI_AN.xlsx\n/kaggle/input/datafilesy/Data/240420RM_BN.xlsx\n/kaggle/input/datafilesy/Data/240511KM_AN.xlsx\n/kaggle/input/datanew/data2/240427SG_BN.xlsx\n/kaggle/input/datanew/data2/240511IH_AN.xlsx\n/kaggle/input/datanew/data2/240405SM_BN.xlsx\n/kaggle/input/datanew/data2/240511IS_BN.xlsx\n/kaggle/input/datanew/data2/240420BR_AN.xlsx\n/kaggle/input/datanew/data2/240501BD_BP.xlsx\n/kaggle/input/datanew/data2/240501SK_BP.xlsx\n/kaggle/input/datanew/data2/240420AM_AP.xlsx\n/kaggle/input/datanew/data2/240501BD_AP.xlsx\n/kaggle/input/datanew/data2/240511BI_BN.xlsx\n/kaggle/input/datanew/data2/240427AD_AP.xlsx\n/kaggle/input/datanew/data2/240420RM_BP.xlsx\n/kaggle/input/datanew/data2/240501DM_BP.xlsx\n/kaggle/input/datanew/data2/240229SM_BN.xlsx\n/kaggle/input/datanew/data2/240427IM_AN.xlsx\n/kaggle/input/datanew/data2/240427BB_BN.xlsx\n/kaggle/input/datanew/data2/240501SK_AP.xlsx\n/kaggle/input/datanew/data2/240420BM_BP.xlsx\n/kaggle/input/datanew/data2/240501DM_AP.xlsx\n/kaggle/input/datanew/data2/240511IH_BN.xlsx\n/kaggle/input/datanew/data2/240405SM_AN.xlsx\n/kaggle/input/datanew/data2/240427SD_AN.xlsx\n/kaggle/input/datanew/data2/240405DH_BP.xlsx\n/kaggle/input/datanew/data2/240427SS_AN.xlsx\n/kaggle/input/datanew/data2/240420AM_BP.xlsx\n/kaggle/input/datanew/data2/240420AJ_BP.xlsx\n/kaggle/input/datanew/data2/240501AR_AP.xlsx\n/kaggle/input/datanew/data2/240501BS_AP.xlsx\n/kaggle/input/datanew/data2/240427BB_AN.xlsx\n/kaggle/input/datanew/data2/240427AD_BP.xlsx\n/kaggle/input/datanew/data2/240420AJ_AP.xlsx\n/kaggle/input/datanew/data2/240420DH_AP.xlsx\n/kaggle/input/datanew/data2/240511IS_AN.xlsx\n/kaggle/input/datanew/data2/240511MD_AN.xlsx\n/kaggle/input/datanew/data2/240427SG_AN.xlsx\n/kaggle/input/datanew/data2/240501AR_BP.xlsx\n/kaggle/input/datanew/data2/240427SD_BN.xlsx\n/kaggle/input/datanew/data2/240420RM_AP.xlsx\n/kaggle/input/datanew/data2/240511JI_BN.xlsx\n/kaggle/input/datanew/data2/240427SS_BN.xlsx\n/kaggle/input/datanew/data2/240229DH_AP.xlsx\n/kaggle/input/datanew/data2/240511MD_BN.xlsx\n/kaggle/input/datanew/data2/240420DH_BP.xlsx\n/kaggle/input/datanew/data2/240511SL_BP.xlsx\n/kaggle/input/datanew/data2/240229SM_AN.xlsx\n/kaggle/input/datanew/data2/240427IM_BN.xlsx\n/kaggle/input/datanew/data2/240511KM_BN.xlsx\n/kaggle/input/datanew/data2/240229DH_BP.xlsx\n/kaggle/input/datanew/data2/240405DH_AP.xlsx\n/kaggle/input/datanew/data2/240511SL_AP.xlsx\n/kaggle/input/datanew/data2/240501BS_BP.xlsx\n/kaggle/input/datanew/data2/240511BI_AN.xlsx\n/kaggle/input/datanew/data2/240420BR_BN.xlsx\n/kaggle/input/datanew/data2/240511JI_AN.xlsx\n/kaggle/input/datanew/data2/240511KM_AN.xlsx\n/kaggle/input/datanew/data2/240420BM_AP.xlsx\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nfrom matplotlib.widgets import CheckButtons\n\n\n# def plot_data(matrix):\n#     time_per_row = np.arange(0, 12000, 1)  # Time vector for 2400 rows\n#     matrix=np.array(matrix)\n#     plt.figure(figsize=(20,8))\n#     for i in range(matrix.shape[0]):\n#       plt.plot(np.array(matrix[i,:]))\n#\n#     # for i, row in enumerate(MATRIX):\n#     #     plt.scatter(time_per_row, row)\n#\n#     # Adding labels and title\n#     plt.title('LH_B_P')\n#     plt.xlabel('TIME')\n#     plt.ylabel('AMPLITUDE')\n#     plt.legend()\n#     plt.show()\n#\n#     # Plot LH data\n#     # for i in range(LH_all_pre.shape[0]):\n#     #     axs[0].plot(time_per_row, LH_all_pre[i, :, 0])\n#     #\n#     # axs[0].set_title(\"LH Data - Time vs Amplitude\")\n#     # axs[0].set_xlabel(\"Time (minutes)\")\n#     # axs[0].set_ylabel(\"Amplitude\")\n#     # axs[0].legend()\n#     # axs[0].grid(True)\n#     #\n#     # # Plot RH data\n#     # for i in range(RH_all_pre.shape[0]):\n#     #     axs[1].plot(time_per_row, RH_all_pre[i, :, 0])\n#     #\n#     # axs[1].set_title(\"RH Data - Time vs Amplitude\")\n#     # axs[1].set_xlabel(\"Time (minutes)\")\n#     # axs[1].set_ylabel(\"Amplitude\")\n#     # axs[1].legend()\n#     # axs[1].grid(True)\n#     #\n#     # # Plot LL data\n#     # for i in range(LL_all_pre.shape[0]):\n#     #     axs[2].plot(time_per_row, LL_all_pre[i, :, 0])\n#     #\n#     # axs[2].set_title(\"LL Data - Time vs Amplitude\")\n#     # axs[2].set_xlabel(\"Time (minutes)\")\n#     # axs[2].set_ylabel(\"Amplitude\")\n#     # axs[2].legend()\n#     # axs[2].grid(True)\n#     #\n#     # # Plot RL data\n#     # for i in range(RL_all_pre.shape[0]):\n#     #     axs[3].plot(time_per_row, RL_all_pre[i, :, 0])\n#     #\n#     # axs[3].set_title(\"RL Data - Time vs Amplitude\")\n#     # axs[3].set_xlabel(\"Time (minutes)\")\n#     # axs[3].set_ylabel(\"Amplitude\")\n#     # axs[3].legend()\n#     # axs[3].grid(True)\n#\n#     plt.tight_layout(pad=2.0)\n#     plt.show()\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport ipywidgets as widgets\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport ipywidgets as widgets\n\ndef plot_with_widgets(matrix):\n    matrix = np.array(matrix)\n    fig, ax = plt.subplots(figsize=(10, 5))\n\n    # Initial plot\n    lines = []\n    for i in range(matrix.shape[0]):\n        line, = ax.plot(matrix[i, :], label=f'Line {i+1}')\n        lines.append(line)\n    ax.legend()\n\n    # Create checkboxes for each line\n    checkboxes = [widgets.Checkbox(value=True, description=f'Line {i+1}') for i in range(matrix.shape[0])]\n    check_ui = widgets.VBox(checkboxes)\n\n    def update_plot(*args):\n        # Clear the current axes\n        ax.clear()\n\n        # Re-plot only the lines with checked checkboxes\n        for i, checkbox in enumerate(checkboxes):\n            if checkbox.value:  # If checkbox is checked\n                ax.plot(matrix[i, :], label=f'Line {i+1}')\n\n        # Redraw the legend and labels\n        ax.legend()\n        ax.set_title('Interactive Line Plot')\n        ax.set_xlabel('X-axis')\n        ax.set_ylabel('Y-axis')\n\n        # Redraw the figure\n        fig.canvas.draw()\n\n    # Link the checkboxes to the update_plot function\n    for checkbox in checkboxes:\n        checkbox.observe(update_plot, 'value')\n\n    # Display the checkboxes UI\n    display(check_ui)\n    plt.show()\n\n\n\n\n\n\ndef plot_data(matrix, save_path='C:\\\\Users\\\\PC1\\\\Pictures\\\\plot\\\\deviation_post_P.jpg'):\n    sns.set(style=\"whitegrid\")\n    matrix = np.array(matrix)\n\n    plt.figure(figsize=(20, 10))\n    palette = sns.color_palette(\"husl\", matrix.shape[0])\n    for i in range(matrix.shape[0]):\n        plt.plot(matrix[i, :], color=palette[i], linewidth=1.0)\n    plt.title('deviation_post_P', fontsize=16)\n    plt.xlabel('TIME', fontsize=14)\n    plt.ylabel('AMPLITUDE', fontsize=14)\n    plt.legend(title=\"Data Rows\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=12)\n\n    plt.tight_layout()\n#     plt.savefig(save_path, format='jpg', dpi=300)\n    plt.show()\n\ndef plot(matrix, save_path='C:\\\\Users\\\\PC1\\\\Pictures\\\\plot\\\\deviation_post_NP.jpg'):\n    sns.set(style=\"whitegrid\")\n    matrix = np.array(matrix)\n    fig, ax = plt.subplots(figsize=(20, 10))\n    palette = sns.color_palette(\"husl\", matrix.shape[0])\n    lines = []\n    for i in range(matrix.shape[0]):\n        line, = ax.plot(matrix[i, :], color=palette[i], linewidth=1.0, label=f'Line {i + 1}')\n        lines.append(line)\n    ax.set_title('deviation_post_NP', fontsize=16)\n    ax.set_xlabel('TIME', fontsize=14)\n    ax.set_ylabel('AMPLITUDE', fontsize=14)\n    labels = [f'Line {i + 1}' for i in range(matrix.shape[0])]\n    check = CheckButtons(ax=plt.axes([0.8, 0.4, 0.1, 0.15]), labels=labels, actives=[True] * len(labels))\n\n    for i, line in enumerate(lines):\n        check.labels[i].set_color(line.get_color())\n    def func(label):\n        index = labels.index(label)\n        lines[index].set_visible(not lines[index].get_visible())\n        plt.draw()\n\n    check.on_clicked(func)\n\n    plt.tight_layout()\n    # plt.savefig(save_path, format='jpg', dpi=300)\n    plt.show()\n\n\ndef plot_state_means(LH_all_pre_P, RH_all_pre_P, save_path='C:\\\\Users\\\\PC1\\\\Pictures\\\\plot\\\\pre_P.jpg'):\n\n    def extract_state_data(combined_LH, combined_RH):\n        states = {}\n        num_samples = combined_LH.shape[0]  # Number of samples\n        num_states = combined_LH.shape[2]    # Number of states (5 in this case)\n\n        for i in range(num_samples):  # Iterate through the samples\n            for j in range(num_states):  # Iterate through the states\n                state_name = f'State {j + 1}'\n                if state_name not in states:\n                    states[state_name] = []\n                states[state_name].append(combined_LH[i, :, j])  # Append LH data\n                states[state_name].append(combined_RH[i, :, j])  # Append RH data\n\n        return states\n\n\n    state_data_pre_P = extract_state_data(LH_all_pre_P, RH_all_pre_P)\n    print(state_data_pre_P)\n    means = {}\n    for state_name, data in state_data_pre_P.items():\n        means[state_name] = [np.mean(data_array) for data_array in data]\n    box_data = {state: [] for state in means.keys()}\n    for state in means.keys():\n        box_data[state].extend(means[state])\n    # plt.figure(figsize=(15, 10))\n    # sns.boxplot(data=[box_data[state] for state in box_data], palette=\"Set3\")\n    # plt.xticks(ticks=np.arange(len(box_data)), labels=box_data.keys())\n    # plt.title('Box Plots of Means for Each State')\n    # plt.xlabel('States')\n    # plt.ylabel('Mean Values')\n    # # plt.legend(['Pre P', 'Post P', 'Pre NP', 'Post NP'], loc='upper right')\n    # plt.savefig(save_path, format='jpg', dpi=300)\n    # plt.tight_layout()\n    # plt.show()\n\n# Example usage:\n# if __name__ == \"__main__\":\n#     plot_state_means(LH_all_pre_P, RH_all_pre_P, LH_all_post_P, RH_all_post_P,\n#                      LH_all_pre_NP, RH_all_pre_NP, LH_all_post_NP, RH_all_post_NP)\n\ndef plot_median_of_means(LH_all_pre_P, RH_all_pre_P):\n\n    combined_data = np.concatenate((LH_all_pre_P, RH_all_pre_P), axis=0)\n\n    #Number of states (columns)\n\n    num_states = combined_data.shape[2] # Assuming 5 states as columns \n    means_per_state = []\n\n    # Calculate mean of each column in each 2D array\n\n    for state in range(num_states): \n        means_for_state = []\n\n        for matrix in combined_data: \n            means_for_state.append(np.mean(matrix[:, state])) # Mean of each column for state \n        means_per_state.append(means_for_state)\n\n    return means_per_state\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.stats import kurtosis,skew\n\n\n\n\ndef calculate_statistics(data_3d):\n    data_3d = np.array(pd.to_numeric(data_3d.flatten(), errors='coerce')).reshape(data_3d.shape)\n\n\n    means = []\n    for i in range(data_3d.shape[0]):\n        data_without_nans = np.nan_to_num(data_3d[i], nan=0.0)\n        mean_value = np.sum(data_without_nans) / (2400 * 5)  # Sum and divide by 2400*5\n        means.append(mean_value)\n\n    std_devs = []\n\n    for i in range(data_3d.shape[0]):\n        data_without_nans = np.nan_to_num(data_3d[i], nan=0.0)\n        mean_value = np.sum(data_without_nans) / (2400 * 5)\n        squared_diffs = (data_without_nans - mean_value) ** 2\n        std_dev_value = np.sqrt(np.sum(squared_diffs) / (2400 * 5))\n        std_devs.append(std_dev_value)\n    # std_devs = np.nanstd(data_3d, axis=1)\n\n    return means, std_devs\n\ndef flatten_3d_to_2d(array_3d):\n    if array_3d.ndim != 3:\n        raise ValueError(\"Input array must be 3D\")\n    flattened_arrays = np.array([array_3d[i].flatten() for i in range(array_3d.shape[0])])\n\n    return flattened_arrays\n\n\n# def flatten_3d_to_2d_col(array_3d):\n#     array_3d = np.array(array_3d)\n#     # if array_3d.ndim != 3:\n#     #     raise ValueError(\"Input array must be 3D\")\n#     n_slices, rows, cols = array_3d.shape\n#     flattened_arrays = []\n#     for i in range(n_slices):\n#         array_2d = array_3d[i]\n#         flattened_array = array_2d.T.flatten()  # Transpose to get columns first, then flatten\n#         flattened_arrays.append(flattened_array)\n#\n#     return np.array(flattened_arrays)\n\nimport numpy as np\n\n\ndef flatten_3d_to_2d_col(array_3d):\n    target_shape = (2400, 5)  # The desired shape for all 2D arrays\n\n    # Ensure array_3d is a list and handle inconsistent shapes\n    if isinstance(array_3d, list):\n        padded_arrays = []\n\n        for sub_array in array_3d:\n            sub_array = np.asarray(sub_array)\n\n            # Check the shape of the current sub-array\n            if sub_array.shape != target_shape:\n                # Pad with zeros to make it (2400, 5)\n                padded_sub_array = np.zeros(target_shape)\n                # Fill in the available data\n                rows, cols = sub_array.shape\n                padded_sub_array[:rows, :cols] = sub_array\n                padded_arrays.append(padded_sub_array)\n            else:\n                padded_arrays.append(sub_array)\n\n        array_3d = np.array(padded_arrays)\n\n    # Check if the input is now a valid 3D array\n    if array_3d.ndim != 3:\n        raise ValueError(\"Input array must be 3D\")\n\n    n_slices, rows, cols = array_3d.shape\n    flattened_arrays = []\n\n    for i in range(n_slices):\n        array_2d = array_3d[i]\n        flattened_array = array_2d.T.flatten()  # Transpose to get columns first, then flatten\n        flattened_arrays.append(flattened_array)\n\n    return np.array(flattened_arrays)\n\n\ndef z_normalize(array_1d, mean, std_dev):\n    return (array_1d - mean) / std_dev\n\n\ndef calculate_normalized_variance_and_kurtosis(data):\n    data = np.asarray(data, dtype=float)  # Convert to float, handles None\n    data = np.nan_to_num(data)\n    normalized_variance = np.var(data, ddof=1)  # Sample variance\n    kurt_value = kurtosis(data)\n    return normalized_variance, kurt_value\n\ndef process_batches_for_normalised(array_1d, batch_size=200):\n    num_batches = len(array_1d) // batch_size\n    variances = []\n    kurtoses = []\n\n    for i in range(num_batches):\n        batch = array_1d[i * batch_size:(i + 1) * batch_size]\n        var, kurt = calculate_normalized_variance_and_kurtosis(batch)\n        variances.append(var)\n        kurtoses.append(kurt)\n\n    return np.array(variances), np.array(kurtoses)\n\ndef calculate_statistics_in_batches(data):\n    # Ensure data is numeric and replace NaN/None with 0\n    data = np.asarray(data, dtype=float)  # Convert to float, handles None\n    data = np.nan_to_num(data)  # Replace NaN with 0\n\n    # Calculate mean, standard deviation, variance, and skewness\n    mean_value = np.mean(data)\n    std_dev_value = np.std(data, ddof=1)  # Sample std deviation\n    variance_value = np.var(data, ddof=1)  # Sample variance\n    skewness_value = skew(data)\n\n    return mean_value, std_dev_value, variance_value, skewness_value\n\ndef process_batches_raw(array_1d, batch_size=200):\n    num_batches = len(array_1d) // batch_size\n    means = []\n    stddevs=[]\n    variances=[]\n    skews=[]\n\n    for i in range(num_batches):\n        batch = array_1d[i * batch_size:(i + 1) * batch_size]\n        mm, sstd, varr, skewness = calculate_statistics_in_batches(batch)\n        variances.append(varr)\n        means.append(mm)\n        stddevs.append(sstd)\n        skews.append(skewness)\n\n\n    return np.array(means), np.array(stddevs), np.array(variances), np.array(skews)","metadata":{"execution":{"iopub.status.busy":"2025-01-22T06:34:42.463372Z","iopub.execute_input":"2025-01-22T06:34:42.463762Z","iopub.status.idle":"2025-01-22T06:34:42.948856Z","shell.execute_reply.started":"2025-01-22T06:34:42.463727Z","shell.execute_reply":"2025-01-22T06:34:42.947879Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tkinter import filedialog\n# from plot import plot_data, plot_state_means\n# from characteristics import calculate_statistics, flatten_3d_to_2d, z_normalize, \\\n#    process_batches_for_normalised, process_batches_raw, flatten_3d_to_2d_col\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndef load_and_extract_data(filepath):\n    # try:\n    dataall = pd.read_excel(filepath, sheet_name='240229SN').to_numpy()\n    # except ValueError as e:\n    #     print(f\"Error processing {filename}: {e}\")\n\n\n    LH = dataall[:, 11:43:7]\n    RH = dataall[:, 12:43:7]\n    LL = dataall[:, 13:43:7]\n    RL = dataall[:, 14:43:7]\n\n    LH = LH[2:2404, :]\n    RH = RH[2:2404, :]\n    LL = LL[2:2404, :]\n    RL = RL[2:2404, :]\n\n\n\n    return LH, RH, LL, RL\n\n\ndef reshape_data(data, target_shape=(2400, 5)):\n    if data.shape[0] < target_shape[0]:\n\n        padded = np.zeros(target_shape)\n        padded[:data.shape[0], :data.shape[1]] = data\n        return padded\n    elif data.shape[0] > target_shape[0]:\n\n        return data[:target_shape[0], :target_shape[1]]\n    return data\n\n# folder_path = input(\"Enter the path of the folder containing Excel files: \")\nfolder_path = '/kaggle/input/datanew/data2'\n\n# folder_path = filedialog.askdirectory(title=\"Select a folder containing Excel files\")\ndata_dictP = {\n    'A': {\n        'LH': [],\n        'RH': [],\n        'LL': [],\n        'RL': [],\n    },\n    'B': {\n        'LH': [],\n        'RH': [],\n        'LL': [],\n        'RL': [],\n    },\n}\n\ndata_dictNP = {\n    'A': {\n        'LH': [],\n        'RH': [],\n        'LL': [],\n        'RL': [],\n    },\n    'B': {\n        'LH': [],\n        'RH': [],\n        'LL': [],\n        'RL': [],\n    },\n}\nprint(folder_path)\n\n# for filename in os.listdir(folder_path):\n#     if filename.endswith('.xlsx'):\n#         filepath = os.path.join(folder_path, filename)\n#         try:\n#             LH, RH, LL, RL = load_and_extract_data(filepath)\n#\n#             # Additional debug print to catch string values before processing\n#             print(f\"Checking for non-numeric values in {filename}\")\n#             print(\"LH:\", LH)\n#             print(\"RH:\", RH)\n#\n#         except ValueError as e:\n#             print(f\"Error processing {filename}: {e}\")\nfor filename in os.listdir(folder_path):\n    if filename.endswith('.xlsx'):\n        filepath = os.path.join(folder_path, filename)\n        LH, RH, LL, RL = load_and_extract_data(filepath)\n\n        if filename[9] == 'A':\n            if filename[10]=='P':\n                data_dictP['A']['LH'].append(reshape_data(LH[:, :5]))\n                data_dictP['A']['RH'].append(reshape_data(RH[:, :5]))\n                data_dictP['A']['LL'].append(reshape_data(LL[:, :5]))\n                data_dictP['A']['RL'].append(reshape_data(RL[:, :5]))\n            else:\n                data_dictNP['A']['LH'].append(reshape_data(LH[:, :5]))\n                data_dictNP['A']['RH'].append(reshape_data(RH[:, :5]))\n                data_dictNP['A']['LL'].append(reshape_data(LL[:, :5]))\n                data_dictNP['A']['RL'].append(reshape_data(RL[:, :5]))\n\n        elif filename[9] == 'B':\n            if filename[10]=='P':\n                data_dictP['B']['LH'].append(reshape_data(LH[:, :5]))\n                data_dictP['B']['RH'].append(reshape_data(RH[:, :5]))\n                data_dictP['B']['LL'].append(reshape_data(LL[:, :5]))\n                data_dictP['B']['RL'].append(reshape_data(RL[:, :5]))\n            else:\n                data_dictNP['B']['LH'].append(reshape_data(LH[:, :5]))\n                data_dictNP['B']['RH'].append(reshape_data(RH[:, :5]))\n                data_dictNP['B']['LL'].append(reshape_data(LL[:, :5]))\n                data_dictNP['B']['RL'].append(reshape_data(RL[:, :5]))\n\n\nfor key in data_dictP:\n    for sub_key in data_dictP[key]:\n        if data_dictP[key][sub_key]:\n            data_dictP[key][sub_key] = np.stack(data_dictP[key][sub_key])\n        else:\n            data_dictP[key][sub_key] = np.zeros((0, 2400, 5))\n\n\ndef pad_array_to_shape(arr, target_shape):\n    arr = np.asarray(arr)\n    if arr.dtype.kind in {'U', 'S', 'O'}:\n        arr = np.where(arr == ' ', 0, arr)\n        arr = arr.astype(float)\n    current_shape = arr.shape\n    padded_array = np.zeros(target_shape)\n    rows = min(current_shape[0], target_shape[0])\n    cols = min(current_shape[1], target_shape[1])\n    padded_array[:rows, :cols] = arr[:rows, :cols]\n\n    return padded_array\n\ntarget_shape = (2400, 5)\n\nfor key in data_dictNP:\n    for sub_key in data_dictNP[key]:\n        if data_dictNP[key][sub_key]:\n            padded_arrays = [pad_array_to_shape(arr, target_shape) for arr in data_dictNP[key][sub_key]]\n            data_dictNP[key][sub_key] = np.stack(padded_arrays)\n        else:\n            data_dictNP[key][sub_key] = np.zeros((0, *target_shape))\n\n# for key in data_dictNP:\n#     for sub_key in data_dictNP[key]:\n#         if data_dictNP[key][sub_key]:\n#             data_dictNP[key][sub_key] = np.stack(data_dictNP[key][sub_key])\n#         else:\n#             data_dictNP[key][sub_key] = np.zeros((0, 2400, 5))\n\n# for key in data_dictNP:\n#     for sub_key in data_dictNP[key]:\n#         try:\n#             if data_dictNP[key][sub_key]:\n#                 data_dictNP[key][sub_key] = np.stack(data_dictNP[key][sub_key])\n#             else:\n#                 data_dictNP[key][sub_key] = np.zeros((0, 2400, 5))\n#         except ValueError as e:\n#             print(f\"Error occurred in key: {key}, sub_key: {sub_key}\")\n#             # print(f\"Data: {data_dictNP[key][sub_key]}\")\n#             print(f\"Error message: {e}\")\n\n\nLH_all_pre_P = 1000*data_dictP['B']['LH']\nRH_all_pre_P = 1000*data_dictP['B']['RH']\n# LL_all_pre_P = data_dictP['B']['LL']\n# RL_all_pre_P = data_dictP['B']['RL']\n\nLH_all_post_P = 1000*data_dictP['A']['LH']\nRH_all_post_P = 1000*data_dictP['A']['RH']\n# LL_all_post_P = data_dictP['A']['LL']\n# RL_all_post_P = data_dictP['A']['RL']\n\n\nLH_all_pre_NP = 1000*data_dictNP['B']['LH']\nRH_all_pre_NP = 1000*data_dictNP['B']['RH']\n# LL_all_pre_NP = data_dictNP['B']['LL']\n# RL_all_pre_NP = data_dictNP['B']['RL']\n\nLH_all_post_NP = 1000*data_dictNP['A']['LH']\nRH_all_post_NP = 1000*data_dictNP['A']['RH']\n# LL_all_post_NP = data_dictNP['A']['LL']\n# RL_all_post_NP = data_dictNP['A']['RL']\n\n\n\n\n# means_LH_all_pre_P, std_devs_LH_all_pre_P = calculate_statistics(LH_all_pre_P)\n# means_LH_all_pre_NP, std_devs_LH_all_pre_NP = calculate_statistics(LH_all_pre_NP)\n# means_RH_all_pre_P, std_devs_RH_all_pre_P = calculate_statistics(RH_all_pre_P)\n# means_RH_all_pre_NP, std_devs_RH_all_pre_NP = calculate_statistics(RH_all_pre_NP)\n# means_LH_all_post_P, std_devs_LH_all_post_P = calculate_statistics(LH_all_post_P)\n# means_LH_all_post_NP, std_devs_LH_all_post_NP = calculate_statistics(LH_all_post_NP)\n# means_RH_all_post_P, std_devs_RH_all_post_P = calculate_statistics(RH_all_post_P)\n# means_RH_all_post_NP, std_devs_RH_all_post_NP = calculate_statistics(RH_all_post_NP)\n\n# means_LH_all_pre_P = np.mean(flattened_LH_all_pre_P, axis=1)\n# means_LH_all_pre_NP = np.mean(flattened_LH_all_pre_NP, axis=1)\n# means_RH_all_pre_P = np.mean(flattened_RH_all_pre_P, axis=1)\n# means_RH_all_pre_NP = np.mean(flattened_RH_all_pre_NP, axis=1)\n# means_LH_all_post_P = np.mean(flattened_LH_all_post_P, axis=1)\n# means_LH_all_post_NP = np.mean(flattened_LH_all_post_NP, axis=1)\n# means_RH_all_post_P = np.mean(flattened_RH_all_post_P, axis=1)\n# means_RH_all_post_NP = np.mean(flattened_RH_all_post_NP, axis=1)\n# print(means_LH_all_pre_NP)\n#\n# for i in range(flattened_LH_all_pre_P.shape[0]):\n#     flattened_LH_all_pre_P[i, :] -= means_LH_all_pre_P[i]\n#\n# for i in range(flattened_LH_all_pre_NP.shape[0]):\n#     flattened_LH_all_pre_NP[i, :] -= means_LH_all_pre_NP[i]\n#\n# for i in range(flattened_RH_all_pre_P.shape[0]):\n#     flattened_RH_all_pre_P[i, :] -= means_RH_all_pre_P[i]\n#\n# for i in range(flattened_RH_all_pre_NP.shape[0]):\n#     flattened_RH_all_pre_NP[i, :] -= means_RH_all_pre_NP[i]\n#\n# for i in range(flattened_LH_all_post_P.shape[0]):\n#     flattened_LH_all_post_P[i, :] -= means_LH_all_post_P[i]\n#\n# for i in range(flattened_LH_all_post_NP.shape[0]):\n#     flattened_LH_all_post_NP[i, :] -= means_LH_all_post_NP[i]\n#\n# for i in range(flattened_RH_all_post_P.shape[0]):\n#     flattened_RH_all_post_P[i, :] -= means_RH_all_post_P[i]\n#\n# for i in range(flattened_RH_all_post_NP.shape[0]):\n#     flattened_RH_all_post_NP[i, :] -= means_RH_all_post_NP[i]\n#\n# deviation_pre_P = np.vstack((flattened_LH_all_pre_P, flattened_RH_all_pre_P))\n# deviation_pre_NP = np.vstack((flattened_LH_all_pre_NP, flattened_RH_all_pre_NP))\n# deviation_post_P = np.vstack((flattened_LH_all_post_P, flattened_RH_all_post_P))\n# deviation_post_NP = np.vstack((flattened_LH_all_post_NP, flattened_RH_all_post_NP))\n# plot_data(deviation_post_P)\n\n# Print the results but datatype np.float\n# print(\"LH Means:\\n\", means_LH)\n# print(\"LH Standard Deviations:\\n\", std_devs_LH)\n# print(\"RH Means:\\n\", means_RH)\n# print(\"RH Standard Deviations:\\n\", std_devs_RH)\n\n\n# print(\"LH Means:\")\n# print([float(mean) for mean in means_LH])\n# print(\"LH Standard Deviations:\")\n# print([float(std_dev) for std_dev in std_devs_LH])\n# print(\"RH Means:\")\n# print([float(mean) for mean in means_RH])\n# print(\"RH Standard Deviations:\")\n# print([float(std_dev) for std_dev in std_devs_RH])\n\n# LH_all_pre_normalised=flatten_3d_to_2d(LH_all_pre)\n# LH_all_pre_raw=LH_all_pre_normalised\n# for i in range(0,len(means_LH_all_pre)):\n#   LH_all_pre_normalised[i]=z_normalize(LH_all_pre_normalised[i],means_LH_all_pre[i],std_devs_LH_all_pre[i])\n#\n# RH_all_pre_normalised=flatten_3d_to_2d(RH_all_pre)\n# RH_all_pre_raw=RH_all_pre_normalised\n# for i in range(0,len(means_RH_all_pre)):\n#   RH_all_pre_normalised[i]=z_normalize(RH_all_pre_normalised[i],means_RH_all_pre[i],std_devs_RH_all_pre[i])\n#\n# LH_all_post_normalised=flatten_3d_to_2d(LH_all_post)\n# LH_all_post_raw=LH_all_post_normalised\n# for i in range(0,len(means_LH_all_post)):\n#   LH_all_post_normalised[i]=z_normalize(LH_all_post_normalised[i],means_LH_all_post[i],std_devs_LH_all_post[i])\n#\n# RH_all_post_normalised=flatten_3d_to_2d(RH_all_post)\n# RH_all_post_raw=RH_all_post_normalised\n# for i in range(0,len(means_RH_all_post)):\n#   RH_all_post_normalised[i]=z_normalize(RH_all_post_normalised[i],means_RH_all_post[i],std_devs_RH_all_post[i])\n#\n#\n#\n#\n# normalised_variances_pre_LH = []\n# normalised_kurtoses_pre_LH = []\n# for array in LH_all_pre_normalised:\n#     var, kurt = process_batches_for_normalised(array)\n#     normalised_variances_pre_LH.append(var)\n#     normalised_kurtoses_pre_LH.append(kurt)\n# normalised_variances_pre_LH = np.array(normalised_variances_pre_LH)\n# normalised_kurtoses_pre_LH = np.array(normalised_kurtoses_pre_LH)\n#\n#\n# normalised_variances_pre_RH = []\n# normalised_kurtoses_pre_RH = []\n# for array in RH_all_pre_normalised:\n#     var, kurt = process_batches_for_normalised(array)\n#     normalised_variances_pre_RH.append(var)\n#     normalised_kurtoses_pre_RH.append(kurt)\n# normalised_variances_pre_RH = np.array(normalised_variances_pre_RH)\n# normalised_kurtoses_pre_RH = np.array(normalised_kurtoses_pre_RH)\n#\n#\n# raw_means_pre_LH=[]\n# raw_stddev_pre_LH=[]\n# raw_variance_pre_LH=[]\n# raw_skewness_pre_LH=[]\n# for array in LH_all_pre_raw:\n#     mean, std, var, skew = process_batches_raw(array)\n#     raw_means_pre_LH.append(mean)\n#     raw_stddev_pre_LH.append(std)\n#     raw_variance_pre_LH.append(var)\n#     raw_skewness_pre_LH.append(skew)\n# raw_means_pre_LH = np.array(raw_means_pre_LH)\n# raw_stddev_pre_LH = np.array(raw_stddev_pre_LH)\n# raw_variance_pre_LH=np.array(raw_variance_pre_LH)\n# raw_skewness_pre_LH=np.array(raw_skewness_pre_LH)\n#\n#\n# raw_means_pre_RH=[]\n# raw_stddev_pre_RH=[]\n# raw_variance_pre_RH=[]\n# raw_skewness_pre_RH=[]\n# for array in RH_all_pre_raw:\n#     mean, std, var, skew = process_batches_raw(array)\n#     raw_means_pre_RH.append(mean)\n#     raw_stddev_pre_RH.append(std)\n#     raw_variance_pre_RH.append(var)\n#     raw_skewness_pre_RH.append(skew)\n# raw_means_pre_RH = np.array(raw_means_pre_RH)\n# raw_stddev_pre_RH = np.array(raw_stddev_pre_RH)\n# raw_variance_pre_RH=np.array(raw_variance_pre_RH)\n# raw_skewness_pre_RH=np.array(raw_skewness_pre_RH)\n#\n#\n#\n# normalised_variances_post_LH = []\n# normalised_kurtoses_post_LH = []\n# for array in LH_all_post_normalised:\n#     var, kurt = process_batches_for_normalised(array)\n#     normalised_variances_post_LH.append(var)\n#     normalised_kurtoses_post_LH.append(kurt)\n# normalised_variances_post_LH = np.array(normalised_variances_post_LH)\n# normalised_kurtoses_post_LH = np.array(normalised_kurtoses_post_LH)\n#\n#\n# normalised_variances_post_RH = []\n# normalised_kurtoses_post_RH = []\n# for array in RH_all_post_normalised:\n#     var, kurt = process_batches_for_normalised(array)\n#     normalised_variances_post_RH.append(var)\n#     normalised_kurtoses_post_RH.append(kurt)\n# normalised_variances_post_RH = np.array(normalised_variances_post_RH)\n# normalised_kurtoses_post_RH = np.array(normalised_kurtoses_post_RH)\n#\n#\n# raw_means_post_LH=[]\n# raw_stddev_post_LH=[]\n# raw_variance_post_LH=[]\n# raw_skewness_post_LH=[]\n# for array in LH_all_post_raw:\n#     mean, std, var, skew = process_batches_raw(array)\n#     raw_means_post_LH.append(mean)\n#     raw_stddev_post_LH.append(std)\n#     raw_variance_post_LH.append(var)\n#     raw_skewness_post_LH.append(skew)\n# raw_means_post_LH = np.array(raw_means_post_LH)\n# raw_stddev_post_LH = np.array(raw_stddev_post_LH)\n# raw_variance_post_LH=np.array(raw_variance_post_LH)\n# raw_skewness_post_LH=np.array(raw_skewness_post_LH)\n#\n#\n# raw_means_post_RH=[]\n# raw_stddev_post_RH=[]\n# raw_variance_post_RH=[]\n# raw_skewness_post_RH=[]\n# for array in RH_all_post_raw:\n#     mean, std, var, skew = process_batches_raw(array)\n#     raw_means_post_RH.append(mean)\n#     raw_stddev_post_RH.append(std)\n#     raw_variance_post_RH.append(var)\n#     raw_skewness_post_RH.append(skew)\n# raw_means_post_RH = np.array(raw_means_post_RH)\n# raw_stddev_post_RH = np.array(raw_stddev_post_RH)\n# raw_variance_post_RH=np.array(raw_variance_post_RH)\n# raw_skewness_post_RH=np.array(raw_skewness_post_RH)","metadata":{"execution":{"iopub.status.busy":"2025-01-22T06:34:56.384908Z","iopub.execute_input":"2025-01-22T06:34:56.385485Z","iopub.status.idle":"2025-01-22T06:35:54.298128Z","shell.execute_reply.started":"2025-01-22T06:34:56.385441Z","shell.execute_reply":"2025-01-22T06:35:54.296918Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/datanew/data2\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# RAW DATA PLOTS****","metadata":{}},{"cell_type":"code","source":"def plot_data(matrix, save_path):\n    sns.set(style=\"whitegrid\")\n    matrix = np.array(matrix)\n\n    plt.figure(figsize=(20, 10))\n#     palette = sns.color_palette(\"husl\", matrix.shape[0])\n    for i in range(matrix.shape[0]):\n        plt.plot(matrix[i, :],color='blue', linewidth=1.0)\n    plt.xlabel('TIME', fontsize=14)\n    plt.ylabel('AMPLITUDE', fontsize=14)\n    plt.ylim(-20,15)\n    plt.savefig(save_path, format='jpg', dpi=300)\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"flattened_LH_all_pre_P= flatten_3d_to_2d_col(LH_all_pre_P)\nflattened_LH_all_pre_NP= flatten_3d_to_2d_col(LH_all_pre_NP)\nflattened_RH_all_pre_P= flatten_3d_to_2d_col(RH_all_pre_P)\nflattened_RH_all_pre_NP= flatten_3d_to_2d_col(RH_all_pre_NP)\nflattened_LH_all_post_P= flatten_3d_to_2d_col(LH_all_post_P)\nflattened_LH_all_post_NP= flatten_3d_to_2d_col(LH_all_post_NP)\nflattened_RH_all_post_P= flatten_3d_to_2d_col(RH_all_post_P)\nflattened_RH_all_post_NP= flatten_3d_to_2d_col(RH_all_post_NP)","metadata":{"execution":{"iopub.status.busy":"2025-01-22T06:35:54.299921Z","iopub.execute_input":"2025-01-22T06:35:54.300415Z","iopub.status.idle":"2025-01-22T06:35:54.32792Z","shell.execute_reply.started":"2025-01-22T06:35:54.30038Z","shell.execute_reply":"2025-01-22T06:35:54.326723Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"plot_data(flattened_LH_all_pre_P,'LH_all_pre_P_new.jpg')\nplot_data(flattened_RH_all_pre_P,'RH_all_pre_P_new.jpg')\nplot_data(flattened_LH_all_post_P,'LH_all_post_P_new.jpg')\nplot_data(flattened_RH_all_post_P,'RH_all_post_P_new.jpg')\nplot_data(flattened_LH_all_pre_NP,'LH_all_pre_NP_new.jpg')\nplot_data(flattened_RH_all_pre_NP,'RH_all_pre_NP_new.jpg')\nplot_data(flattened_LH_all_post_NP,'LH_all_post_NP_new.jpg')\nplot_data(flattened_RH_all_post_NP,'RH_all_post_NP_new.jpg')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DEVIATION PLOTS****","metadata":{}},{"cell_type":"code","source":"def plot_data(matrix, save_path):\n    sns.set(style=\"whitegrid\")\n    matrix = np.array(matrix)\n\n    plt.figure(figsize=(20, 10))\n#     palette = sns.color_palette(\"husl\", matrix.shape[0])\n    for i in range(matrix.shape[0]):\n        plt.plot(matrix[i, :],color='blue', linewidth=1.0)\n    plt.xlabel('TIME', fontsize=14)\n    plt.ylabel('AMPLITUDE', fontsize=14)\n    plt.ylim(-8,8)\n    plt.savefig(save_path, format='jpg', dpi=300)\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_deviations(LH_all_pre_P, RH_all_pre_P):\n    # Initialize output arrays with the same shapes\n    LH_all_pre_P_dev = np.zeros_like(LH_all_pre_P)\n    RH_all_pre_P_dev = np.zeros_like(RH_all_pre_P)\n\n    # Loop through each 2D array (along the first axis) for LH_all_pre_P\n    for i in range(LH_all_pre_P.shape[0]):\n        mean_values = np.mean(LH_all_pre_P[i], axis=0)\n        LH_all_pre_P_dev[i] = LH_all_pre_P[i] - mean_values\n\n    # Loop through each 2D array (along the first axis) for RH_all_pre_P\n    for i in range(RH_all_pre_P.shape[0]):\n        mean_values = np.mean(RH_all_pre_P[i], axis=0)\n        RH_all_pre_P_dev[i] = RH_all_pre_P[i] - mean_values\n    flattened_LH_all_pre_P= flatten_3d_to_2d_col(LH_all_pre_P_dev)\n    flattened_RH_all_pre_P= flatten_3d_to_2d_col(RH_all_pre_P_dev)\n    deviation_pre_P = np.vstack((flattened_LH_all_pre_P, flattened_RH_all_pre_P))\n\n    return deviation_pre_P\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"deviation_pre_P=calculate_deviations(LH_all_pre_P, RH_all_pre_P)\ndeviation_post_P=calculate_deviations(LH_all_post_P, RH_all_post_P)\ndeviation_pre_NP=calculate_deviations(LH_all_pre_NP, RH_all_pre_NP)\ndeviation_post_NP=calculate_deviations(LH_all_post_NP, RH_all_post_NP)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_data(deviation_pre_P,'deviation_pre_P_new.jpg')\nplot_data(deviation_post_P,'deviation_post_P_new.jpg')\nplot_data(deviation_pre_NP,'deviation_pre_NP_new.jpg')\nplot_data(deviation_post_NP,'deviation_post_NP_new.jpg')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CSV OF MEANS (190)****","metadata":{}},{"cell_type":"code","source":"def median_of_means(LH_all_pre_P, RH_all_pre_P):\n\n    combined_data = np.concatenate((LH_all_pre_P, RH_all_pre_P), axis=0)\n\n    #Number of states (columns)\n\n    num_states = combined_data.shape[2] # Assuming 5 states as columns \n    means_per_state = []\n\n    # Calculate mean of each column in each 2D array\n\n    for state in range(num_states): \n        means_for_state = []\n\n        for matrix in combined_data: \n            means_for_state.append(np.mean(matrix[:, state])) # Mean of each column for state \n        means_per_state.append(means_for_state)\n\n    return means_per_state","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x=median_of_means(LH_all_pre_P, RH_all_pre_P)\n# (np.array(x)).tofile('p_pre.csv', sep=',')\n# x=median_of_means(LH_all_post_P, RH_all_post_P)\n# (np.array(x)).tofile('p_post.csv', sep=',')\n# x=median_of_means(LH_all_post_NP, RH_all_post_NP)\n# (np.array(x)).tofile('np_post.csv', sep=',')\n# x=median_of_means(LH_all_pre_NP, RH_all_pre_NP)\n# (np.array(x)).tofile('np_pre.csv', sep=',')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CSV OF STDS****","metadata":{}},{"cell_type":"code","source":"def median_of_stds(LH_all_pre_P, RH_all_pre_P):\n\n    combined_data = np.concatenate((LH_all_pre_P, RH_all_pre_P), axis=0)\n\n    #Number of states (columns)\n\n    num_states = combined_data.shape[2] # Assuming 5 states as columns \n    means_per_state = []\n\n    # Calculate mean of each column in each 2D array\n\n    for state in range(num_states): \n        means_for_state = []\n\n        for matrix in combined_data: \n            means_for_state.append(np.std(matrix[:, state])) # Mean of each column for state \n        means_per_state.append(means_for_state)\n\n    return means_per_state","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x=median_of_stds(LH_all_pre_P, RH_all_pre_P)\n# (np.array(x)).tofile('p_pre_std.csv', sep=',')\n# x=median_of_stds(LH_all_post_P, RH_all_post_P)\n# (np.array(x)).tofile('p_post_std.csv', sep=',')\n# x=median_of_stds(LH_all_post_NP, RH_all_post_NP)\n# (np.array(x)).tofile('np_post_std.csv', sep=',')\n# x=median_of_stds(LH_all_pre_NP, RH_all_pre_NP)\n# (np.array(x)).tofile('np_pre_std.csv', sep=',')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# BOX PLOTS OF MEAN****","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.special import erfcinv\n\ndef plot_median_of_mean(LH_all_pre_P, RH_all_pre_P, save_path):\n    combined_data = np.concatenate((LH_all_pre_P, RH_all_pre_P), axis=0)\n    \n    # Number of states (columns)\n    num_states = combined_data.shape[2]  # Assuming states are represented as columns\n    stds_per_state = []\n\n    # Calculate standard deviation of each column in each 2D array for each state\n    for state in range(num_states): \n        stds_for_state = []\n        for matrix in combined_data: \n            stds_for_state.append(np.mean(matrix[:, state]))  # Standard deviation for each state\n        stds_per_state.append(stds_for_state)\n\n    # Prepare data for boxplot\n    data_for_boxplot = [stds_per_state[state] for state in range(num_states)]\n\n    # Calculate outliers based on scaled MAD\n    c = -1 / (np.sqrt(2) * erfcinv(3 / 2))  # Scaling constant\n    outliers_per_state = []\n    \n    for state_data in data_for_boxplot:\n        median = np.median(state_data)\n        mad = np.median(np.abs(state_data - median))  # Median Absolute Deviation\n        scaled_mad = c * mad\n        \n        lower_bound = median - 3 * scaled_mad\n        upper_bound = median + 3 * scaled_mad\n        \n        # Store the outliers for each state, excluding those within the range (-16, 12) as normal\n        outliers = [\n            x for x in state_data \n            if (x < lower_bound or x > upper_bound)\n        ]\n        outliers_per_state.append(outliers)\n\n    # Plotting\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(data=data_for_boxplot, flierprops=dict(marker='+', markersize=10), whis=[0, 100])  # Include all data points\n#     sns.boxplot(data=data_for_boxplot, flierprops=dict(marker='+', markersize=10))\n    \n    # Manually plot outliers calculated above\n    for state in range(num_states):\n        plt.scatter([state] * len(outliers_per_state[state]), outliers_per_state[state], color='red', label='Outliers' if state == 0 else \"\", marker='o')\n\n    plt.xticks(ticks=np.arange(num_states), labels=[f'State {i+1}' for i in range(num_states)])\n    plt.ylabel('Mean')\n    plt.grid(True)\n    plt.ylim(-16, 12)\n    plt.savefig(save_path, format='jpg', dpi=300)\n    plt.show()\n\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_median_of_mean(LH_all_pre_P, RH_all_pre_P, 'mean_pre_P_new.jpg')\nplot_median_of_mean(LH_all_post_P, RH_all_post_P, 'mean_post_P_new.jpg')\nplot_median_of_mean(LH_all_post_NP, RH_all_post_NP, 'mean_post_NP_new.jpg')\nplot_median_of_mean(LH_all_pre_NP, RH_all_pre_NP, 'mean_pre_NP_new.jpg')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# BOX PLOTS OF STDS****","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.special import erfcinv\n\ndef plot_median_of_std(LH_all_pre_P, RH_all_pre_P, save_path):\n    combined_data = np.concatenate((LH_all_pre_P, RH_all_pre_P), axis=0)\n    \n    # Number of states (columns)\n    num_states = combined_data.shape[2]  # Assuming states are represented as columns\n    stds_per_state = []\n\n    # Calculate standard deviation of each column in each 2D array for each state\n    for state in range(num_states): \n        stds_for_state = []\n        for matrix in combined_data: \n            stds_for_state.append(np.std(matrix[:, state]))  # Standard deviation for each state\n        stds_per_state.append(stds_for_state)\n\n    # Prepare data for boxplot\n    data_for_boxplot = [stds_per_state[state] for state in range(num_states)]\n\n    # Calculate outliers based on scaled MAD\n    c = -1 / (np.sqrt(2) * erfcinv(3 / 2))  # Scaling constant\n    outliers_per_state = []\n    \n    for state_data in data_for_boxplot:\n        median = np.median(state_data)\n        mad = np.median(np.abs(state_data - median))  # Median Absolute Deviation\n        scaled_mad = c * mad\n        \n        lower_bound = median - 3 * scaled_mad\n        upper_bound = median + 3 * scaled_mad\n        \n        # Store the outliers for each state\n        outliers = [x for x in state_data if x < lower_bound or x > upper_bound]\n        outliers_per_state.append(outliers)\n\n    # Plotting\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(data=data_for_boxplot, flierprops=dict(marker='+', markersize=10))\n    \n    # Manually plot outliers calculated above\n#     for state in range(num_states):\n#         plt.scatter([state] * len(outliers_per_state[state]), outliers_per_state[state], label='Outliers' if state == 0 else \"\")\n\n    plt.xticks(ticks=np.arange(num_states), labels=[f'State {i+1}' for i in range(num_states)])\n    plt.ylabel('SD')\n    plt.grid(True)\n    plt.ylim(0, 5)\n    plt.savefig(save_path, format='jpg', dpi=300)  # Save the figure\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_median_of_std(LH_all_pre_P, RH_all_pre_P, 'std_pre_P_new.jpg')\nplot_median_of_std(LH_all_pre_NP, RH_all_pre_NP, 'std_pre_NP_new.jpg')\nplot_median_of_std(LH_all_post_P, RH_all_post_P, 'std_post_P_new.jpg')\nplot_median_of_std(LH_all_post_NP, RH_all_post_NP, 'std_post_NP_new.jpg')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SELECTED LINES PLOTTING****","metadata":{}},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import numpy as np\n\n# # Function to plot based on selected lines\n# def plot_selected_lines(matrix, selected_lines):\n#     matrix = np.array(matrix)\n#     plt.figure(figsize=(10, 5))\n    \n#     for i in selected_lines:\n#         plt.plot(matrix[i, :], label=f'Line {i+1}')\n    \n#     plt.title('Selected Lines')\n#     plt.xlabel('X-axis')\n#     plt.ylabel('Y-axis')\n#     plt.legend()\n#     plt.show()\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initial plot of all lines\n# plot_selected_lines(matrix, list(range(matrix.shape[0])))\n\n# # Loop to allow user to select lines until they wish to stop\n# while True:\n#     # Simulating user input for line selection\n#     selected_lines = input(\"Enter the indices of lines to plot (e.g., 0, 2, 3): \")\n#     selected_lines = [int(i) for i in selected_lines.split(\",\")]\n    \n#     # Re-plot based on user selection\n#     plot_selected_lines(matrix, selected_lines)\n    \n#     # Ask the user if they wish to continue\n#     continue_plotting = input(\"Do you wish to continue? (yes/no): \").strip().lower()\n    \n#     # Check if user input means 'no', if yes, break the loop\n#     if continue_plotting in ['no', 'n']:\n#         print(\"Exiting plotting.\")\n#         break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# flattened_LH_all_pre_P= flatten_3d_to_2d_col(LH_all_pre_P)\n# plot_selected_lines(flattened_LH_all_pre_P, list(range(flattened_LH_all_pre_P.shape[0])))\n# while True:\n#     selected_lines = input(\"Enter the indices of lines to plot (e.g., 0, 2, 3): \")\n#     selected_lines = [int(i) for i in selected_lines.split(\",\")]\n#     plot_selected_lines(flattened_LH_all_pre_P, selected_lines)\n#     continue_plotting = input(\"Do you wish to continue? (yes/no): \").strip().lower()\n\n#     # Check if user input means 'no', if yes, break the loop\n#     if continue_plotting in ['no', 'n']:\n#         print(\"Exiting plotting.\")\n#         break\n\n        \n# plot_selected_lines(flattened_LH_all_pre_P)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# NEXT ...****","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Function to calculate mean and standard deviation for 1D arrays\ndef calculate_stats(arr):\n    mean = np.mean(arr)\n    std = np.std(arr)\n    return mean, std\n\n# Function to divide a 1D array into sets of 200 points and calculate stats for each set\ndef segment_stats(arr, segment_size=200):\n    segments = [arr[i:i+segment_size] for i in range(0, len(arr), segment_size)]\n    stats = [calculate_stats(segment) for segment in segments if len(segment) == segment_size]\n    return stats\n\n# Compute mean and standard deviation for each 1D array\noverall_stats_LH_all_pre_P = [calculate_stats(row) for row in flattened_LH_all_pre_P]\noverall_stats_LH_all_post_P = [calculate_stats(row) for row in flattened_LH_all_post_P]\noverall_stats_RH_all_pre_P = [calculate_stats(row) for row in flattened_RH_all_pre_P]\noverall_stats_RH_all_post_P = [calculate_stats(row) for row in flattened_RH_all_post_P]\n\noverall_stats_LH_all_pre_NP = [calculate_stats(row) for row in flattened_LH_all_pre_NP]\noverall_stats_LH_all_post_NP = [calculate_stats(row) for row in flattened_LH_all_post_NP]\noverall_stats_RH_all_pre_NP = [calculate_stats(row) for row in flattened_RH_all_pre_NP]\noverall_stats_RH_all_post_NP = [calculate_stats(row) for row in flattened_RH_all_post_NP]\n\n# Define a function to normalize the array using mean and std\ndef normalize_array(arr, mean, std):\n    return (arr - mean) / std\n\n# Normalize the flattened arrays by using the calculated stats\nnormalized_LH_all_pre_P = [\n    normalize_array(row, *overall_stats_LH_all_pre_P[i]) for i, row in enumerate(flattened_LH_all_pre_P)\n]\nnormalized_LH_all_post_P = [\n    normalize_array(row, *overall_stats_LH_all_post_P[i]) for i, row in enumerate(flattened_LH_all_post_P)\n]\nnormalized_RH_all_pre_P = [\n    normalize_array(row, *overall_stats_RH_all_pre_P[i]) for i, row in enumerate(flattened_RH_all_pre_P)\n]\nnormalized_RH_all_post_P = [\n    normalize_array(row, *overall_stats_RH_all_post_P[i]) for i, row in enumerate(flattened_RH_all_post_P)\n]\n\nnormalized_LH_all_pre_NP = [\n    normalize_array(row, *overall_stats_LH_all_pre_NP[i]) for i, row in enumerate(flattened_LH_all_pre_NP)\n]\nnormalized_LH_all_post_NP = [\n    normalize_array(row, *overall_stats_LH_all_post_NP[i]) for i, row in enumerate(flattened_LH_all_post_NP)\n]\nnormalized_RH_all_pre_NP = [\n    normalize_array(row, *overall_stats_RH_all_pre_NP[i]) for i, row in enumerate(flattened_RH_all_pre_NP)\n]\nnormalized_RH_all_post_NP = [\n    normalize_array(row, *overall_stats_RH_all_post_NP[i]) for i, row in enumerate(flattened_RH_all_post_NP)\n]\n\n\n# Compute segment-wise stats for each 1D array\nsegmented_LH_all_pre_P = [segment_stats(row) for row in flattened_LH_all_pre_P]\nsegmented_LH_all_post_P = [segment_stats(row) for row in flattened_LH_all_post_P]\nsegmented_RH_all_pre_P = [segment_stats(row) for row in flattened_RH_all_pre_P]\nsegmented_RH_all_post_P = [segment_stats(row) for row in flattened_RH_all_post_P]\n\nsegmented_LH_all_pre_NP = [segment_stats(row) for row in flattened_LH_all_pre_NP]\nsegmented_LH_all_post_NP = [segment_stats(row) for row in flattened_LH_all_post_NP]\nsegmented_RH_all_pre_NP = [segment_stats(row) for row in flattened_RH_all_pre_NP]\nsegmented_RH_all_post_NP = [segment_stats(row) for row in flattened_RH_all_post_NP]\n\ndef divide_into_segments(arr, segment_size=200):\n    # Split the array into segments of the specified size\n    segments = [arr[i:i+segment_size] for i in range(0, len(arr), segment_size)]\n    return segments\n\n# Normalize segments for each row in the 2D array\nsegments_LH_all_pre_P = [\n    divide_into_segments(row)\n    for idx, row in enumerate(flattened_LH_all_pre_P)\n]\nsegments_LH_all_post_P = [\n    divide_into_segments(row)\n    for idx, row in enumerate(flattened_LH_all_post_P)\n]\nsegments_RH_all_pre_P = [\n    divide_into_segments(row)\n    for idx, row in enumerate(flattened_RH_all_pre_P)\n]\nsegments_RH_all_post_P = [\n    divide_into_segments(row)\n    for idx, row in enumerate(flattened_RH_all_post_P)\n]\n\nsegments_LH_all_pre_NP = [\n    divide_into_segments(row)\n    for idx, row in enumerate(flattened_LH_all_pre_NP)\n]\nsegments_LH_all_post_NP = [\n    divide_into_segments(row)\n    for idx, row in enumerate(flattened_LH_all_post_NP)\n]\nsegments_RH_all_pre_NP = [\n    divide_into_segments(row)\n    for idx, row in enumerate(flattened_RH_all_pre_NP)\n]\nsegments_RH_all_post_NP = [\n    divide_into_segments(row)\n    for idx, row in enumerate(flattened_RH_all_post_NP)\n]\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T06:36:05.152321Z","iopub.execute_input":"2025-01-22T06:36:05.152739Z","iopub.status.idle":"2025-01-22T06:36:05.893593Z","shell.execute_reply.started":"2025-01-22T06:36:05.152702Z","shell.execute_reply":"2025-01-22T06:36:05.892407Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from scipy.stats import skew, kurtosis\nimport numpy as np\n\n# Function to calculate desired statistics for a single segment\ndef calculate_extended_stats(segment):\n    # Convert the segment to a numeric type (float) to avoid object type issues\n    segment = np.asarray(segment, dtype=np.float64)\n    \n    # Handle any NaN values appropriately using nan_policy='omit'\n    mean = np.nanmean(segment)  # Use nanmean to ignore NaN values\n    std = np.nanstd(segment)    # Use nanstd to ignore NaN values\n    var = np.nanvar(segment)    # Use nanvar to ignore NaN values\n    skewness = skew(segment, nan_policy='omit')  # Omit NaN values in skew calculation\n    kurt = kurtosis(segment, nan_policy='omit')  # Omit NaN values in kurtosis calculation\n    \n    return mean, std, var, skewness, kurt\n\n# Calculate statistics for all rows\ndef calculate_row_stats(normalized_row_segments):\n    row_stats = [calculate_extended_stats(segment) for segment in normalized_row_segments]\n    return row_stats\n\n# Example calculation for all rows (you will use your actual segments here)\nall_rows_stats_LH_pre_P = [\n    calculate_row_stats(row) for row in segments_LH_all_pre_P\n]\nall_rows_stats_LH_post_P = [\n    calculate_row_stats(row) for row in segments_LH_all_post_P\n]\nall_rows_stats_RH_pre_P = [\n    calculate_row_stats(row) for row in segments_RH_all_pre_P\n]\nall_rows_stats_RH_post_P = [\n    calculate_row_stats(row) for row in segments_RH_all_post_P\n]\n\nall_rows_stats_LH_pre_NP = [\n    calculate_row_stats(row) for row in segments_LH_all_pre_NP\n]\nall_rows_stats_LH_post_NP = [\n    calculate_row_stats(row) for row in segments_LH_all_post_NP\n]\nall_rows_stats_RH_pre_NP = [\n    calculate_row_stats(row) for row in segments_RH_all_pre_NP\n]\nall_rows_stats_RH_post_NP = [\n    calculate_row_stats(row) for row in segments_RH_all_post_NP\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T06:37:12.95386Z","iopub.execute_input":"2025-01-22T06:37:12.954741Z","iopub.status.idle":"2025-01-22T06:37:22.564411Z","shell.execute_reply.started":"2025-01-22T06:37:12.954695Z","shell.execute_reply":"2025-01-22T06:37:22.563159Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import csv\n\n# Prepare header for the CSV file\nheader = [\"Row\", \"Mean\", \"Std\", \"Variance\", \"Skewness\", \"Kurtosis\", \"resp\"]\n\n# Create a function to format the data into rows for CSV writing\ndef format_stats_for_csv(stats, resp_label):\n    csv_rows = []\n    for row_index, row_stats in enumerate(stats):\n        for segment_index, (mean, std, var, skewness, kurt) in enumerate(row_stats):\n            csv_rows.append([segment_index + 1, mean, std, var, skewness, kurt, resp_label])\n    return csv_rows\n\n# Prepare the data for CSV by formatting all rows for each dataset\ncsv_data = []\n\n# Append data for each dataset\ncsv_data += format_stats_for_csv(all_rows_stats_LH_pre_P, \"lh pre p\")\ncsv_data += format_stats_for_csv(all_rows_stats_LH_post_P, \"lh post p\")\ncsv_data += format_stats_for_csv(all_rows_stats_RH_pre_P, \"rh pre p\")\ncsv_data += format_stats_for_csv(all_rows_stats_RH_post_P, \"rh post p\")\n\ncsv_data += format_stats_for_csv(all_rows_stats_LH_pre_NP, \"lh pre np\")\ncsv_data += format_stats_for_csv(all_rows_stats_LH_post_NP, \"lh post np\")\ncsv_data += format_stats_for_csv(all_rows_stats_RH_pre_NP, \"rh pre np\")\ncsv_data += format_stats_for_csv(all_rows_stats_RH_post_NP, \"rh post np\")\n\n# Write the data to a CSV file\ndef write_stats_to_csv(csv_data, filename='stats_output.csv'):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(header)  # Writing the header row\n        writer.writerows(csv_data)  # Writing the data rows\n    print(f\"CSV file '{filename}' has been created.\")\n\n# Write to the CSV\nwrite_stats_to_csv(csv_data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T06:37:31.110056Z","iopub.execute_input":"2025-01-22T06:37:31.110446Z","iopub.status.idle":"2025-01-22T06:37:31.182355Z","shell.execute_reply.started":"2025-01-22T06:37:31.110411Z","shell.execute_reply":"2025-01-22T06:37:31.181101Z"}},"outputs":[{"name":"stdout","text":"CSV file 'stats_output.csv' has been created.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import csv\n\n# Prepare data for CSV\ncsv_data = [[\"Row\", \"Segment\", \"Mean\", \"Std\", \"Variance\", \"Skewness\", \"Kurtosis\", \"resp\"]]\n\n# Define the mapping for each stats list to the corresponding \"resp\" value\nstats_to_resp = {\n    \"all_rows_stats_LH_pre_P\": \"LH pre Player\",\n    \"all_rows_stats_LH_post_P\": \"LH post Player\",\n    \"all_rows_stats_RH_pre_P\": \"RH pre Player\",\n    \"all_rows_stats_RH_post_P\": \"RH post Player\",\n    \"all_rows_stats_LH_pre_NP\": \"LH pre Non-Player\",\n    \"all_rows_stats_LH_post_NP\": \"LH post Non-Player\",\n    \"all_rows_stats_RH_pre_NP\": \"RH pre Non-Player\",\n    \"all_rows_stats_RH_post_NP\": \"RH post Non-Player\"\n}\n\n# Define all your stats datasets here\nall_stats_dict = {\n    \"all_rows_stats_LH_pre_P\": all_rows_stats_LH_pre_P,\n    \"all_rows_stats_LH_post_P\": all_rows_stats_LH_post_P,\n    \"all_rows_stats_RH_pre_P\": all_rows_stats_RH_pre_P,\n    \"all_rows_stats_RH_post_P\": all_rows_stats_RH_post_P,\n    \"all_rows_stats_LH_pre_NP\": all_rows_stats_LH_pre_NP,\n    \"all_rows_stats_LH_post_NP\": all_rows_stats_LH_post_NP,\n    \"all_rows_stats_RH_pre_NP\": all_rows_stats_RH_pre_NP,\n    \"all_rows_stats_RH_post_NP\": all_rows_stats_RH_post_NP\n}\n\n# Loop through each dataset and add the corresponding \"resp\"\nfor stats_name, stats_data in all_stats_dict.items():\n    resp_value = stats_to_resp[stats_name]\n    \n    # Add data to csv_data with the 'resp' column\n    for row_idx, row_stats in enumerate(stats_data):\n        for seg_idx, stats in enumerate(row_stats):\n            csv_data.append([row_idx + 1, seg_idx + 1] + list(stats) + [resp_value])\n\n# Write to CSV\noutput_csv_path = \"stats_with_all_resps.csv\"\nwith open(output_csv_path, mode=\"w\", newline=\"\") as file:\n    writer = csv.writer(file)\n    writer.writerows(csv_data)\n\nprint(f\"CSV file saved to {output_csv_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\nimport pandas as pd\n\n# Mapping of row numbers to sub-codes\nrow_to_subcode = {\n    1: \"S041\", 2: \"S041\", 3: \"S046\", 4: \"S045\", 5: \"SO44\",\n    6: \"S041\", 7: \"S042\", 8: \"SO49\", 9: \"S057\", 10: \"SO53\",\n    11: \"S055\", 12: \"SO54\", 13: \"S058\", 14: \"S064\"\n}\n\n# Load the CSV file\ninput_csv_path = \"LH_all_pre_stats.csv\"\ndf = pd.read_csv(input_csv_path)\n\n# We assume 60 segments per row. We will replace 'Row' column values in sets of 60\nsub_codes = []\n\n# Iterate through the rows in chunks of 60\nfor i in range(0, len(df), 60):\n    # Find the sub-code for the current row's group\n    row_num = (i // 60) + 1  # Determine the row number based on chunk position\n    sub_code = row_to_subcode.get(row_num, \"Unknown\")\n    \n    # For each 60-row block, assign the corresponding sub-code\n    sub_codes.extend([sub_code] * 60)\n\n# Replace the 'Row' column with the sub-codes for the entire chunk\ndf['Sub-Code'] = sub_codes\ndf['resp'] = \"LH pre Player\"\n\n# Drop the original 'Row' column\ndf.drop(columns=['Row'], inplace=True)\n\n# Save the updated CSV\noutput_csv_path = \"LH_all_pre_stats_with_subcodes.csv\"\ndf.to_csv(output_csv_path, index=False)\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_rows = len(all_rows_stats)  # Number of rows\nnum_segments_per_row = [len(row) for row in all_rows_stats]  # Number of segments per row\n\nprint(\"Shape of normalized_segments_LH_all_pre:\")\nprint(f\"Rows: {num_rows}\")\nprint(f\"Segments per row: {num_segments_per_row}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Function to divide each row into 5 sets of 200 points each\ndef divide_into_sets(array, num_sets=5, points_per_set=2400):\n    # Check that each row has the correct number of points (num_sets * points_per_set)\n    if array.shape[1] != num_sets * points_per_set:\n        raise ValueError(f\"Each row must have {num_sets * points_per_set} points to divide into {num_sets} sets of {points_per_set} points.\")\n    \n    # Split each row into the specified number of sets\n    sets = []\n    for row in array:\n        row_sets = np.split(row, num_sets)\n        sets.append(row_sets)\n    \n    return np.array(sets)\n\n# Combine arrays row-wise\ncombined_LH_RH_pre_P = np.vstack((flattened_LH_all_pre_P, flattened_RH_all_pre_P))\ncombined_LH_RH_pre_NP = np.vstack((flattened_LH_all_pre_NP, flattened_RH_all_pre_NP))\ncombined_LH_RH_post_P = np.vstack((flattened_LH_all_post_P, flattened_RH_all_post_P))\ncombined_LH_RH_post_NP = np.vstack((flattened_LH_all_post_NP, flattened_RH_all_post_NP))\n\n\n# Divide each row into 5 sets of 2400 points each\ndata_pre_P = divide_into_sets(combined_LH_RH_pre_P)\ndata_pre_NP = divide_into_sets(combined_LH_RH_pre_NP)\ndata_post_P = divide_into_sets(combined_LH_RH_post_P)\ndata_post_NP = divide_into_sets(combined_LH_RH_post_NP)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T06:39:33.429429Z","iopub.execute_input":"2025-01-22T06:39:33.429809Z","iopub.status.idle":"2025-01-22T06:39:33.480257Z","shell.execute_reply.started":"2025-01-22T06:39:33.429777Z","shell.execute_reply":"2025-01-22T06:39:33.479131Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"combined_LH_RH_pre_P.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef zci_and_m(data):\n    \"\"\"\n    Calculate zero-crossing index (zci) and slope (m) for subsets of 2400 points\n    for each row in the input data.\n\n    Parameters:\n        data (numpy.ndarray): Input data with dimensions (num_rows, num_columns).\n\n    Returns:\n        zci_all (numpy.ndarray): Zero-crossing indices for all subsets and rows.\n        m_all (numpy.ndarray): Slopes for all subsets and rows.\n    \"\"\"\n    num_rows, num_columns = data.shape\n    if num_columns % 2400 != 0:\n        raise ValueError(\"Number of columns in each row must be divisible by 2400.\")\n    \n    # Number of subsets per row\n    num_subsets = num_columns // 2400\n\n    # Initialize results\n    zci_all = np.zeros((num_rows, num_subsets))\n    m_all = np.zeros((num_rows, num_subsets))\n    \n    # Process each row\n    for row_idx in range(num_rows):\n        for subset_idx in range(num_subsets):\n            # Extract the subset for the current row\n            start_idx = subset_idx * 2400\n            end_idx = start_idx + 2400\n            subset = data[row_idx, start_idx:end_idx]\n            \n            # Normalize data by subtracting the mean\n            db_data = subset - np.mean(subset)\n            \n            # Find the sign of the db_data values\n            sgn = np.sign(db_data)\n            zero_crossings = []\n            \n            # Detect zero-crossings\n            for i in range(1, db_data.shape[0]):\n                if sgn[i] != sgn[i-1]:\n                    zero_crossings.append(i)\n            \n            # Find the crossing closest to the center\n            if zero_crossings:\n                ix = np.argmin(np.abs(np.array(zero_crossings) - 2400 / 2))\n                zci_all[row_idx, subset_idx] = zero_crossings[ix]\n                \n                # Calculate the numerator and denominator for slope\n                nu = (np.arange(2400) - zero_crossings[ix]) * db_data\n                dn = (np.arange(2400) - zero_crossings[ix]) ** 2\n                \n                # Compute slope\n                m_all[row_idx, subset_idx] = np.sum(nu) / np.sum(dn)\n    \n    # Normalize zci and scale m\n    zci_all /= 1200\n    m_all *= 1200\n    \n    return zci_all, m_all\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T06:40:23.844601Z","iopub.execute_input":"2025-01-22T06:40:23.845056Z","iopub.status.idle":"2025-01-22T06:40:23.856194Z","shell.execute_reply.started":"2025-01-22T06:40:23.845014Z","shell.execute_reply":"2025-01-22T06:40:23.854929Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"zci_m_results = {}\n\n# Compute zci and m for each combined dataset\nzci_m_results['LH_pre_P'] = zci_and_m(flattened_LH_all_pre_P)\nzci_m_results['RH_pre_P'] = zci_and_m(flattened_RH_all_pre_P)\nzci_m_results['LH_pre_NP'] = zci_and_m(flattened_LH_all_pre_NP)\nzci_m_results['RH_pre_NP'] = zci_and_m(flattened_RH_all_pre_NP)\nzci_m_results['LH_post_P'] = zci_and_m(flattened_LH_all_post_P)\nzci_m_results['RH_post_P'] = zci_and_m(flattened_RH_all_post_P)\nzci_m_results['LH_post_NP'] = zci_and_m(flattened_LH_all_post_NP)\nzci_m_results['RH_post_NP'] = zci_and_m(flattened_RH_all_post_NP)\n\n# Assign results to variables for clarity\nzci_LH_pre_P, m_LH_pre_P = zci_m_results['LH_pre_P']\nzci_RH_pre_P, m_RH_pre_P = zci_m_results['RH_pre_P']\nzci_LH_pre_NP, m_LH_pre_NP = zci_m_results['LH_pre_NP']\nzci_RH_pre_NP, m_RH_pre_NP = zci_m_results['RH_pre_NP']\nzci_LH_post_P, m_LH_post_P = zci_m_results['LH_post_P']\nzci_RH_post_P, m_RH_post_P = zci_m_results['RH_post_P']\nzci_LH_post_NP, m_LH_post_NP = zci_m_results['LH_post_NP']\nzci_RH_post_NP, m_RH_post_NP = zci_m_results['RH_post_NP']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T06:40:42.0029Z","iopub.execute_input":"2025-01-22T06:40:42.003313Z","iopub.status.idle":"2025-01-22T06:40:42.477443Z","shell.execute_reply.started":"2025-01-22T06:40:42.003275Z","shell.execute_reply":"2025-01-22T06:40:42.476191Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import numpy as np\nimport csv\n\n# Path to the CSV file located in Kaggle's output directory\ncsv_file_path = '/kaggle/working/stats_output.csv'\n\n\n\n# Function to repeat the zci and m values for each dataset\ndef repeat_values(values):\n    # Repeat each value in the set (axis=1 is for repeating along columns)\n    return np.repeat(values, 12, axis=1)\n\n# Repeat the zci and m values for all datasets\nzci_LH_pre_P_repeated = repeat_values(zci_LH_pre_P)\nzci_RH_pre_P_repeated = repeat_values(zci_RH_pre_P)\nzci_LH_pre_NP_repeated = repeat_values(zci_LH_pre_NP)\nzci_RH_pre_NP_repeated = repeat_values(zci_RH_pre_NP)\nzci_LH_post_P_repeated = repeat_values(zci_LH_post_P)\nzci_RH_post_P_repeated = repeat_values(zci_RH_post_P)\nzci_LH_post_NP_repeated = repeat_values(zci_LH_post_NP)\nzci_RH_post_NP_repeated = repeat_values(zci_RH_post_NP)\n\nm_LH_pre_P_repeated = repeat_values(m_LH_pre_P)\nm_RH_pre_P_repeated = repeat_values(m_RH_pre_P)\nm_LH_pre_NP_repeated = repeat_values(m_LH_pre_NP)\nm_RH_pre_NP_repeated = repeat_values(m_RH_pre_NP)\nm_LH_post_P_repeated = repeat_values(m_LH_post_P)\nm_RH_post_P_repeated = repeat_values(m_RH_post_P)\nm_LH_post_NP_repeated = repeat_values(m_LH_post_NP)\nm_RH_post_NP_repeated = repeat_values(m_RH_post_NP)\n\n# Combine the repeated zci and m values into single lists (all values under one column)\nzci_values = np.concatenate([zci_LH_pre_P_repeated.flatten(), zci_RH_pre_P_repeated.flatten(),\n                             zci_LH_pre_NP_repeated.flatten(), zci_RH_pre_NP_repeated.flatten(),\n                             zci_LH_post_P_repeated.flatten(), zci_RH_post_P_repeated.flatten(),\n                             zci_LH_post_NP_repeated.flatten(), zci_RH_post_NP_repeated.flatten()])\n\nm_values = np.concatenate([m_LH_pre_P_repeated.flatten(), m_RH_pre_P_repeated.flatten(),\n                           m_LH_pre_NP_repeated.flatten(), m_RH_pre_NP_repeated.flatten(),\n                           m_LH_post_P_repeated.flatten(), m_RH_post_P_repeated.flatten(),\n                           m_LH_post_NP_repeated.flatten(), m_RH_post_NP_repeated.flatten()])\n\n# Function to append the zci and m values to the existing CSV file\ndef append_zci_m_to_csv(zci_values, m_values, filename=csv_file_path):\n    # Read the existing rows of the CSV first\n    with open(filename, 'r', newline='') as read_file:\n        reader = csv.reader(read_file)\n        rows = list(reader)\n\n    # Ensure the number of zci and m values match the number of rows in the CSV\n    num_rows_in_csv = len(rows) - 1  # excluding the header row\n    total_values_in_flat = len(zci_values)  # Length of flattened arrays\n\n    if num_rows_in_csv != total_values_in_flat:\n        raise ValueError(f\"Number of rows in CSV does not match the number of zci and m values. \"\n                         f\"CSV rows: {num_rows_in_csv}, zci values: {total_values_in_flat}\")\n\n    # Now iterate and append zci and m values to the respective columns\n    index = 0\n    for i in range(1, len(rows)):  # Skip header row\n        # If the index exceeds the number of available zci and m values, skip appending\n        if index < len(zci_values):\n            zci = zci_values[index]\n            m = m_values[index]\n            rows[i].append(zci)  # Append zci value\n            rows[i].append(m)    # Append m value\n        index += 1\n        if index >= total_values_in_flat:\n            break  # Prevent out of bounds access if index exceeds flattened array size\n\n    # Write the modified rows back into the file\n    with open(filename, 'w', newline='') as write_file:\n        writer = csv.writer(write_file)\n        rows[0].extend([\"zci\", \"m\"]) \n        writer.writerows(rows)\n\n    print(f\"zci and m values have been appended to '{filename}'.\")\n\n# Call the function to append to the existing CSV\nappend_zci_m_to_csv(zci_values, m_values)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T06:43:30.961015Z","iopub.execute_input":"2025-01-22T06:43:30.961428Z","iopub.status.idle":"2025-01-22T06:43:31.037556Z","shell.execute_reply.started":"2025-01-22T06:43:30.961391Z","shell.execute_reply":"2025-01-22T06:43:31.036354Z"}},"outputs":[{"name":"stdout","text":"zci and m values have been appended to '/kaggle/working/stats_output.csv'.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import numpy as np\nfrom scipy.signal import welch\ndef pow2db(power):\n    return 10 * np.log10(power)\n\ndef mnf_and_pmnf(data, fs):\n    \"\"\"\n    Calculate Mean Frequency (MNF) for subsets of 2400 points for each row in the input data.\n\n    Parameters:\n        data (numpy.ndarray): Input data with dimensions (num_rows, num_columns).\n        fs (float): Sampling frequency.\n\n    Returns:\n        mnf_all (numpy.ndarray): Mean frequency values for all subsets and rows.\n    \"\"\"\n    data = np.array(data)\n    num_rows, num_columns = data.shape\n    if num_columns % 200 != 0:\n        raise ValueError(\"Number of columns in each row must be divisible by 2400.\")\n    \n    # Number of subsets per row\n    num_subsets = num_columns // 200\n\n    # Initialize results\n    mnf_all = np.zeros((num_rows, num_subsets))\n    pmnf_all = np.zeros((num_rows, num_subsets))\n    \n    \n    # Process each row\n    for row_idx in range(data.shape[0]):\n        for subset_idx in range(num_subsets):  # 5 subsets\n            start_idx = subset_idx * 200\n            end_idx = start_idx + 200\n            subset = data[row_idx, start_idx:end_idx]\n            \n            # Initialize variables for power and frequency calculation\n        # Initialize variables for power and frequency calculation\n            total_power_real = 0.0\n            total_power_imag = 0.0\n            freq_weighted_power_real = 0.0\n            freq_weighted_power_imag = 0.0\n            \n            # Calculate Power Spectral Density (PSD) using Welch's method\n            f, pxx = welch(subset, fs, nperseg=200, noverlap=0, nfft=200)\n    \n            # Perform calculations on real and imaginary parts of PSD\n            pxx_real = np.real(pxx)\n            pxx_imag = np.imag(pxx)\n    \n            # Calculate total power (separate real and imaginary sums)\n            total_power_real = np.sum(pxx_real)\n            total_power_imag = np.sum(pxx_imag)\n\n        # Calculate frequency * power and sum it for both real and imaginary parts\n            for i in range(len(f)):\n                freq_weighted_power_real += f[i] * pxx_real[i]\n                freq_weighted_power_imag += f[i] * pxx_imag[i]\n            \n            # Avoid division by zero and calculate Mean Frequency (MNF)\n            if (total_power_real + total_power_imag) != 0:\n                mnf = (freq_weighted_power_real + 1j * freq_weighted_power_imag) / (total_power_real + 1j * total_power_imag)\n                # Store only the real part of MNF (or real + imaginary parts if needed)\n                mnf_all[row_idx, subset_idx] = np.real(mnf)  # Store only real part\n            total_power = np.sqrt(total_power_real**2 + total_power_imag**2)\n            pmnf_all[row_idx, subset_idx] = pow2db(total_power)\n\n    \n    return mnf_all,pmnf_all\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T06:51:59.693857Z","iopub.execute_input":"2025-01-22T06:51:59.694277Z","iopub.status.idle":"2025-01-22T06:51:59.705735Z","shell.execute_reply.started":"2025-01-22T06:51:59.694239Z","shell.execute_reply":"2025-01-22T06:51:59.704465Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"mnf_pmnf_results = {}\nfs=50\n\n# Compute zci and m for each combined dataset\nmnf_pmnf_results['LH_pre_P'] = mnf_and_pmnf(normalized_LH_all_pre_P,fs)\nmnf_pmnf_results['RH_pre_P'] = mnf_and_pmnf(normalized_RH_all_pre_P,fs)\nmnf_pmnf_results['LH_pre_NP'] = mnf_and_pmnf(normalized_LH_all_pre_NP,fs)\nmnf_pmnf_results['RH_pre_NP'] = mnf_and_pmnf(normalized_RH_all_pre_NP,fs)\nmnf_pmnf_results['LH_post_P'] = mnf_and_pmnf(normalized_LH_all_post_P,fs)\nmnf_pmnf_results['RH_post_P'] = mnf_and_pmnf(normalized_RH_all_post_P,fs)\nmnf_pmnf_results['LH_post_NP'] = mnf_and_pmnf(normalized_LH_all_post_NP,fs)\nmnf_pmnf_results['RH_post_NP'] = mnf_and_pmnf(normalized_RH_all_post_NP,fs)\n\n# Assign results to variables for clarity\nmnf_LH_pre_P, pmnf_LH_pre_P = mnf_pmnf_results['LH_pre_P']\nmnf_RH_pre_P, pmnf_RH_pre_P = mnf_pmnf_results['RH_pre_P']\nmnf_LH_pre_NP, pmnf_LH_pre_NP = mnf_pmnf_results['LH_pre_NP']\nmnf_RH_pre_NP, pmnf_RH_pre_NP = mnf_pmnf_results['RH_pre_NP']\nmnf_LH_post_P, pmnf_LH_post_P = mnf_pmnf_results['LH_post_P']\nmnf_RH_post_P, pmnf_RH_post_P = mnf_pmnf_results['RH_post_P']\nmnf_LH_post_NP, pmnf_LH_post_NP = mnf_pmnf_results['LH_post_NP']\nmnf_RH_post_NP, pmnf_RH_post_NP = mnf_pmnf_results['RH_post_NP']\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T06:52:09.55829Z","iopub.execute_input":"2025-01-22T06:52:09.55924Z","iopub.status.idle":"2025-01-22T06:52:12.76072Z","shell.execute_reply.started":"2025-01-22T06:52:09.559182Z","shell.execute_reply":"2025-01-22T06:52:12.75961Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3044593448.py:66: ComplexWarning: Casting complex values to real discards the imaginary part\n  pmnf_all[row_idx, subset_idx] = pow2db(total_power)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import numpy as np\nimport csv\n\n# Path to the CSV file located in Kaggle's output directory\ncsv_file_path = '/kaggle/working/stats_output.csv'\n\n# Function to append the mnf and pmnf values to the existing CSV file\ndef append_mnf_pmnf_to_csv(mnf_values, pmnf_values, filename=csv_file_path):\n    # Read the existing rows of the CSV first\n    with open(filename, 'r', newline='') as read_file:\n        reader = csv.reader(read_file)\n        rows = list(reader)\n\n    # Ensure the number of mnf and pmnf values match the number of rows in the CSV\n    num_rows_in_csv = len(rows) - 1  # excluding the header row\n    total_values_in_flat = len(mnf_values)  # Length of flattened arrays\n\n\n    # Now iterate and append mnf and pmnf values to the respective columns\n    index = 0\n    for i in range(1, len(rows)):  # Skip header row\n        # If the index exceeds the number of available mnf and pmnf values, skip appending\n        if index < len(mnf_values):\n            mnf = mnf_values[index]\n            pmnf = pmnf_values[index]\n            rows[i].append(mnf)  # Append mnf value\n            rows[i].append(pmnf)  # Append pmnf value\n        index += 1\n        if index >= total_values_in_flat:\n            break  # Prevent out of bounds access if index exceeds flattened array size\n\n    # Write the modified rows back into the file\n    with open(filename, 'w', newline='') as write_file:\n        writer = csv.writer(write_file)\n        # Write the header first\n        rows[0].extend([\"mnf\", \"pmnf\"])  # Add the \"mnf\" and \"pmnf\" columns to the header\n        writer.writerows(rows)\n\n    print(f\"mnf and pmnf values have been appended to '{filename}'.\")\n\n# Combine the mnf and pmnf values into single lists (flattened arrays)\nmnf_values = np.concatenate([mnf_LH_pre_P.flatten(), mnf_RH_pre_P.flatten(),\n                             mnf_LH_pre_NP.flatten(), mnf_RH_pre_NP.flatten(),\n                             mnf_LH_post_P.flatten(), mnf_RH_post_P.flatten(),\n                             mnf_LH_post_NP.flatten(), mnf_RH_post_NP.flatten()])\n\npmnf_values = np.concatenate([pmnf_LH_pre_P.flatten(), pmnf_RH_pre_P.flatten(),\n                              pmnf_LH_pre_NP.flatten(), pmnf_RH_pre_NP.flatten(),\n                              pmnf_LH_post_P.flatten(), pmnf_RH_post_P.flatten(),\n                              pmnf_LH_post_NP.flatten(), pmnf_RH_post_NP.flatten()])\n\n# Call the function to append to the existing CSV\nappend_mnf_pmnf_to_csv(mnf_values, pmnf_values)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T06:52:25.698581Z","iopub.execute_input":"2025-01-22T06:52:25.699547Z","iopub.status.idle":"2025-01-22T06:52:25.784719Z","shell.execute_reply.started":"2025-01-22T06:52:25.699504Z","shell.execute_reply":"2025-01-22T06:52:25.78343Z"}},"outputs":[{"name":"stdout","text":"mnf and pmnf values have been appended to '/kaggle/working/stats_output.csv'.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import numpy as np\nfrom scipy.signal import welch\n\n\n\ndef medfreq(pxx, f):\n    \"\"\"\n    Calculate the median frequency from the power spectral density (PSD).\n    \n    Parameters:\n        pxx (numpy.ndarray): Power spectral density (may contain complex values).\n        f (numpy.ndarray): Frequency bins corresponding to the PSD (may contain complex values).\n    \n    Returns:\n        median_freq (float): The median frequency.\n        total_power (float): The total power of the signal.\n    \"\"\"\n    # Convert complex values to magnitude (root(a^2 + b^2)) for pxx and f\n    pxx_magnitude = np.abs(pxx)  # Magnitude of the PSD\n    f_magnitude = np.abs(f)      # Magnitude of the frequency bins\n    \n    # Compute the cumulative power (integral of the PSD)\n    cumulative_power = np.cumsum(pxx_magnitude)\n    total_power = cumulative_power[-1]\n\n    # Find the frequency corresponding to the median power\n    median_freq = f_magnitude[np.searchsorted(cumulative_power, total_power / 2)]\n\n    return median_freq, total_power\n\ndef pmdf(data, fs):\n    \"\"\"\n    Calculate the median frequency (mdf) and power in dB for each 2400-point set in the input data.\n    \n    Parameters:\n        data (numpy.ndarray): Input data with dimensions (num_points, num_channels).\n        fs (float): Sampling frequency.\n    \n    Returns:\n        freq_md (numpy.ndarray): Median frequencies for each set of 2400 points in each row.\n        pow_md (numpy.ndarray): Power in dB for each set of 2400 points in each row.\n    \"\"\"\n    data = np.array(data)\n    # Initialize result arrays\n    num_rows, num_cols = data.shape\n    num_sets = num_cols // 200  # 5 sets of 2400 points\n    freq_md_all = np.zeros((num_rows, num_sets))\n    pow_md_all = np.zeros((num_rows, num_sets))\n\n    # Iterate over each row\n    for row in range(num_rows):\n        # Process each set of 2400 points in the row\n        for set_idx in range(num_sets):\n            start_idx = set_idx * 200\n            end_idx = start_idx + 200\n            subset = data[row, start_idx:end_idx]\n            \n            # Compute the Power Spectral Density (PSD) using Welch's method\n            f, pxx = welch(subset, fs, nperseg=200, noverlap=0, nfft=200)\n            \n            # Calculate the median frequency and total power\n            median_freq, total_power = medfreq(pxx, f)\n            \n            # Store the results\n            freq_md_all[row, set_idx] = median_freq\n            pow_md_all[row, set_idx] = total_power\n    \n    # Convert power to dB\n    pow_md_all = 10 * np.log10(pow_md_all)\n    \n    return freq_md_all, pow_md_all\n\n# Example usage:\nfs = 50  # Sampling frequency (replace with the actual value)\n\nfreq_md_LH_pre_P, pow_md_LH_pre_P = pmdf(normalized_LH_all_pre_P,fs)\nfreq_md_RH_pre_P, pow_md_RH_pre_P = pmdf(normalized_RH_all_pre_P,fs)\nfreq_md_LH_pre_NP, pow_md_LH_pre_NP = pmdf(normalized_LH_all_pre_NP,fs)\nfreq_md_RH_pre_NP, pow_md_RH_pre_NP = pmdf(normalized_RH_all_pre_NP,fs)\nfreq_md_LH_post_P, pow_md_LH_post_P = pmdf(normalized_LH_all_post_P,fs)\nfreq_md_RH_post_P, pow_md_RH_post_P = pmdf(normalized_RH_all_post_P,fs)\nfreq_md_LH_post_NP, pow_md_LH_post_NP = pmdf(normalized_LH_all_post_NP,fs)\nfreq_md_RH_post_NP, pow_md_RH_post_NP = pmdf(normalized_RH_all_post_NP,fs)\n\n\n# freq_md_pre_P, pow_md_pre_P, etc., will now hold the median frequencies and power in dB for each set of 2400 points for each row\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T06:53:11.791126Z","iopub.execute_input":"2025-01-22T06:53:11.792036Z","iopub.status.idle":"2025-01-22T06:53:13.600383Z","shell.execute_reply.started":"2025-01-22T06:53:11.791991Z","shell.execute_reply":"2025-01-22T06:53:13.599273Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import numpy as np\nimport csv\n\n# Path to the CSV file located in Kaggle's output directory\ncsv_file_path = '/kaggle/working/stats_output.csv'\n\n# Function to append the mnf and pmnf values to the existing CSV file\ndef append_mdf_pmdf_to_csv(mdf_values, pmdf_values, filename=csv_file_path):\n    # Read the existing rows of the CSV first\n    with open(filename, 'r', newline='') as read_file:\n        reader = csv.reader(read_file)\n        rows = list(reader)\n\n    # Ensure the number of mdf and pmdf values match the number of rows in the CSV\n    num_rows_in_csv = len(rows) - 1  # excluding the header row\n    total_values_in_flat = len(mdf_values)  # Length of flattened arrays\n\n\n    # Now iterate and append mnf and pmnf values to the respective columns\n    index = 0\n    for i in range(1, len(rows)):  # Skip header row\n        # If the index exceeds the number of available mdf and pmdf values, skip appending\n        if index < len(mdf_values):\n            mdf = mdf_values[index]\n            pmdf = pmdf_values[index]\n            rows[i].append(mdf)  # Append mdf value\n            rows[i].append(pmdf)  # Append pmdf value\n        index += 1\n        if index >= total_values_in_flat:\n            break  # Prevent out of bounds access if index exceeds flattened array size\n\n    # Write the modified rows back into the file\n    with open(filename, 'w', newline='') as write_file:\n        writer = csv.writer(write_file)\n        # Write the header first\n        rows[0].extend([\"mdf\", \"pmdf\"])  # Add the \"mnf\" and \"pmnf\" columns to the header\n        writer.writerows(rows)\n\n    print(f\"mdf and pmdf values have been appended to '{filename}'.\")\n\n# Combine the mnf and pmnf values into single lists (flattened arrays)\nmdf_values = np.concatenate([freq_md_LH_pre_P.flatten(), freq_md_RH_pre_P.flatten(),\n                             freq_md_LH_pre_NP.flatten(), freq_md_RH_pre_NP.flatten(),\n                             freq_md_LH_post_P.flatten(), freq_md_RH_post_P.flatten(),\n                             freq_md_LH_post_NP.flatten(), freq_md_RH_post_NP.flatten()])\n\npmdf_values = np.concatenate([pow_md_LH_pre_P.flatten(), pow_md_RH_pre_P.flatten(),\n                              pow_md_LH_pre_NP.flatten(), pow_md_RH_pre_NP.flatten(),\n                              pow_md_LH_post_P.flatten(), pow_md_RH_post_P.flatten(),\n                              pow_md_LH_post_NP.flatten(), pow_md_RH_post_NP.flatten()])\n\n# Call the function to append to the existing CSV\nappend_mdf_pmdf_to_csv(mdf_values, pmdf_values)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T06:53:23.106324Z","iopub.execute_input":"2025-01-22T06:53:23.107146Z","iopub.status.idle":"2025-01-22T06:53:23.200489Z","shell.execute_reply.started":"2025-01-22T06:53:23.107102Z","shell.execute_reply":"2025-01-22T06:53:23.199395Z"}},"outputs":[{"name":"stdout","text":"mdf and pmdf values have been appended to '/kaggle/working/stats_output.csv'.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import numpy as np\nfrom scipy.signal import welch\n\ndef sm1(data, fs):\n    \"\"\"\n    Calculate the first moment (Mom1) of the power spectral density (PSD) for each subset of 2400 points.\n    \n    Parameters:\n        data (numpy.ndarray): Input data with dimensions (num_points, num_channels).\n        fs (float): Sampling frequency.\n    \n    Returns:\n        Mom1_all (numpy.ndarray): First moments for each subset and each row.\n    \"\"\"\n    data = np.array(data)\n    num_points, num_channels = data.shape\n    num_subsets = num_channels // 200  # Each subset has 2400 points\n    \n    # Initialize result array\n    Mom1_all = np.zeros((num_points, num_subsets))\n    \n    # Process each row (each row corresponds to one signal across all channels)\n    for row_idx in range(num_points):\n        for subset_idx in range(num_subsets):\n            start_idx = subset_idx * 200\n            end_idx = start_idx + 200\n            subset = data[row_idx, start_idx:end_idx]\n            \n            # Calculate PSD using Welch's method\n            f, pxx = welch(subset, fs, nperseg=200, noverlap=0, nfft=200)\n            pxx_magnitude = np.abs(pxx)  # Magnitude of the PSD\n            pxx_magnitude = np.asarray(pxx_magnitude)\n            f_magnitude = np.abs(f) \n\n            \n            log_pxx = np.array([np.log(val) / np.log(10) for val in pxx_magnitude])\n            # Calculate the first moment of the power spectral density\n            Mom1_all[row_idx, subset_idx] = np.sum(10*log_pxx * f)\n    \n    return Mom1_all\n\n# Example usage:\nfs = 50  # Example sampling frequency\n\nsm1_LH_pre_P= sm1(normalized_LH_all_pre_P,fs)\nsm1_RH_pre_P= sm1(normalized_RH_all_pre_P,fs)\nsm1_LH_pre_NP = sm1(normalized_LH_all_pre_NP,fs)\nsm1_RH_pre_NP= sm1(normalized_RH_all_pre_NP,fs)\nsm1_LH_post_P= sm1(normalized_LH_all_post_P,fs)\nsm1_RH_post_P= sm1(normalized_RH_all_post_P,fs)\nsm1_LH_post_NP= sm1(normalized_LH_all_post_NP,fs)\nsm1_RH_post_NP= sm1(normalized_RH_all_post_NP,fs)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T06:53:41.580848Z","iopub.execute_input":"2025-01-22T06:53:41.58124Z","iopub.status.idle":"2025-01-22T06:53:45.122931Z","shell.execute_reply.started":"2025-01-22T06:53:41.581206Z","shell.execute_reply":"2025-01-22T06:53:45.121703Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import numpy as np\nimport csv\n\n# Path to the CSV file located in Kaggle's output directory\ncsv_file_path = '/kaggle/working/stats_output.csv'\n\n# Function to append the mnf and pmnf values to the existing CSV file\ndef append_sm1_to_csv(sm1_values, filename=csv_file_path):\n    # Read the existing rows of the CSV first\n    with open(filename, 'r', newline='') as read_file:\n        reader = csv.reader(read_file)\n        rows = list(reader)\n\n    # Ensure the number of mdf and pmdf values match the number of rows in the CSV\n    num_rows_in_csv = len(rows) - 1  # excluding the header row\n    total_values_in_flat = len(sm1_values)  # Length of flattened arrays\n\n\n    # Now iterate and append mnf and pmnf values to the respective columns\n    index = 0\n    for i in range(1, len(rows)):  # Skip header row\n        # If the index exceeds the number of available mdf and pmdf values, skip appending\n        if index < len(sm1_values):\n            sm1 = sm1_values[index]\n            rows[i].append(sm1)  # Append mdf value\n        index += 1\n        if index >= total_values_in_flat:\n            break  # Prevent out of bounds access if index exceeds flattened array size\n\n    # Write the modified rows back into the file\n    with open(filename, 'w', newline='') as write_file:\n        writer = csv.writer(write_file)\n        # Write the header first\n        rows[0].extend([\"sm1\"])  # Add the \"mnf\" and \"pmnf\" columns to the header\n        writer.writerows(rows)\n\n\n\n# Combine the mnf and pmnf values into single lists (flattened arrays)\nsm1_values = np.concatenate([sm1_LH_pre_P.flatten(), sm1_RH_pre_P.flatten(),\n                             sm1_LH_pre_NP.flatten(), sm1_RH_pre_NP.flatten(),\n                             sm1_LH_post_P.flatten(), sm1_RH_post_P.flatten(),\n                             sm1_LH_post_NP.flatten(), sm1_RH_post_NP.flatten()])\n\n\n# Call the function to append to the existing CSV\nappend_sm1_to_csv(sm1_values)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T06:53:52.953899Z","iopub.execute_input":"2025-01-22T06:53:52.954986Z","iopub.status.idle":"2025-01-22T06:53:53.043169Z","shell.execute_reply.started":"2025-01-22T06:53:52.954936Z","shell.execute_reply":"2025-01-22T06:53:53.042069Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import numpy as np\nfrom scipy.signal import welch\n\ndef sm2(data, fs):\n    \"\"\"\n    Calculate the first moment (Mom1) of the power spectral density (PSD) for each subset of 2400 points.\n    \n    Parameters:\n        data (numpy.ndarray): Input data with dimensions (num_points, num_channels).\n        fs (float): Sampling frequency.\n    \n    Returns:\n        Mom1_all (numpy.ndarray): First moments for each subset and each row.\n    \"\"\"\n    data = np.array(data)\n    num_points, num_channels = data.shape\n    num_subsets = num_channels // 200  # Each subset has 2400 points\n    \n    # Initialize result array\n    Mom2_all = np.zeros((num_points, num_subsets))\n    \n    # Process each row (each row corresponds to one signal across all channels)\n    for row_idx in range(num_points):\n        for subset_idx in range(num_subsets):\n            start_idx = subset_idx * 200\n            end_idx = start_idx + 200\n            subset = data[row_idx, start_idx:end_idx]\n            \n            # Calculate PSD using Welch's method\n            f, pxx = welch(subset, fs, nperseg=200, noverlap=0, nfft=200)\n            pxx_magnitude = np.abs(pxx)  # Magnitude of the PSD\n            pxx_magnitude = np.asarray(pxx_magnitude)\n            f_magnitude = np.abs(f) \n\n            \n            log_pxx = np.array([np.log(val) / np.log(10) for val in pxx_magnitude])\n            # Calculate the first moment of the power spectral density\n            Mom2_all[row_idx, subset_idx] = np.sum(10*log_pxx * (f**2))\n    \n    return Mom2_all\n\n# Example usage:\nfs = 50  # Example sampling frequency\nsm2_LH_pre_P= sm2(normalized_LH_all_pre_P,fs)\nsm2_RH_pre_P= sm2(normalized_RH_all_pre_P,fs)\nsm2_LH_pre_NP = sm2(normalized_LH_all_pre_NP,fs)\nsm2_RH_pre_NP= sm2(normalized_RH_all_pre_NP,fs)\nsm2_LH_post_P= sm2(normalized_LH_all_post_P,fs)\nsm2_RH_post_P= sm2(normalized_RH_all_post_P,fs)\nsm2_LH_post_NP= sm2(normalized_LH_all_post_NP,fs)\nsm2_RH_post_NP= sm2(normalized_RH_all_post_NP,fs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T06:54:16.224491Z","iopub.execute_input":"2025-01-22T06:54:16.224962Z","iopub.status.idle":"2025-01-22T06:54:19.851837Z","shell.execute_reply.started":"2025-01-22T06:54:16.224921Z","shell.execute_reply":"2025-01-22T06:54:19.850702Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import numpy as np\nimport csv\n\n# Path to the CSV file located in Kaggle's output directory\ncsv_file_path = '/kaggle/working/stats_output.csv'\n\n# Function to append the mnf and pmnf values to the existing CSV file\ndef append_sm2_to_csv(sm1_values, filename=csv_file_path):\n    # Read the existing rows of the CSV first\n    with open(filename, 'r', newline='') as read_file:\n        reader = csv.reader(read_file)\n        rows = list(reader)\n\n    # Ensure the number of mdf and pmdf values match the number of rows in the CSV\n    num_rows_in_csv = len(rows) - 1  # excluding the header row\n    total_values_in_flat = len(sm2_values)  # Length of flattened arrays\n\n\n    # Now iterate and append mnf and pmnf values to the respective columns\n    index = 0\n    for i in range(1, len(rows)):  # Skip header row\n        # If the index exceeds the number of available mdf and pmdf values, skip appending\n        if index < len(sm2_values):\n            sm2 = sm2_values[index]\n            rows[i].append(sm2)  # Append mdf value\n        index += 1\n        if index >= total_values_in_flat:\n            break  # Prevent out of bounds access if index exceeds flattened array size\n\n    # Write the modified rows back into the file\n    with open(filename, 'w', newline='') as write_file:\n        writer = csv.writer(write_file)\n        # Write the header first\n        rows[0].extend([\"sm2\"])  # Add the \"mnf\" and \"pmnf\" columns to the header\n        writer.writerows(rows)\n\n\n\n# Combine the mnf and pmnf values into single lists (flattened arrays)\nsm2_values = np.concatenate([sm2_LH_pre_P.flatten(), sm2_RH_pre_P.flatten(),\n                             sm2_LH_pre_NP.flatten(), sm2_RH_pre_NP.flatten(),\n                             sm2_LH_post_P.flatten(), sm2_RH_post_P.flatten(),\n                             sm2_LH_post_NP.flatten(), sm2_RH_post_NP.flatten()])\n\n\n# Call the function to append to the existing CSV\nappend_sm2_to_csv(sm2_values)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T06:55:03.274167Z","iopub.execute_input":"2025-01-22T06:55:03.274587Z","iopub.status.idle":"2025-01-22T06:55:03.378278Z","shell.execute_reply.started":"2025-01-22T06:55:03.274549Z","shell.execute_reply":"2025-01-22T06:55:03.377115Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"import numpy as np\nfrom scipy.signal import welch\n\ndef sm3(data, fs):\n    \"\"\"\n    Calculate the first moment (Mom1) of the power spectral density (PSD) for each subset of 2400 points.\n    \n    Parameters:\n        data (numpy.ndarray): Input data with dimensions (num_points, num_channels).\n        fs (float): Sampling frequency.\n    \n    Returns:\n        Mom1_all (numpy.ndarray): First moments for each subset and each row.\n    \"\"\"\n    data = np.array(data)\n    num_points, num_channels = data.shape\n    num_subsets = num_channels // 200  # Each subset has 2400 points\n    \n    # Initialize result array\n    Mom3_all = np.zeros((num_points, num_subsets))\n    \n    # Process each row (each row corresponds to one signal across all channels)\n    for row_idx in range(num_points):\n        for subset_idx in range(num_subsets):\n            start_idx = subset_idx * 200\n            end_idx = start_idx + 200\n            subset = data[row_idx, start_idx:end_idx]\n            \n            # Calculate PSD using Welch's method\n            f, pxx = welch(subset, fs, nperseg=200, noverlap=0, nfft=200)\n            pxx_magnitude = np.abs(pxx)  # Magnitude of the PSD\n            pxx_magnitude = np.asarray(pxx_magnitude)\n            f_magnitude = np.abs(f) \n\n            \n            log_pxx = np.array([np.log(val) / np.log(10) for val in pxx_magnitude])\n            # Calculate the first moment of the power spectral density\n            Mom3_all[row_idx, subset_idx] = np.sum(10*log_pxx * (f**3))\n    \n    return Mom3_all\n\n# Example usage:\nfs = 50  # Example sampling frequency\nsm3_LH_pre_P= sm3(normalized_LH_all_pre_P,fs)\nsm3_RH_pre_P= sm3(normalized_RH_all_pre_P,fs)\nsm3_LH_pre_NP = sm3(normalized_LH_all_pre_NP,fs)\nsm3_RH_pre_NP= sm3(normalized_RH_all_pre_NP,fs)\nsm3_LH_post_P= sm3(normalized_LH_all_post_P,fs)\nsm3_RH_post_P= sm3(normalized_RH_all_post_P,fs)\nsm3_LH_post_NP= sm3(normalized_LH_all_post_NP,fs)\nsm3_RH_post_NP= sm3(normalized_RH_all_post_NP,fs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T06:55:56.307409Z","iopub.execute_input":"2025-01-22T06:55:56.308012Z","iopub.status.idle":"2025-01-22T06:56:00.353369Z","shell.execute_reply.started":"2025-01-22T06:55:56.307944Z","shell.execute_reply":"2025-01-22T06:56:00.352206Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import numpy as np\nimport csv\n\n# Path to the CSV file located in Kaggle's output directory\ncsv_file_path = '/kaggle/working/stats_output.csv'\n\n# Function to append the mnf and pmnf values to the existing CSV file\ndef append_sm3_to_csv(sm1_values, filename=csv_file_path):\n    # Read the existing rows of the CSV first\n    with open(filename, 'r', newline='') as read_file:\n        reader = csv.reader(read_file)\n        rows = list(reader)\n\n    # Ensure the number of mdf and pmdf values match the number of rows in the CSV\n    num_rows_in_csv = len(rows) - 1  # excluding the header row\n    total_values_in_flat = len(sm3_values)  # Length of flattened arrays\n\n\n    # Now iterate and append mnf and pmnf values to the respective columns\n    index = 0\n    for i in range(1, len(rows)):  # Skip header row\n        # If the index exceeds the number of available mdf and pmdf values, skip appending\n        if index < len(sm3_values):\n            sm3 = sm3_values[index]\n            rows[i].append(sm3)  # Append mdf value\n        index += 1\n        if index >= total_values_in_flat:\n            break  # Prevent out of bounds access if index exceeds flattened array size\n\n    # Write the modified rows back into the file\n    with open(filename, 'w', newline='') as write_file:\n        writer = csv.writer(write_file)\n        # Write the header first\n        rows[0].extend([\"sm3\"])  # Add the \"mnf\" and \"pmnf\" columns to the header\n        writer.writerows(rows)\n\n\n\n# Combine the mnf and pmnf values into single lists (flattened arrays)\nsm3_values = np.concatenate([sm3_LH_pre_P.flatten(), sm3_RH_pre_P.flatten(),\n                             sm3_LH_pre_NP.flatten(), sm3_RH_pre_NP.flatten(),\n                             sm3_LH_post_P.flatten(), sm3_RH_post_P.flatten(),\n                             sm3_LH_post_NP.flatten(), sm3_RH_post_NP.flatten()])\n\n\n# Call the function to append to the existing CSV\nappend_sm3_to_csv(sm3_values)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T06:56:06.470484Z","iopub.execute_input":"2025-01-22T06:56:06.471062Z","iopub.status.idle":"2025-01-22T06:56:06.578833Z","shell.execute_reply.started":"2025-01-22T06:56:06.471015Z","shell.execute_reply":"2025-01-22T06:56:06.577399Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import numpy as np\nfrom scipy.stats import kurtosis\n\ndef normkurt(data, set_size):\n    \"\"\"\n    Calculate the normalized kurtosis for each subset of the data.\n    \n    Parameters:\n        data (numpy.ndarray): Input data with dimensions (28, 12000).\n        set_size (int): Size of each subset (2400 points).\n    \n    Returns:\n        numpy.ndarray: Normalized kurtosis for each subset of each row.\n        \n    \"\"\"\n    data = np.array(data)\n    num_rows, num_cols = data.shape\n    count = num_cols // set_size  # Number of subsets per row\n    \n    # Initialize the result array\n    x = np.zeros_like(data)\n    \n    # Loop through each row\n    for row_idx in range(num_rows):\n        for i in range(count):\n            # Define the current subset\n            subset = data[row_idx, i * set_size : (i + 1) * set_size]\n            \n            # Define the last subset (which is the i-th subset in the row)\n            last_subset = data[row_idx, (i + 1) * set_size - set_size : (i + 1) * set_size]\n            \n            # Calculate the kurtosis for the current subset and the last subset\n            kurt_current = kurtosis(subset)\n            kurt_last = kurtosis(last_subset)\n            \n            # Normalize the kurtosis\n            norm_kurt = kurt_current / kurt_last\n            \n            # Store the result in the output array\n            x[row_idx, i * set_size - 11 : (i + 1) * set_size] = norm_kurt\n    \n    return x\nfs=200\nnormkurt_LH_pre_P= normkurt(flattened_LH_all_pre_P,fs)\nnormkurt_RH_pre_P= normkurt(flattened_RH_all_pre_P,fs)\nnormkurt_LH_pre_NP = normkurt(flattened_LH_all_pre_NP,fs)\nnormkurt_RH_pre_NP= normkurt(flattened_RH_all_pre_NP,fs)\nnormkurt_LH_post_P= normkurt(flattened_LH_all_post_P,fs)\nnormkurt_RH_post_P= normkurt(flattened_RH_all_post_P,fs)\nnormkurt_LH_post_NP= normkurt(flattened_LH_all_post_NP,fs)\nnormkurt_RH_post_NP= normkurt(flattened_RH_all_post_NP,fs)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T07:11:00.50057Z","iopub.execute_input":"2025-01-22T07:11:00.501079Z","iopub.status.idle":"2025-01-22T07:11:01.192943Z","shell.execute_reply.started":"2025-01-22T07:11:00.501036Z","shell.execute_reply":"2025-01-22T07:11:01.191319Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[35], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     43\u001b[0m fs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m\n\u001b[0;32m---> 44\u001b[0m normkurt_LH_pre_P\u001b[38;5;241m=\u001b[39m \u001b[43mnormkurt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened_LH_all_pre_P\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m normkurt_RH_pre_P\u001b[38;5;241m=\u001b[39m normkurt(flattened_RH_all_pre_P,fs)\n\u001b[1;32m     46\u001b[0m normkurt_LH_pre_NP \u001b[38;5;241m=\u001b[39m normkurt(flattened_LH_all_pre_NP,fs)\n","Cell \u001b[0;32mIn[35], line 33\u001b[0m, in \u001b[0;36mnormkurt\u001b[0;34m(data, set_size)\u001b[0m\n\u001b[1;32m     30\u001b[0m last_subset \u001b[38;5;241m=\u001b[39m data[row_idx, (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m set_size \u001b[38;5;241m-\u001b[39m set_size : (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m set_size]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Calculate the kurtosis for the current subset and the last subset\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m kurt_current \u001b[38;5;241m=\u001b[39m \u001b[43mkurtosis\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m kurt_last \u001b[38;5;241m=\u001b[39m kurtosis(last_subset)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Normalize the kurtosis\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/stats/_axis_nan_policy.py:573\u001b[0m, in \u001b[0;36m_axis_nan_policy_factory.<locals>.axis_nan_policy_decorator.<locals>.axis_nan_policy_wrapper\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    570\u001b[0m     res \u001b[38;5;241m=\u001b[39m _add_reduced_axes(res, reduced_axes, keepdims)\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tuple_to_result(\u001b[38;5;241m*\u001b[39mres)\n\u001b[0;32m--> 573\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mhypotest_fun_out\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m res \u001b[38;5;241m=\u001b[39m result_to_tuple(res)\n\u001b[1;32m    575\u001b[0m res \u001b[38;5;241m=\u001b[39m _add_reduced_axes(res, reduced_axes, keepdims)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/stats/_stats_py.py:1312\u001b[0m, in \u001b[0;36mkurtosis\u001b[0;34m(a, axis, fisher, bias, nan_policy)\u001b[0m\n\u001b[1;32m   1310\u001b[0m mean \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmean(a, axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1311\u001b[0m mean_reduced \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39msqueeze(mean, axis\u001b[38;5;241m=\u001b[39maxis)  \u001b[38;5;66;03m# needed later\u001b[39;00m\n\u001b[0;32m-> 1312\u001b[0m m2 \u001b[38;5;241m=\u001b[39m \u001b[43m_moment\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1313\u001b[0m m4 \u001b[38;5;241m=\u001b[39m _moment(a, \u001b[38;5;241m4\u001b[39m, axis, mean\u001b[38;5;241m=\u001b[39mmean, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/stats/_stats_py.py:1081\u001b[0m, in \u001b[0;36m_moment\u001b[0;34m(a, order, axis, mean, xp)\u001b[0m\n\u001b[1;32m   1078\u001b[0m mean \u001b[38;5;241m=\u001b[39m mean[()] \u001b[38;5;28;01mif\u001b[39;00m mean\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m mean\n\u001b[1;32m   1079\u001b[0m a_zero_mean \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m-\u001b[39m mean\n\u001b[0;32m-> 1081\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39meps \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(divide\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1084\u001b[0m     rel_diff \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmax(xp\u001b[38;5;241m.\u001b[39mabs(a_zero_mean), axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   1085\u001b[0m                       keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m/\u001b[39m xp\u001b[38;5;241m.\u001b[39mabs(mean)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/core/getlimits.py:516\u001b[0m, in \u001b[0;36mfinfo.__new__\u001b[0;34m(cls, dtype)\u001b[0m\n\u001b[1;32m    514\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m newdtype\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(dtype, numeric\u001b[38;5;241m.\u001b[39minexact):\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata type \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m not inexact\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (dtype))\n\u001b[1;32m    517\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_finfo_cache\u001b[38;5;241m.\u001b[39mget(dtype)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mValueError\u001b[0m: data type <class 'numpy.object_'> not inexact"],"ename":"ValueError","evalue":"data type <class 'numpy.object_'> not inexact","output_type":"error"}],"execution_count":35},{"cell_type":"code","source":"import numpy as np\nfrom scipy.stats import kurtosis\n\ndef preprocess_data(data):\n    \"\"\"\n    Convert data elements to numeric values. Handles scientific notation (E) and replaces invalid entries with NaN.\n    \n    Parameters:\n        data (list or numpy.ndarray): Input data to preprocess.\n        \n    Returns:\n        numpy.ndarray: Preprocessed data as a numeric array.\n    \"\"\"\n    def convert_value(val):\n        try:\n            return float(val)  # Convert to float, handles scientific notation like '1.2E3'\n        except ValueError:\n            return np.nan  # Replace invalid entries with NaN\n    \n    # Vectorized conversion of the data\n    vectorized_converter = np.vectorize(convert_value)\n    numeric_data = vectorized_converter(data)\n    return numeric_data\n\ndef normkurt(data, set_size):\n    \"\"\"\n    Calculate the normalized kurtosis for each subset of the data.\n    \n    Parameters:\n        data (list or numpy.ndarray): Input data with dimensions (28, 12000).\n        set_size (int): Size of each subset (e.g., 2400 points).\n    \n    Returns:\n        numpy.ndarray: Normalized kurtosis for each subset of each row.\n    \"\"\"\n    # Preprocess the data to ensure numeric dtype\n    data = preprocess_data(data)\n    \n    # Check for invalid (NaN) values\n    if np.isnan(data).any():\n        raise ValueError(\"Data contains invalid (non-numeric) values after preprocessing.\")\n    \n    num_rows, num_cols = data.shape\n    count = num_cols // set_size  # Number of subsets per row\n    \n    # Initialize the result array\n    x = np.zeros((num_rows, count))\n    \n    # Loop through\n # Loop through each row\n    for row_idx in range(num_rows):\n        last_subset = data[row_idx, -set_size:]  # The last subset of the row\n        getcontext().prec = 18\n        for i in range(count):\n            # Define the current subset\n            subset = data[row_idx, i * set_size : (i + 1) * set_size]\n            \n            # Define the last subset (which is the i-th subset in the row)\n            # last_subset = data[row_idx, (i + 1) * set_size - set_size : (i + 1) * set_size]\n            \n            # Calculate the kurtosis for the current subset and the last subset\n            kurt_current = kurtosis(subset)\n            kurt_last = kurtosis(last_subset)\n            # print(kurt_current, kurt_last)\n            \n            # Normalize the kurtosis\n            norm_kurt = kurt_current / kurt_last\n            \n            # Store the result in the output array\n            # x[row_idx, i * set_size - 11 : (i + 1) * set_size] = norm_kurt\n            start_idx = i * set_size\n            end_idx = (i + 1) * set_size\n            x[row_idx, start_idx:end_idx] = norm_kurt\n    \n    return x\nfs=200\nnormkurt_LH_pre_P= normkurt(flattened_LH_all_pre_P,fs)\nnormkurt_RH_pre_P= normkurt(flattened_RH_all_pre_P,fs)\nnormkurt_LH_pre_NP = normkurt(flattened_LH_all_pre_NP,fs)\nnormkurt_RH_pre_NP= normkurt(flattened_RH_all_pre_NP,fs)\nnormkurt_LH_post_P= normkurt(flattened_LH_all_post_P,fs)\nnormkurt_RH_post_P= normkurt(flattened_RH_all_post_P,fs)\nnormkurt_LH_post_NP= normkurt(flattened_LH_all_post_NP,fs)\nnormkurt_RH_post_NP= normkurt(flattened_RH_all_post_NP,fs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T07:43:20.993706Z","iopub.execute_input":"2025-01-22T07:43:20.994518Z","iopub.status.idle":"2025-01-22T07:43:29.324568Z","shell.execute_reply.started":"2025-01-22T07:43:20.994473Z","shell.execute_reply":"2025-01-22T07:43:29.323482Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"import numpy as np\nimport csv\n\n# Path to the CSV file located in Kaggle's output directory\ncsv_file_path = '/kaggle/working/stats_output.csv'\n\n# Function to append the mnf and pmnf values to the existing CSV file\ndef append_normkurt_to_csv(normkurt_values, filename=csv_file_path):\n    # Read the existing rows of the CSV first\n    with open(filename, 'r', newline='') as read_file:\n        reader = csv.reader(read_file)\n        rows = list(reader)\n\n    # Ensure the number of mdf and pmdf values match the number of rows in the CSV\n    num_rows_in_csv = len(rows) - 1  # excluding the header row\n    total_values_in_flat = len(normkurt_values)  # Length of flattened arrays\n\n\n    # Now iterate and append mnf and pmnf values to the respective columns\n    index = 0\n    for i in range(1, len(rows)):  # Skip header row\n        # If the index exceeds the number of available mdf and pmdf values, skip appending\n        if index < len(normkurt_values):\n            normkurt = normkurt_values[index]\n            rows[i].append(normkurt)  # Append mdf value\n        index += 1\n        if index >= total_values_in_flat:\n            break  # Prevent out of bounds access if index exceeds flattened array size\n\n    # Write the modified rows back into the file\n    with open(filename, 'w', newline='') as write_file:\n        writer = csv.writer(write_file)\n        # Write the header first\n        rows[0].extend([\"normkurt\"])  # Add the \"mnf\" and \"pmnf\" columns to the header\n        writer.writerows(rows)\n\n\n\n# Combine the mnf and pmnf values into single lists (flattened arrays)\nnormkurt_values = np.concatenate([normkurt_LH_pre_P.flatten(), normkurt_RH_pre_P.flatten(),\n                             normkurt_LH_pre_NP.flatten(), normkurt_RH_pre_NP.flatten(),\n                             normkurt_LH_post_P.flatten(), normkurt_RH_post_P.flatten(),\n                             normkurt_LH_post_NP.flatten(), normkurt_RH_post_NP.flatten()])\n\n\n# Call the function to append to the existing CSV\nappend_normkurt_to_csv(normkurt_values)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T07:43:29.461682Z","iopub.execute_input":"2025-01-22T07:43:29.462604Z","iopub.status.idle":"2025-01-22T07:43:29.580387Z","shell.execute_reply.started":"2025-01-22T07:43:29.462556Z","shell.execute_reply":"2025-01-22T07:43:29.579303Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"import numpy as np\n\ndef normvar(data, set_size):\n    \"\"\"\n    Calculate the normalized variance for each subset of the data.\n    \n    Parameters:\n        data (numpy.ndarray): Input data with dimensions (28, 12000).\n        set_size (int): Size of each subset (2400 points).\n    \n    Returns:\n        numpy.ndarray: Normalized variance for each subset of each row.\n    \"\"\"\n    # Ensure the data is in numeric format\n    data = np.asarray(data, dtype=np.float64)\n    \n    num_rows, num_cols = data.shape\n    count = num_cols // set_size  # Number of subsets per row\n    \n    # Initialize the result array\n    x = np.zeros_like(data)\n    \n    # Loop through each row\n    for row_idx in range(num_rows):\n        last_subset = data[row_idx, -set_size:]  # The last subset of the row\n        getcontext().prec = 18\n        for i in range(count):\n            # Define the current subset\n            subset = data[row_idx, i * set_size : (i + 1) * set_size]\n            \n            # Calculate the variance for the current subset and the last subset\n            var_current = np.var(subset)\n            var_last = np.var(last_subset)\n            \n            # Normalize the variance\n            norm_var = var_current / var_last\n            \n            \n            # Store the result in the output array\n            start_idx = i * set_size\n            end_idx = (i + 1) * set_size\n            x[row_idx, start_idx:end_idx] = norm_var\n            # x[row_idx, i * set_size - 11 : (i + 1) * set_size] = norm_var\n    \n    return x\n\n# Example usage:\n# Assuming combined_LH_RH_pre_P is already defined\nfs = 200\nnormvar_LH_pre_P= normvar(flattened_LH_all_pre_P,fs)\nnormvar_RH_pre_P= normvar(flattened_RH_all_pre_P,fs)\nnormvar_LH_pre_NP = normvar(flattened_LH_all_pre_NP,fs)\nnormvar_RH_pre_NP= normvar(flattened_RH_all_pre_NP,fs)\nnormvar_LH_post_P= normvar(flattened_LH_all_post_P,fs)\nnormvar_RH_post_P= normvar(flattened_RH_all_post_P,fs)\nnormvar_LH_post_NP= normvar(flattened_LH_all_post_NP,fs)\nnormvar_RH_post_NP= normvar(flattened_RH_all_post_NP,fs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T07:45:12.038941Z","iopub.execute_input":"2025-01-22T07:45:12.039979Z","iopub.status.idle":"2025-01-22T07:45:12.358763Z","shell.execute_reply.started":"2025-01-22T07:45:12.039917Z","shell.execute_reply":"2025-01-22T07:45:12.357714Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"import numpy as np\nimport csv\n\n# Path to the CSV file located in Kaggle's output directory\ncsv_file_path = '/kaggle/working/stats_output.csv'\n\n# Function to append the mnf and pmnf values to the existing CSV file\ndef append_normvar_to_csv(normvar_values, filename=csv_file_path):\n    # Read the existing rows of the CSV first\n    with open(filename, 'r', newline='') as read_file:\n        reader = csv.reader(read_file)\n        rows = list(reader)\n\n    # Ensure the number of mdf and pmdf values match the number of rows in the CSV\n    num_rows_in_csv = len(rows) - 1  # excluding the header row\n    total_values_in_flat = len(normvar_values)  # Length of flattened arrays\n\n\n    # Now iterate and append mnf and pmnf values to the respective columns\n    index = 0\n    for i in range(1, len(rows)):  # Skip header row\n        # If the index exceeds the number of available mdf and pmdf values, skip appending\n        if index < len(normvar_values):\n            normvar = normvar_values[index]\n            rows[i].append(normvar)  # Append mdf value\n        index += 1\n        if index >= total_values_in_flat:\n            break  # Prevent out of bounds access if index exceeds flattened array size\n\n    # Write the modified rows back into the file\n    with open(filename, 'w', newline='') as write_file:\n        writer = csv.writer(write_file)\n        # Write the header first\n        rows[0].extend([\"normvar\"])  # Add the \"mnf\" and \"pmnf\" columns to the header\n        writer.writerows(rows)\n\n\n\n# Combine the mnf and pmnf values into single lists (flattened arrays)\nnormvar_values = np.concatenate([normvar_LH_pre_P.flatten(), normvar_RH_pre_P.flatten(),\n                             normvar_LH_pre_NP.flatten(), normvar_RH_pre_NP.flatten(),\n                             normvar_LH_post_P.flatten(), normvar_RH_post_P.flatten(),\n                             normvar_LH_post_NP.flatten(), normvar_RH_post_NP.flatten()])\n\n\n# Call the function to append to the existing CSV\nappend_normvar_to_csv(normvar_values)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T07:45:20.467804Z","iopub.execute_input":"2025-01-22T07:45:20.468859Z","iopub.status.idle":"2025-01-22T07:45:20.594954Z","shell.execute_reply.started":"2025-01-22T07:45:20.468787Z","shell.execute_reply":"2025-01-22T07:45:20.593189Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.stats import entropy\n\ndef entr(data, fs):\n    \"\"\"\n    Calculate the Shannon entropy for each subset of the data.\n    \n    Parameters:\n        data (numpy.ndarray): Input data with dimensions (28, 12000).\n        fs (int): Sampling frequency.\n    \n    Returns:\n        numpy.ndarray: Entropy values for each subset of each row.\n    \"\"\"\n    ts = 1 / fs  # Time step based on the sampling frequency\n    set_size = 200  # Number of points per subset\n    data = np.array(data)\n    num_rows, num_cols = data.shape\n    count = num_cols // set_size  # Number of sets per row\n    \n    # Initialize the result array for storing entropy values\n    ent_values = np.zeros(num_rows)\n    \n    # Loop through each row\n    for row_idx in range(num_rows):\n        row_entropy = []\n        for i in range(count):\n            # Extract the current subset\n            subset = data[row_idx, i * set_size : (i + 1) * set_size]\n            \n            # Create the time vector for this subset (it should match the length of the subset)\n            t = np.arange(0, len(subset)) * ts  # Create time vector with the same length as the subset\n            tdur = pd.to_timedelta(t, unit='s')  # Convert time to pandas Timedelta for correct format\n            \n            # Create a pandas DataFrame (equivalent to a timetable) with time as the index\n            xt = pd.DataFrame(subset, index=tdur, columns=['data'])\n            \n            # Calculate the histogram and normalize it\n            hist, bin_edges = np.histogram(xt['data'], bins=100, density=True)\n            \n            # Calculate the Shannon entropy\n            shannon_entropy = entropy(hist)\n            \n            # Store the entropy for this subset\n            row_entropy.append(shannon_entropy)\n        \n        # Average entropy for the row\n        ent_values[row_idx] = np.mean(row_entropy)\n    \n    return ent_values\n\nfs = 200\nentr_LH_pre_P= entr(normalized_LH_all_pre_P,fs)\nentr_RH_pre_P= entr(normalized_RH_all_pre_P,fs)\nentr_LH_pre_NP = entr(normalized_LH_all_pre_NP,fs)\nentr_RH_pre_NP= entr(normalized_RH_all_pre_NP,fs)\nentr_LH_post_P= entr(normalized_LH_all_post_P,fs)\nentr_RH_post_P= entr(normalized_RH_all_post_P,fs)\nentr_LH_post_NP= entr(normalized_LH_all_post_NP,fs)\nentr_RH_post_NP= entr(normalized_RH_all_post_NP,fs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T06:57:37.571461Z","iopub.execute_input":"2025-01-22T06:57:37.572449Z","iopub.status.idle":"2025-01-22T06:57:45.587513Z","shell.execute_reply.started":"2025-01-22T06:57:37.572401Z","shell.execute_reply":"2025-01-22T06:57:45.58633Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.stats import entropy\n\ndef entr(data, fs):\n    \"\"\"\n    Calculate the Shannon entropy for each subset of the data.\n    \n    Parameters:\n        data (numpy.ndarray): Input data with dimensions (num_rows, num_cols).\n        fs (int): Sampling frequency.\n    \n    Returns:\n        numpy.ndarray: Entropy values for each subset of each row.\n    \"\"\"\n    ts = 1 / fs  # Time step based on the sampling frequency\n    set_size = 200  # Number of points per subset\n    data = np.array(data)\n    num_rows, num_cols = data.shape\n    count = num_cols // set_size  # Number of subsets per row\n    \n    # Initialize the result array for storing entropy values\n    ent_values = np.zeros(num_rows)\n    \n    # Loop through each row\n    for row_idx in range(num_rows):\n        row_entropy = []\n        for i in range(count):\n            # Extract the current subset\n            subset = data[row_idx, i * set_size : (i + 1) * set_size]\n            \n            # Calculate the histogram and normalize it\n            hist, _ = np.histogram(subset, bins=100, density=True)\n            \n            # Calculate the Shannon entropy\n            shannon_entropy = entropy(hist)\n            \n            # Store the entropy for this subset\n            row_entropy.append(shannon_entropy)\n        \n        # Average entropy for the row\n        ent_values[row_idx] = np.mean(row_entropy)\n    \n    return ent_values\n\n# Example usage\nfs = 200\n\n\n\n# Calculate entropy values\nentr_LH_pre_P = entr(normalized_LH_all_pre_P, fs)\nentr_RH_pre_P = entr(normalized_RH_all_pre_P, fs)\nentr_LH_pre_NP = entr(normalized_LH_all_pre_NP, fs)\nentr_RH_pre_NP = entr(normalized_RH_all_pre_NP, fs)\nentr_LH_post_P = entr(normalized_LH_all_post_P, fs)\nentr_RH_post_P = entr(normalized_RH_all_post_P, fs)\nentr_LH_post_NP = entr(normalized_LH_all_post_NP, fs)\nentr_RH_post_NP = entr(normalized_RH_all_post_NP, fs)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T07:18:42.497906Z","iopub.execute_input":"2025-01-22T07:18:42.498336Z","iopub.status.idle":"2025-01-22T07:18:46.692463Z","shell.execute_reply.started":"2025-01-22T07:18:42.498297Z","shell.execute_reply":"2025-01-22T07:18:46.691322Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"print(\"Entropy LH Pre P:\", entr_LH_pre_P)\nprint(\"Entropy RH Pre P:\", entr_RH_pre_P)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T07:21:11.503667Z","iopub.execute_input":"2025-01-22T07:21:11.504096Z","iopub.status.idle":"2025-01-22T07:21:11.511115Z","shell.execute_reply.started":"2025-01-22T07:21:11.504059Z","shell.execute_reply":"2025-01-22T07:21:11.509778Z"}},"outputs":[{"name":"stdout","text":"Entropy LH Pre P: [3.97604702 3.92104952 3.91014268 3.96676505 3.93636332 3.96117665\n 3.88751659 3.91378397 3.83419637 3.88362877 3.96939166 3.94440295\n 3.94180561 3.90092621]\nEntropy RH Pre P: [4.01098015 3.9520205  3.93973231 3.89935708 4.02837797 3.96035768\n 3.96821151 3.91053019 3.88023804 3.94794099 3.87181985 3.96040678\n 3.96432128 3.93463203]\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"import numpy as np\nimport csv\n\n# Path to the CSV file located in Kaggle's output directory\ncsv_file_path = '/kaggle/working/stats_output.csv'\n\n# Function to append the mnf and pmnf values to the existing CSV file\ndef append_entr_to_csv(entr_values, filename=csv_file_path):\n    # Read the existing rows of the CSV first\n    with open(filename, 'r', newline='') as read_file:\n        reader = csv.reader(read_file)\n        rows = list(reader)\n\n    # Ensure the number of mdf and pmdf values match the number of rows in the CSV\n    num_rows_in_csv = len(rows) - 1  # excluding the header row\n    total_values_in_flat = len(entr_values)  # Length of flattened arrays\n\n\n    # Now iterate and append mnf and pmnf values to the respective columns\n    index = 0\n    for i in range(1, len(rows)):  # Skip header row\n        # If the index exceeds the number of available mdf and pmdf values, skip appending\n        if index < len(entr_values):\n            entr = entr_values[index]\n            rows[i].append(entr)  # Append mdf value\n        index += 1\n        if index >= total_values_in_flat:\n            break  # Prevent out of bounds access if index exceeds flattened array size\n\n    # Write the modified rows back into the file\n    with open(filename, 'w', newline='') as write_file:\n        writer = csv.writer(write_file)\n        # Write the header first\n        rows[0].extend([\"entr\"])  # Add the \"mnf\" and \"pmnf\" columns to the header\n        writer.writerows(rows)\n\n\n\n# Combine the mnf and pmnf values into single lists (flattened arrays)\nentr_values = np.concatenate([entr_LH_pre_P.flatten(), entr_RH_pre_P.flatten(),\n                             entr_LH_pre_NP.flatten(), entr_RH_pre_NP.flatten(),\n                             entr_LH_post_P.flatten(), entr_RH_post_P.flatten(),\n                             entr_LH_post_NP.flatten(), entr_RH_post_NP.flatten()])\n\n\n# Call the function to append to the existing CSV\nappend_entr_to_csv(entr_values)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T07:22:15.585073Z","iopub.execute_input":"2025-01-22T07:22:15.586059Z","iopub.status.idle":"2025-01-22T07:22:15.691723Z","shell.execute_reply.started":"2025-01-22T07:22:15.586011Z","shell.execute_reply":"2025-01-22T07:22:15.690606Z"}},"outputs":[],"execution_count":41}]}